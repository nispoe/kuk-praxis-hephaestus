{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using Mistral 8x22B v0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bef3d-89b9-42ed-b158-a39bd61f6a31",
   "metadata": {},
   "source": [
    "### Base Model and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe5f014-527c-4a83-863b-4b6330c72ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 0: NVIDIA GeForce RTX 4090\n",
    "# GPU 1: NVIDIA GeForce RTX 4090\n",
    "# GPU 2: NVIDIA GeForce RTX 4090\n",
    "# GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
    "# GPU 5: NVIDIA GeForce RTX 3090\n",
    "# GPU 6: NVIDIA GeForce RTX 3090\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # \"\"makes all visible, \"0\" GPU 0 visible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ffaa11-3936-40a0-8f2a-3d083ff2afef",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77f7e3d-3af3-4dc8-9571-d13398c29ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef535c7a-848e-4c6e-a692-208f98610d82",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfef1a8-c05a-4e14-af0e-358bfb04352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cce100-9b2e-4675-bfda-f029e7c4056e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ba548dd005432caf15171d5a834fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15ec782b-0776-4354-ad08-66f1e9e50187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"Mixtral-8x22B-v0.1\"\n",
    "checkpoint = \"mistralai/\"+model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6c2e39-684b-46c7-bf2c-a9b64a566698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1eba816fef43e3a5bc1807268fca14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x22B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, quantization_config=bnb_config, device_map=\"auto\", num_labels=2, token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83f74939-3ab3-4011-90cb-8e90c22ea162",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "MixtralForSequenceClassification                                  --\n",
       "├─MixtralModel: 1-1                                               --\n",
       "│    └─Embedding: 2-1                                             196,608,000\n",
       "│    └─ModuleList: 2-2                                            --\n",
       "│    │    └─MixtralDecoderLayer: 3-1                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-2                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-3                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-4                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-5                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-6                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-7                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-8                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-9                              1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-10                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-11                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-12                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-13                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-14                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-15                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-16                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-17                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-18                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-19                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-20                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-21                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-22                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-23                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-24                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-25                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-26                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-27                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-28                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-29                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-30                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-31                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-32                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-33                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-34                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-35                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-36                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-37                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-38                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-39                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-40                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-41                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-42                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-43                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-44                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-45                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-46                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-47                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-48                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-49                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-50                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-51                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-52                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-53                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-54                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-55                             1,252,036,608\n",
       "│    │    └─MixtralDecoderLayer: 3-56                             1,252,036,608\n",
       "│    └─MixtralRMSNorm: 2-3                                        6,144\n",
       "├─Linear: 1-2                                                     12,288\n",
       "==========================================================================================\n",
       "Total params: 70,310,676,480\n",
       "Trainable params: 197,314,560\n",
       "Non-trainable params: 70,113,361,920\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49bbe47c-4430-4f4a-90d8-1113bd320a18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtralForSequenceClassification(\n",
       "  (model): MixtralModel(\n",
       "    (embed_tokens): Embedding(32000, 6144)\n",
       "    (layers): ModuleList(\n",
       "      (0-55): 56 x MixtralDecoderLayer(\n",
       "        (self_attn): MixtralSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=6144, out_features=6144, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=6144, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=6144, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=6144, out_features=6144, bias=False)\n",
       "          (rotary_emb): MixtralRotaryEmbedding()\n",
       "        )\n",
       "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "          (gate): Linear4bit(in_features=6144, out_features=8, bias=False)\n",
       "          (experts): ModuleList(\n",
       "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "              (w1): Linear4bit(in_features=6144, out_features=16384, bias=False)\n",
       "              (w2): Linear4bit(in_features=16384, out_features=6144, bias=False)\n",
       "              (w3): Linear4bit(in_features=6144, out_features=16384, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (input_layernorm): MixtralRMSNorm()\n",
       "        (post_attention_layernorm): MixtralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MixtralRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=6144, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6627b1da-57d6-4a17-8ea3-746b5cb1f3d9",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc372cc-0cae-4b49-a2d7-8e57f58244a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad450a2-6cef-432e-9e63-7ff985b4726e",
   "metadata": {},
   "source": [
    "### Tokenization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2dc68c-c0cf-416a-932b-c2918e693e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, token=access_token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "    return tokenizer(news['article'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02efc4f0-742a-4838-887d-5556a26ae15f",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3e3101-df30-4af2-9976-49ba0cf44d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/Mixtral-8x22B-v0.1-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/Mixtral-8x22B-v0.1-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/Mixtral-8x22B-v0.1-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307cc4fb-9766-4b0f-943b-4a1c90053e09",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44321182-e1b9-4570-bd24-7a5cf42ba504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64be678b-d2a8-403e-bcc8-ef9e7eabb0cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [1, 560, 264, 879, 970, 3153, 294, 1926, 28725, 5017, 274, 304, 7006, 537, 497, 7483, 7191, 28725, 1450, 2726, 3805, 1309, 2672, 4592, 9311, 6088, 28705, 28740, 28723, 28787, 4153, 297, 28705, 28750, 28734, 28740, 28782, 28725, 15317, 396, 7544, 17468, 369, 3125, 297, 272, 28705, 28740, 28774, 28774, 28734, 28713, 28725, 11800, 8268, 9238, 263, 4246, 1896, 1061, 266, 773, 9262, 28723, 13, 3167, 264, 4231, 9887, 395, 20671, 6502, 340, 2025, 293, 691, 28725, 1896, 1061, 266, 8466, 286, 1432, 879, 28809, 28713, 9311, 13110, 28725, 690, 400, 773, 28725, 739, 9837, 395, 396, 1019, 6084, 17468, 297, 28705, 28750, 28734, 28740, 28781, 28725, 1658, 298, 1846, 272, 4813, 369, 15045, 8512, 2018, 3481, 28809, 28707, 3688, 916, 272, 633, 10298, 438, 3805, 6756, 28723, 13, 28835, 23475, 478, 506, 553, 741, 19363, 10223, 28725, 741, 12095, 297, 2552, 13187, 28725, 272, 7544, 9156, 297, 544, 813, 9311, 13187, 10352, 298, 576, 1060, 3372, 1896, 1061, 266, 2240, 25875, 28723, 981, 1313, 403, 264, 1215, 1179, 879, 354, 592, 28725, 28705, 28750, 28734, 28740, 28782, 28723, 13, 28769, 7412, 1926, 7483, 486, 28705, 28781, 28723, 28782, 4153, 297, 28705, 28750, 28734, 28740, 28782, 28725, 11862, 298, 28705, 28770, 28782, 28734, 477, 28705, 28770, 28770, 28770, 297, 272, 4681, 879, 28725, 690, 403, 272, 15341, 1854, 28705, 28740, 28774, 28774, 28781, 28725, 773, 27694, 9238, 263, 384, 858, 322, 985, 28708, 28723, 399, 7722, 7483, 28705, 28784, 4153, 304, 7006, 537, 497, 8536, 28705, 28750, 4153, 28725, 773, 985, 28708, 28725, 693, 349, 297, 5685, 302, 1178, 5442, 304, 6933, 354, 272, 11800, 8268, 28723, 13, 1014, 3889, 7544, 9311, 13110, 1988, 684, 2940, 298, 767, 985, 28708, 1987, 981, 3731, 286, 19046, 3372, 970, 19407, 1038, 4045, 8431, 28713, 1019, 2070, 272, 7544, 1474, 302, 954, 267, 7533, 594, 403, 272, 15341, 297, 272, 2990, 1854, 28705, 28750, 28734, 28734, 28770, 28723, 13, 13849, 287, 11376, 28713, 1040, 21638, 304, 272, 8303, 28744, 1040, 2590, 2672, 4592, 17440, 5247, 486, 28705, 28770, 4153, 304, 28705, 28781, 4153, 28725, 8628, 28725, 985, 28708, 773, 28723, 21638, 28809, 28713, 5247, 403, 12215, 486, 680, 7006, 537, 497, 28725, 1312, 272, 8303, 28744, 28725, 5432, 6252, 396, 7544, 9311, 5247, 28725, 553, 767, 400, 773, 403, 264, 981, 28720, 540, 4361, 282, 28838, 13388, 297, 6041, 742, 28723, 3805, 5734, 28725, 6041, 742, 654, 1060, 297, 28705, 28750, 28734, 28740, 28782, 684, 28705, 28770, 4153, 28725, 298, 28705, 28740, 28725, 28740, 28734, 28770, 477, 28705, 28740, 28725, 28740, 28787, 28750, 297, 28705, 28750, 28734, 28740, 28781, 28723, 13, 4853, 28708, 12282, 26133, 272, 28705, 28750, 28734, 28740, 28782, 5247, 297, 5017, 274, 298, 13980, 3524, 3814, 395, 23430, 684, 10813, 477, 1267, 2609, 28723, 13, 7384, 582, 298, 625, 272, 7345, 11756, 2483, 7615, 1466, 28742, 28713, 15102, 288, 7615, 389, 9916, 297, 574, 297, 2858, 28723, 2463, 24675, 9315, 582, 28725, 368, 5861, 298, 813, 12917, 4920, 28723, 13, 28835, 21173, 3743, 4153, 302, 1167, 5017, 274, 1539, 28809, 28707, 4804, 297, 28705, 28750, 28734, 28740, 28782, 3372, 400, 773, 28723], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7f4f8-9eb3-4feb-86e7-fb0dad88753f",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3eab29e-fa47-4a13-abc4-d6425ae741b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e040f1-e596-476d-9f26-ffa6f8a4a548",
   "metadata": {},
   "source": [
    "### LoRA - Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0532bdc6-4317-474b-859c-4e5d2fe6f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd29494-4453-4c01-b878-ef2f4533d34d",
   "metadata": {},
   "source": [
    "### Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5058f6d-185d-45af-83b9-bb7f61d4bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b14fe8f2-eccd-4cfe-9d20-ef48bc178842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 17444864 || all params: 70328121344 || trainable%: 0.02480496232036532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MixtralForSequenceClassification(\n",
       "      (model): MixtralModel(\n",
       "        (embed_tokens): Embedding(32000, 6144)\n",
       "        (layers): ModuleList(\n",
       "          (0-55): 56 x MixtralDecoderLayer(\n",
       "            (self_attn): MixtralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=6144, out_features=6144, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=6144, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=6144, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=6144, out_features=6144, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=6144, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): MixtralRotaryEmbedding()\n",
       "            )\n",
       "            (block_sparse_moe): MixtralSparseMoeBlock(\n",
       "              (gate): Linear4bit(in_features=6144, out_features=8, bias=False)\n",
       "              (experts): ModuleList(\n",
       "                (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
       "                  (w1): Linear4bit(in_features=6144, out_features=16384, bias=False)\n",
       "                  (w2): Linear4bit(in_features=16384, out_features=6144, bias=False)\n",
       "                  (w3): Linear4bit(in_features=6144, out_features=16384, bias=False)\n",
       "                  (act_fn): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (input_layernorm): MixtralRMSNorm()\n",
       "            (post_attention_layernorm): MixtralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MixtralRMSNorm()\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=6144, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=6144, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_trainable_parameters(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4782-03f8-4335-bfda-2a65794bff2c",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c27c3328-1926-4adc-8696-b3b343ec4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdf68d0e-8786-489d-a4b8-b7478513efea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun 11 19:18:11 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.67                 Driver Version: 550.67         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   38C    P2            124W /  420W |   10451MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   43C    P2             71W /  450W |   10185MiB /  24564MiB |      3%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2B:00.0 Off |                  Off |\n",
      "| 31%   42C    P2            102W /  450W |   10467MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   38C    P2            113W /  420W |   11723MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   47C    P2             54W /  450W |   10610MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   43C    P2             45W /  450W |   10596MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 30%   39C    P2             96W /  450W |   10455MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A     15624      C   /usr/bin/python3                            10434MiB |\n",
      "|    1   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                             86MiB |\n",
      "|    1   N/A  N/A      2599      G   /usr/bin/gnome-shell                           13MiB |\n",
      "|    1   N/A  N/A     15624      C   /usr/bin/python3                            10068MiB |\n",
      "|    2   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A     15624      C   /usr/bin/python3                            10450MiB |\n",
      "|    3   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A     15624      C   /usr/bin/python3                            11706MiB |\n",
      "|    4   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A     15624      C   /usr/bin/python3                            10592MiB |\n",
      "|    5   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A     15624      C   /usr/bin/python3                            10578MiB |\n",
      "|    6   N/A  N/A      2515      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A     15624      C   /usr/bin/python3                            10438MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605fe8e-b267-49d6-a15c-6c9a9a51685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1199841a-6519-4422-8259-2fdbb114e466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8c7f3ee-6997-4ed7-b28e-5d574831fb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-Mixtral-8x22B-v0.1-small-finetune'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7a69bde-1d50-46a0-9838-00812afbdb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3cd41-fd17-4fd8-83b8-54f464df79ff",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2df4e2a5-2a08-434b-983a-f6cd42f33da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 19:18:12.325592: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 19:18:12.905548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/data/kuk/Praxis/wandb/run-20240611_191814-cw845ltq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/cw845ltq' target=\"_blank\">pretty-frog-379</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/cw845ltq' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/cw845ltq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12606' max='12606' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12606/12606 77:54:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008017</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>0.999267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013603</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>0.999267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017048</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>0.999267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12606, training_loss=0.02111224345856955, metrics={'train_runtime': 280508.7831, 'train_samples_per_second': 0.359, 'train_steps_per_second': 0.045, 'total_flos': 4.344210895102437e+19, 'train_loss': 0.02111224345856955, 'epoch': 3.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from pathlib import Path\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=3,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train(resume_from_checkpoint=False)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13982d-53cb-4c88-bd32-a98d4a5a9874",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bd570c7-8feb-4b69-bda7-9c678bfd92c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 Jun 11 19:18 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun 12 21:16 checkpoint-4202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun 13 23:15 checkpoint-8404\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 Jun 15 01:13 checkpoint-12606\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97771269-d77c-4c1b-b264-34e082db6cbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-Mixtral-8x22B-v0.1-small-finetune/logs/events.out.tfevents.1718151493.hephaestus.15624.0\n",
      "Step: 10, train/loss: 5.2484002113342285\n",
      "Step: 10, train/grad_norm: 134.17823791503906\n",
      "Step: 10, train/learning_rate: 4.9960337491938844e-05\n",
      "Step: 10, train/epoch: 0.0023798190522938967\n",
      "Step: 20, train/loss: 2.1171000003814697\n",
      "Step: 20, train/grad_norm: 72.31343841552734\n",
      "Step: 20, train/learning_rate: 4.992067260900512e-05\n",
      "Step: 20, train/epoch: 0.004759638104587793\n",
      "Step: 30, train/loss: 1.0960999727249146\n",
      "Step: 30, train/grad_norm: 133.9967041015625\n",
      "Step: 30, train/learning_rate: 4.98810077260714e-05\n",
      "Step: 30, train/epoch: 0.007139457389712334\n",
      "Step: 40, train/loss: 0.4381999969482422\n",
      "Step: 40, train/grad_norm: 117.1510009765625\n",
      "Step: 40, train/learning_rate: 4.984134648111649e-05\n",
      "Step: 40, train/epoch: 0.009519276209175587\n",
      "Step: 50, train/loss: 0.20149999856948853\n",
      "Step: 50, train/grad_norm: 0.013337704353034496\n",
      "Step: 50, train/learning_rate: 4.980168159818277e-05\n",
      "Step: 50, train/epoch: 0.011899095959961414\n",
      "Step: 60, train/loss: 0.13269999623298645\n",
      "Step: 60, train/grad_norm: 0.3822653889656067\n",
      "Step: 60, train/learning_rate: 4.976201671524905e-05\n",
      "Step: 60, train/epoch: 0.014278914779424667\n",
      "Step: 70, train/loss: 0.005900000222027302\n",
      "Step: 70, train/grad_norm: 0.032834284007549286\n",
      "Step: 70, train/learning_rate: 4.972235547029413e-05\n",
      "Step: 70, train/epoch: 0.016658734530210495\n",
      "Step: 80, train/loss: 0.015399999916553497\n",
      "Step: 80, train/grad_norm: 2.6704012725531356e-06\n",
      "Step: 80, train/learning_rate: 4.968269058736041e-05\n",
      "Step: 80, train/epoch: 0.019038552418351173\n",
      "Step: 90, train/loss: 0.04600000008940697\n",
      "Step: 90, train/grad_norm: 0.0011097420938313007\n",
      "Step: 90, train/learning_rate: 4.964302570442669e-05\n",
      "Step: 90, train/epoch: 0.021418372169137\n",
      "Step: 100, train/loss: 0.02290000021457672\n",
      "Step: 100, train/grad_norm: 1.106797942185267e-08\n",
      "Step: 100, train/learning_rate: 4.960336445947178e-05\n",
      "Step: 100, train/epoch: 0.02379819191992283\n",
      "Step: 110, train/loss: 0.0008999999845400453\n",
      "Step: 110, train/grad_norm: 3.0615751600571173e-10\n",
      "Step: 110, train/learning_rate: 4.9563699576538056e-05\n",
      "Step: 110, train/epoch: 0.026178009808063507\n",
      "Step: 120, train/loss: 0.3610000014305115\n",
      "Step: 120, train/grad_norm: 2.6039319891424384e-06\n",
      "Step: 120, train/learning_rate: 4.9524034693604335e-05\n",
      "Step: 120, train/epoch: 0.028557829558849335\n",
      "Step: 130, train/loss: 0.0\n",
      "Step: 130, train/grad_norm: 1.2270621141396987e-07\n",
      "Step: 130, train/learning_rate: 4.948437344864942e-05\n",
      "Step: 130, train/epoch: 0.030937649309635162\n",
      "Step: 140, train/loss: 0.3781000077724457\n",
      "Step: 140, train/grad_norm: 74.62997436523438\n",
      "Step: 140, train/learning_rate: 4.94447085657157e-05\n",
      "Step: 140, train/epoch: 0.03331746906042099\n",
      "Step: 150, train/loss: 0.0\n",
      "Step: 150, train/grad_norm: 9.907519142871024e-07\n",
      "Step: 150, train/learning_rate: 4.940504368278198e-05\n",
      "Step: 150, train/epoch: 0.03569728881120682\n",
      "Step: 160, train/loss: 0.4672999978065491\n",
      "Step: 160, train/grad_norm: 109.05107879638672\n",
      "Step: 160, train/learning_rate: 4.9365382437827066e-05\n",
      "Step: 160, train/epoch: 0.03807710483670235\n",
      "Step: 170, train/loss: 0.3125\n",
      "Step: 170, train/grad_norm: 0.0198566522449255\n",
      "Step: 170, train/learning_rate: 4.9325717554893345e-05\n",
      "Step: 170, train/epoch: 0.040456924587488174\n",
      "Step: 180, train/loss: 0.0\n",
      "Step: 180, train/grad_norm: 0.0003607745165936649\n",
      "Step: 180, train/learning_rate: 4.9286052671959624e-05\n",
      "Step: 180, train/epoch: 0.042836744338274\n",
      "Step: 190, train/loss: 0.210999995470047\n",
      "Step: 190, train/grad_norm: 8.064604116952978e-06\n",
      "Step: 190, train/learning_rate: 4.924639142700471e-05\n",
      "Step: 190, train/epoch: 0.04521656408905983\n",
      "Step: 200, train/loss: 0.0\n",
      "Step: 200, train/grad_norm: 0.0012919591972604394\n",
      "Step: 200, train/learning_rate: 4.920672654407099e-05\n",
      "Step: 200, train/epoch: 0.04759638383984566\n",
      "Step: 210, train/loss: 0.04129999876022339\n",
      "Step: 210, train/grad_norm: 1.732141981847235e-06\n",
      "Step: 210, train/learning_rate: 4.916706166113727e-05\n",
      "Step: 210, train/epoch: 0.049976203590631485\n",
      "Step: 220, train/loss: 0.0\n",
      "Step: 220, train/grad_norm: 0.013265854679048061\n",
      "Step: 220, train/learning_rate: 4.9127400416182354e-05\n",
      "Step: 220, train/epoch: 0.052356019616127014\n",
      "Step: 230, train/loss: 0.0\n",
      "Step: 230, train/grad_norm: 0.0009040384320542216\n",
      "Step: 230, train/learning_rate: 4.908773553324863e-05\n",
      "Step: 230, train/epoch: 0.05473583936691284\n",
      "Step: 240, train/loss: 0.0008999999845400453\n",
      "Step: 240, train/grad_norm: 3.3062191505450755e-05\n",
      "Step: 240, train/learning_rate: 4.904807065031491e-05\n",
      "Step: 240, train/epoch: 0.05711565911769867\n",
      "Step: 250, train/loss: 9.999999747378752e-05\n",
      "Step: 250, train/grad_norm: 7.647205785277222e-11\n",
      "Step: 250, train/learning_rate: 4.900840940536e-05\n",
      "Step: 250, train/epoch: 0.0594954788684845\n",
      "Step: 260, train/loss: 0.0\n",
      "Step: 260, train/grad_norm: 3.371277657038263e-08\n",
      "Step: 260, train/learning_rate: 4.896874452242628e-05\n",
      "Step: 260, train/epoch: 0.061875298619270325\n",
      "Step: 270, train/loss: 0.0\n",
      "Step: 270, train/grad_norm: 7.808682312315796e-06\n",
      "Step: 270, train/learning_rate: 4.8929079639492556e-05\n",
      "Step: 270, train/epoch: 0.06425511837005615\n",
      "Step: 280, train/loss: 0.0\n",
      "Step: 280, train/grad_norm: 5.9118960393789166e-08\n",
      "Step: 280, train/learning_rate: 4.888941839453764e-05\n",
      "Step: 280, train/epoch: 0.06663493812084198\n",
      "Step: 290, train/loss: 0.18129999935626984\n",
      "Step: 290, train/grad_norm: 7.741456087284604e-11\n",
      "Step: 290, train/learning_rate: 4.884975351160392e-05\n",
      "Step: 290, train/epoch: 0.06901475787162781\n",
      "Step: 300, train/loss: 0.0\n",
      "Step: 300, train/grad_norm: 2.245401446998585e-06\n",
      "Step: 300, train/learning_rate: 4.88100886286702e-05\n",
      "Step: 300, train/epoch: 0.07139457762241364\n",
      "Step: 310, train/loss: 0.0\n",
      "Step: 310, train/grad_norm: 0.00011887060099979863\n",
      "Step: 310, train/learning_rate: 4.877042738371529e-05\n",
      "Step: 310, train/epoch: 0.07377438992261887\n",
      "Step: 320, train/loss: 9.999999747378752e-05\n",
      "Step: 320, train/grad_norm: 1.765796696417965e-05\n",
      "Step: 320, train/learning_rate: 4.8730762500781566e-05\n",
      "Step: 320, train/epoch: 0.0761542096734047\n",
      "Step: 330, train/loss: 0.0\n",
      "Step: 330, train/grad_norm: 1.0875121461140225e-06\n",
      "Step: 330, train/learning_rate: 4.869110125582665e-05\n",
      "Step: 330, train/epoch: 0.07853402942419052\n",
      "Step: 340, train/loss: 0.05249999836087227\n",
      "Step: 340, train/grad_norm: 2.0311544446371954e-08\n",
      "Step: 340, train/learning_rate: 4.865143637289293e-05\n",
      "Step: 340, train/epoch: 0.08091384917497635\n",
      "Step: 350, train/loss: 0.27889999747276306\n",
      "Step: 350, train/grad_norm: 0.004503167700022459\n",
      "Step: 350, train/learning_rate: 4.861177148995921e-05\n",
      "Step: 350, train/epoch: 0.08329366892576218\n",
      "Step: 360, train/loss: 0.000699999975040555\n",
      "Step: 360, train/grad_norm: 2.76046157523524e-05\n",
      "Step: 360, train/learning_rate: 4.8572110245004296e-05\n",
      "Step: 360, train/epoch: 0.085673488676548\n",
      "Step: 370, train/loss: 0.0\n",
      "Step: 370, train/grad_norm: 0.00010028688848251477\n",
      "Step: 370, train/learning_rate: 4.8532445362070575e-05\n",
      "Step: 370, train/epoch: 0.08805330842733383\n",
      "Step: 380, train/loss: 0.2563000023365021\n",
      "Step: 380, train/grad_norm: 1.2389431702786169e-08\n",
      "Step: 380, train/learning_rate: 4.8492780479136854e-05\n",
      "Step: 380, train/epoch: 0.09043312817811966\n",
      "Step: 390, train/loss: 0.1371999979019165\n",
      "Step: 390, train/grad_norm: 6.690541744232178\n",
      "Step: 390, train/learning_rate: 4.845311923418194e-05\n",
      "Step: 390, train/epoch: 0.09281294792890549\n",
      "Step: 400, train/loss: 0.20080000162124634\n",
      "Step: 400, train/grad_norm: 2.4145222710103553e-07\n",
      "Step: 400, train/learning_rate: 4.841345435124822e-05\n",
      "Step: 400, train/epoch: 0.09519276767969131\n",
      "Step: 410, train/loss: 0.0\n",
      "Step: 410, train/grad_norm: 6.034414301581137e-09\n",
      "Step: 410, train/learning_rate: 4.83737894683145e-05\n",
      "Step: 410, train/epoch: 0.09757258743047714\n",
      "Step: 420, train/loss: 0.04340000078082085\n",
      "Step: 420, train/grad_norm: 6.908527083737681e-09\n",
      "Step: 420, train/learning_rate: 4.8334128223359585e-05\n",
      "Step: 420, train/epoch: 0.09995240718126297\n",
      "Step: 430, train/loss: 0.0\n",
      "Step: 430, train/grad_norm: 5.1005386580982304e-08\n",
      "Step: 430, train/learning_rate: 4.8294463340425864e-05\n",
      "Step: 430, train/epoch: 0.1023322194814682\n",
      "Step: 440, train/loss: 0.0\n",
      "Step: 440, train/grad_norm: 1.521409664917428e-08\n",
      "Step: 440, train/learning_rate: 4.825479845749214e-05\n",
      "Step: 440, train/epoch: 0.10471203923225403\n",
      "Step: 450, train/loss: 0.0\n",
      "Step: 450, train/grad_norm: 1.3829659906150482e-07\n",
      "Step: 450, train/learning_rate: 4.821513721253723e-05\n",
      "Step: 450, train/epoch: 0.10709185898303986\n",
      "Step: 460, train/loss: 0.0\n",
      "Step: 460, train/grad_norm: 3.0595914256997814e-12\n",
      "Step: 460, train/learning_rate: 4.817547232960351e-05\n",
      "Step: 460, train/epoch: 0.10947167873382568\n",
      "Step: 470, train/loss: 0.025800000876188278\n",
      "Step: 470, train/grad_norm: 1.3571319415484062e-11\n",
      "Step: 470, train/learning_rate: 4.813580744666979e-05\n",
      "Step: 470, train/epoch: 0.11185149848461151\n",
      "Step: 480, train/loss: 0.0\n",
      "Step: 480, train/grad_norm: 5.224648269575027e-09\n",
      "Step: 480, train/learning_rate: 4.809614620171487e-05\n",
      "Step: 480, train/epoch: 0.11423131823539734\n",
      "Step: 490, train/loss: 0.0\n",
      "Step: 490, train/grad_norm: 2.8774748209170475e-11\n",
      "Step: 490, train/learning_rate: 4.805648131878115e-05\n",
      "Step: 490, train/epoch: 0.11661113798618317\n",
      "Step: 500, train/loss: 0.0\n",
      "Step: 500, train/grad_norm: 3.439112958858459e-07\n",
      "Step: 500, train/learning_rate: 4.801681643584743e-05\n",
      "Step: 500, train/epoch: 0.118990957736969\n",
      "Step: 510, train/loss: 0.0\n",
      "Step: 510, train/grad_norm: 1.270787777229998e-07\n",
      "Step: 510, train/learning_rate: 4.797715519089252e-05\n",
      "Step: 510, train/epoch: 0.12137077748775482\n",
      "Step: 520, train/loss: 0.0\n",
      "Step: 520, train/grad_norm: 2.137148236158737e-08\n",
      "Step: 520, train/learning_rate: 4.79374903079588e-05\n",
      "Step: 520, train/epoch: 0.12375059723854065\n",
      "Step: 530, train/loss: 0.0\n",
      "Step: 530, train/grad_norm: 1.382161798346715e-07\n",
      "Step: 530, train/learning_rate: 4.7897825425025076e-05\n",
      "Step: 530, train/epoch: 0.12613041698932648\n",
      "Step: 540, train/loss: 0.0\n",
      "Step: 540, train/grad_norm: 8.324962053052332e-09\n",
      "Step: 540, train/learning_rate: 4.785816418007016e-05\n",
      "Step: 540, train/epoch: 0.1285102367401123\n",
      "Step: 550, train/loss: 0.0\n",
      "Step: 550, train/grad_norm: 1.2640079954939765e-11\n",
      "Step: 550, train/learning_rate: 4.781849929713644e-05\n",
      "Step: 550, train/epoch: 0.13089005649089813\n",
      "Step: 560, train/loss: 0.0\n",
      "Step: 560, train/grad_norm: 5.0510319852037355e-05\n",
      "Step: 560, train/learning_rate: 4.777883441420272e-05\n",
      "Step: 560, train/epoch: 0.13326987624168396\n",
      "Step: 570, train/loss: 0.009100000374019146\n",
      "Step: 570, train/grad_norm: 6.481392489376958e-08\n",
      "Step: 570, train/learning_rate: 4.7739173169247806e-05\n",
      "Step: 570, train/epoch: 0.1356496959924698\n",
      "Step: 580, train/loss: 0.0\n",
      "Step: 580, train/grad_norm: 4.151290422527154e-11\n",
      "Step: 580, train/learning_rate: 4.7699508286314085e-05\n",
      "Step: 580, train/epoch: 0.13802951574325562\n",
      "Step: 590, train/loss: 0.0\n",
      "Step: 590, train/grad_norm: 0.014266842044889927\n",
      "Step: 590, train/learning_rate: 4.7659843403380364e-05\n",
      "Step: 590, train/epoch: 0.14040933549404144\n",
      "Step: 600, train/loss: 0.0\n",
      "Step: 600, train/grad_norm: 1.4454770280281082e-05\n",
      "Step: 600, train/learning_rate: 4.762018215842545e-05\n",
      "Step: 600, train/epoch: 0.14278915524482727\n",
      "Step: 610, train/loss: 0.0\n",
      "Step: 610, train/grad_norm: 2.2344700312260102e-07\n",
      "Step: 610, train/learning_rate: 4.758051727549173e-05\n",
      "Step: 610, train/epoch: 0.1451689600944519\n",
      "Step: 620, train/loss: 0.00019999999494757503\n",
      "Step: 620, train/grad_norm: 2.6905305761104614e-10\n",
      "Step: 620, train/learning_rate: 4.754085239255801e-05\n",
      "Step: 620, train/epoch: 0.14754877984523773\n",
      "Step: 630, train/loss: 0.0\n",
      "Step: 630, train/grad_norm: 7.803494656011267e-11\n",
      "Step: 630, train/learning_rate: 4.7501191147603095e-05\n",
      "Step: 630, train/epoch: 0.14992859959602356\n",
      "Step: 640, train/loss: 0.0\n",
      "Step: 640, train/grad_norm: 8.958646324774833e-11\n",
      "Step: 640, train/learning_rate: 4.7461526264669374e-05\n",
      "Step: 640, train/epoch: 0.1523084193468094\n",
      "Step: 650, train/loss: 0.0\n",
      "Step: 650, train/grad_norm: 2.563304603298011e-12\n",
      "Step: 650, train/learning_rate: 4.742186138173565e-05\n",
      "Step: 650, train/epoch: 0.15468823909759521\n",
      "Step: 660, train/loss: 0.0\n",
      "Step: 660, train/grad_norm: 2.658771203201127e-09\n",
      "Step: 660, train/learning_rate: 4.738220013678074e-05\n",
      "Step: 660, train/epoch: 0.15706805884838104\n",
      "Step: 670, train/loss: 0.0\n",
      "Step: 670, train/grad_norm: 3.837175716059349e-11\n",
      "Step: 670, train/learning_rate: 4.734253525384702e-05\n",
      "Step: 670, train/epoch: 0.15944787859916687\n",
      "Step: 680, train/loss: 0.4203000068664551\n",
      "Step: 680, train/grad_norm: 3.3534373500287984e-08\n",
      "Step: 680, train/learning_rate: 4.73028703709133e-05\n",
      "Step: 680, train/epoch: 0.1618276983499527\n",
      "Step: 690, train/loss: 0.0\n",
      "Step: 690, train/grad_norm: 1.7497814042144455e-07\n",
      "Step: 690, train/learning_rate: 4.726320912595838e-05\n",
      "Step: 690, train/epoch: 0.16420751810073853\n",
      "Step: 700, train/loss: 0.0\n",
      "Step: 700, train/grad_norm: 5.724669183138076e-09\n",
      "Step: 700, train/learning_rate: 4.722354424302466e-05\n",
      "Step: 700, train/epoch: 0.16658733785152435\n",
      "Step: 710, train/loss: 0.4124999940395355\n",
      "Step: 710, train/grad_norm: 5.777176781407434e-08\n",
      "Step: 710, train/learning_rate: 4.718387936009094e-05\n",
      "Step: 710, train/epoch: 0.16896715760231018\n",
      "Step: 720, train/loss: 0.0\n",
      "Step: 720, train/grad_norm: 0.021020924672484398\n",
      "Step: 720, train/learning_rate: 4.714421811513603e-05\n",
      "Step: 720, train/epoch: 0.171346977353096\n",
      "Step: 730, train/loss: 0.0\n",
      "Step: 730, train/grad_norm: 1.46160118674743e-07\n",
      "Step: 730, train/learning_rate: 4.7104553232202306e-05\n",
      "Step: 730, train/epoch: 0.17372679710388184\n",
      "Step: 740, train/loss: 0.1054999977350235\n",
      "Step: 740, train/grad_norm: 2.169403927609892e-07\n",
      "Step: 740, train/learning_rate: 4.7064888349268585e-05\n",
      "Step: 740, train/epoch: 0.17610661685466766\n",
      "Step: 750, train/loss: 0.0044999998062849045\n",
      "Step: 750, train/grad_norm: 9.06580854831418e-09\n",
      "Step: 750, train/learning_rate: 4.702522710431367e-05\n",
      "Step: 750, train/epoch: 0.1784864366054535\n",
      "Step: 760, train/loss: 0.0\n",
      "Step: 760, train/grad_norm: 0.00011666147474898025\n",
      "Step: 760, train/learning_rate: 4.698556222137995e-05\n",
      "Step: 760, train/epoch: 0.18086625635623932\n",
      "Step: 770, train/loss: 0.0\n",
      "Step: 770, train/grad_norm: 3.2400939709020804e-09\n",
      "Step: 770, train/learning_rate: 4.694589733844623e-05\n",
      "Step: 770, train/epoch: 0.18324607610702515\n",
      "Step: 780, train/loss: 0.056299999356269836\n",
      "Step: 780, train/grad_norm: 0.0052193533629179\n",
      "Step: 780, train/learning_rate: 4.6906236093491316e-05\n",
      "Step: 780, train/epoch: 0.18562589585781097\n",
      "Step: 790, train/loss: 0.0\n",
      "Step: 790, train/grad_norm: 1.121654413793749e-08\n",
      "Step: 790, train/learning_rate: 4.6866571210557595e-05\n",
      "Step: 790, train/epoch: 0.1880057156085968\n",
      "Step: 800, train/loss: 0.016599999740719795\n",
      "Step: 800, train/grad_norm: 65.77535247802734\n",
      "Step: 800, train/learning_rate: 4.6826906327623874e-05\n",
      "Step: 800, train/epoch: 0.19038553535938263\n",
      "Step: 810, train/loss: 0.13199999928474426\n",
      "Step: 810, train/grad_norm: 6.8401527641981374e-06\n",
      "Step: 810, train/learning_rate: 4.678724508266896e-05\n",
      "Step: 810, train/epoch: 0.19276535511016846\n",
      "Step: 820, train/loss: 0.0940999984741211\n",
      "Step: 820, train/grad_norm: 0.002262819092720747\n",
      "Step: 820, train/learning_rate: 4.674758019973524e-05\n",
      "Step: 820, train/epoch: 0.19514517486095428\n",
      "Step: 830, train/loss: 0.00019999999494757503\n",
      "Step: 830, train/grad_norm: 1.387352540405118e-06\n",
      "Step: 830, train/learning_rate: 4.670791531680152e-05\n",
      "Step: 830, train/epoch: 0.1975249946117401\n",
      "Step: 840, train/loss: 0.19380000233650208\n",
      "Step: 840, train/grad_norm: 2.2163136748076795e-07\n",
      "Step: 840, train/learning_rate: 4.6668254071846604e-05\n",
      "Step: 840, train/epoch: 0.19990481436252594\n",
      "Step: 850, train/loss: 0.022299999371170998\n",
      "Step: 850, train/grad_norm: 2.901303552960144e-08\n",
      "Step: 850, train/learning_rate: 4.6628589188912883e-05\n",
      "Step: 850, train/epoch: 0.20228461921215057\n",
      "Step: 860, train/loss: 9.999999747378752e-05\n",
      "Step: 860, train/grad_norm: 0.028297167271375656\n",
      "Step: 860, train/learning_rate: 4.658892430597916e-05\n",
      "Step: 860, train/epoch: 0.2046644389629364\n",
      "Step: 870, train/loss: 0.09189999848604202\n",
      "Step: 870, train/grad_norm: 1.4655391566975595e-07\n",
      "Step: 870, train/learning_rate: 4.654926306102425e-05\n",
      "Step: 870, train/epoch: 0.20704425871372223\n",
      "Step: 880, train/loss: 0.0\n",
      "Step: 880, train/grad_norm: 2.0616649012983324e-11\n",
      "Step: 880, train/learning_rate: 4.650959817809053e-05\n",
      "Step: 880, train/epoch: 0.20942407846450806\n",
      "Step: 890, train/loss: 0.0\n",
      "Step: 890, train/grad_norm: 2.8266713622548956e-11\n",
      "Step: 890, train/learning_rate: 4.646993329515681e-05\n",
      "Step: 890, train/epoch: 0.21180389821529388\n",
      "Step: 900, train/loss: 0.0\n",
      "Step: 900, train/grad_norm: 1.1180220296650237e-13\n",
      "Step: 900, train/learning_rate: 4.643027205020189e-05\n",
      "Step: 900, train/epoch: 0.2141837179660797\n",
      "Step: 910, train/loss: 0.0\n",
      "Step: 910, train/grad_norm: 4.763617056369185e-08\n",
      "Step: 910, train/learning_rate: 4.639060716726817e-05\n",
      "Step: 910, train/epoch: 0.21656353771686554\n",
      "Step: 920, train/loss: 0.0\n",
      "Step: 920, train/grad_norm: 0.15352515876293182\n",
      "Step: 920, train/learning_rate: 4.635094228433445e-05\n",
      "Step: 920, train/epoch: 0.21894335746765137\n",
      "Step: 930, train/loss: 0.0003000000142492354\n",
      "Step: 930, train/grad_norm: 3.60125790299648e-10\n",
      "Step: 930, train/learning_rate: 4.631128103937954e-05\n",
      "Step: 930, train/epoch: 0.2213231772184372\n",
      "Step: 940, train/loss: 0.00039999998989515007\n",
      "Step: 940, train/grad_norm: 1.1087802898046561e-10\n",
      "Step: 940, train/learning_rate: 4.6271616156445816e-05\n",
      "Step: 940, train/epoch: 0.22370299696922302\n",
      "Step: 950, train/loss: 0.0\n",
      "Step: 950, train/grad_norm: 1.8553145131328108e-10\n",
      "Step: 950, train/learning_rate: 4.6231951273512095e-05\n",
      "Step: 950, train/epoch: 0.22608281672000885\n",
      "Step: 960, train/loss: 0.0\n",
      "Step: 960, train/grad_norm: 1.1292721069366962e-07\n",
      "Step: 960, train/learning_rate: 4.619229002855718e-05\n",
      "Step: 960, train/epoch: 0.22846263647079468\n",
      "Step: 970, train/loss: 0.0\n",
      "Step: 970, train/grad_norm: 7.349427177949508e-10\n",
      "Step: 970, train/learning_rate: 4.615262514562346e-05\n",
      "Step: 970, train/epoch: 0.2308424562215805\n",
      "Step: 980, train/loss: 0.4187999963760376\n",
      "Step: 980, train/grad_norm: 6.708287259016288e-08\n",
      "Step: 980, train/learning_rate: 4.611296026268974e-05\n",
      "Step: 980, train/epoch: 0.23322227597236633\n",
      "Step: 990, train/loss: 0.0\n",
      "Step: 990, train/grad_norm: 4.565494293728989e-07\n",
      "Step: 990, train/learning_rate: 4.6073299017734826e-05\n",
      "Step: 990, train/epoch: 0.23560209572315216\n",
      "Step: 1000, train/loss: 0.0\n",
      "Step: 1000, train/grad_norm: 7.67419405747205e-05\n",
      "Step: 1000, train/learning_rate: 4.6033634134801105e-05\n",
      "Step: 1000, train/epoch: 0.237981915473938\n",
      "Step: 1010, train/loss: 0.1890999972820282\n",
      "Step: 1010, train/grad_norm: 2.1726802401644818e-07\n",
      "Step: 1010, train/learning_rate: 4.599397288984619e-05\n",
      "Step: 1010, train/epoch: 0.24036173522472382\n",
      "Step: 1020, train/loss: 0.0\n",
      "Step: 1020, train/grad_norm: 1.4284904636951978e-07\n",
      "Step: 1020, train/learning_rate: 4.595430800691247e-05\n",
      "Step: 1020, train/epoch: 0.24274155497550964\n",
      "Step: 1030, train/loss: 0.0\n",
      "Step: 1030, train/grad_norm: 1.1504948105311996e-09\n",
      "Step: 1030, train/learning_rate: 4.591464312397875e-05\n",
      "Step: 1030, train/epoch: 0.24512137472629547\n",
      "Step: 1040, train/loss: 0.0\n",
      "Step: 1040, train/grad_norm: 1.3879744020783846e-09\n",
      "Step: 1040, train/learning_rate: 4.5874981879023835e-05\n",
      "Step: 1040, train/epoch: 0.2475011944770813\n",
      "Step: 1050, train/loss: 0.0\n",
      "Step: 1050, train/grad_norm: 3.492431233098614e-08\n",
      "Step: 1050, train/learning_rate: 4.5835316996090114e-05\n",
      "Step: 1050, train/epoch: 0.24988101422786713\n",
      "Step: 1060, train/loss: 0.0\n",
      "Step: 1060, train/grad_norm: 4.682187082494238e-09\n",
      "Step: 1060, train/learning_rate: 4.579565211315639e-05\n",
      "Step: 1060, train/epoch: 0.25226083397865295\n",
      "Step: 1070, train/loss: 0.0\n",
      "Step: 1070, train/grad_norm: 4.753925253453417e-08\n",
      "Step: 1070, train/learning_rate: 4.575599086820148e-05\n",
      "Step: 1070, train/epoch: 0.2546406388282776\n",
      "Step: 1080, train/loss: 0.0\n",
      "Step: 1080, train/grad_norm: 2.013366806608019e-09\n",
      "Step: 1080, train/learning_rate: 4.571632598526776e-05\n",
      "Step: 1080, train/epoch: 0.2570204734802246\n",
      "Step: 1090, train/loss: 0.0\n",
      "Step: 1090, train/grad_norm: 1.0479547007946621e-08\n",
      "Step: 1090, train/learning_rate: 4.567666110233404e-05\n",
      "Step: 1090, train/epoch: 0.25940027832984924\n",
      "Step: 1100, train/loss: 0.40630000829696655\n",
      "Step: 1100, train/grad_norm: 2.6935971675356996e-08\n",
      "Step: 1100, train/learning_rate: 4.5636999857379124e-05\n",
      "Step: 1100, train/epoch: 0.26178011298179626\n",
      "Step: 1110, train/loss: 0.0\n",
      "Step: 1110, train/grad_norm: 6.880199634906603e-06\n",
      "Step: 1110, train/learning_rate: 4.55973349744454e-05\n",
      "Step: 1110, train/epoch: 0.2641599178314209\n",
      "Step: 1120, train/loss: 0.3970000147819519\n",
      "Step: 1120, train/grad_norm: 2.8246204237802885e-05\n",
      "Step: 1120, train/learning_rate: 4.555767009151168e-05\n",
      "Step: 1120, train/epoch: 0.2665397524833679\n",
      "Step: 1130, train/loss: 0.001500000013038516\n",
      "Step: 1130, train/grad_norm: 1.0496754290420518e-12\n",
      "Step: 1130, train/learning_rate: 4.551800884655677e-05\n",
      "Step: 1130, train/epoch: 0.26891955733299255\n",
      "Step: 1140, train/loss: 0.0\n",
      "Step: 1140, train/grad_norm: 1.428208634024486e-06\n",
      "Step: 1140, train/learning_rate: 4.547834396362305e-05\n",
      "Step: 1140, train/epoch: 0.2712993919849396\n",
      "Step: 1150, train/loss: 9.999999747378752e-05\n",
      "Step: 1150, train/grad_norm: 1.5232280929922126e-05\n",
      "Step: 1150, train/learning_rate: 4.5438679080689326e-05\n",
      "Step: 1150, train/epoch: 0.2736791968345642\n",
      "Step: 1160, train/loss: 0.0\n",
      "Step: 1160, train/grad_norm: 2.3006447236184613e-07\n",
      "Step: 1160, train/learning_rate: 4.539901783573441e-05\n",
      "Step: 1160, train/epoch: 0.27605903148651123\n",
      "Step: 1170, train/loss: 0.0\n",
      "Step: 1170, train/grad_norm: 2.6309035661142843e-07\n",
      "Step: 1170, train/learning_rate: 4.535935295280069e-05\n",
      "Step: 1170, train/epoch: 0.27843883633613586\n",
      "Step: 1180, train/loss: 0.0\n",
      "Step: 1180, train/grad_norm: 0.0001244871673407033\n",
      "Step: 1180, train/learning_rate: 4.531968806986697e-05\n",
      "Step: 1180, train/epoch: 0.2808186709880829\n",
      "Step: 1190, train/loss: 0.0\n",
      "Step: 1190, train/grad_norm: 1.8155298675992526e-05\n",
      "Step: 1190, train/learning_rate: 4.5280026824912056e-05\n",
      "Step: 1190, train/epoch: 0.2831984758377075\n",
      "Step: 1200, train/loss: 0.0\n",
      "Step: 1200, train/grad_norm: 7.417914060781072e-10\n",
      "Step: 1200, train/learning_rate: 4.5240361941978335e-05\n",
      "Step: 1200, train/epoch: 0.28557831048965454\n",
      "Step: 1210, train/loss: 0.0\n",
      "Step: 1210, train/grad_norm: 1.9132668967358768e-05\n",
      "Step: 1210, train/learning_rate: 4.5200697059044614e-05\n",
      "Step: 1210, train/epoch: 0.2879581153392792\n",
      "Step: 1220, train/loss: 0.0\n",
      "Step: 1220, train/grad_norm: 4.0630779296613184e-10\n",
      "Step: 1220, train/learning_rate: 4.51610358140897e-05\n",
      "Step: 1220, train/epoch: 0.2903379201889038\n",
      "Step: 1230, train/loss: 0.0\n",
      "Step: 1230, train/grad_norm: 0.00013720161223318428\n",
      "Step: 1230, train/learning_rate: 4.512137093115598e-05\n",
      "Step: 1230, train/epoch: 0.29271775484085083\n",
      "Step: 1240, train/loss: 0.04259999841451645\n",
      "Step: 1240, train/grad_norm: 1.3330393455746048e-09\n",
      "Step: 1240, train/learning_rate: 4.508170604822226e-05\n",
      "Step: 1240, train/epoch: 0.29509755969047546\n",
      "Step: 1250, train/loss: 0.875\n",
      "Step: 1250, train/grad_norm: 0.00021932860545348376\n",
      "Step: 1250, train/learning_rate: 4.5042044803267345e-05\n",
      "Step: 1250, train/epoch: 0.2974773943424225\n",
      "Step: 1260, train/loss: 0.0\n",
      "Step: 1260, train/grad_norm: 3.9937699369829716e-08\n",
      "Step: 1260, train/learning_rate: 4.5002379920333624e-05\n",
      "Step: 1260, train/epoch: 0.2998571991920471\n",
      "Step: 1270, train/loss: 0.0\n",
      "Step: 1270, train/grad_norm: 7.668347279832233e-06\n",
      "Step: 1270, train/learning_rate: 4.49627150373999e-05\n",
      "Step: 1270, train/epoch: 0.30223703384399414\n",
      "Step: 1280, train/loss: 0.0\n",
      "Step: 1280, train/grad_norm: 3.766507461477886e-08\n",
      "Step: 1280, train/learning_rate: 4.492305379244499e-05\n",
      "Step: 1280, train/epoch: 0.3046168386936188\n",
      "Step: 1290, train/loss: 9.999999747378752e-05\n",
      "Step: 1290, train/grad_norm: 2.9662078304681927e-07\n",
      "Step: 1290, train/learning_rate: 4.488338890951127e-05\n",
      "Step: 1290, train/epoch: 0.3069966733455658\n",
      "Step: 1300, train/loss: 0.0\n",
      "Step: 1300, train/grad_norm: 3.815930682549151e-09\n",
      "Step: 1300, train/learning_rate: 4.484372402657755e-05\n",
      "Step: 1300, train/epoch: 0.30937647819519043\n",
      "Step: 1310, train/loss: 0.0\n",
      "Step: 1310, train/grad_norm: 0.00013273397053126246\n",
      "Step: 1310, train/learning_rate: 4.480406278162263e-05\n",
      "Step: 1310, train/epoch: 0.31175631284713745\n",
      "Step: 1320, train/loss: 0.0\n",
      "Step: 1320, train/grad_norm: 1.3535301682295205e-15\n",
      "Step: 1320, train/learning_rate: 4.476439789868891e-05\n",
      "Step: 1320, train/epoch: 0.3141361176967621\n",
      "Step: 1330, train/loss: 0.0\n",
      "Step: 1330, train/grad_norm: 1.736996455292683e-05\n",
      "Step: 1330, train/learning_rate: 4.472473301575519e-05\n",
      "Step: 1330, train/epoch: 0.3165159523487091\n",
      "Step: 1340, train/loss: 0.0\n",
      "Step: 1340, train/grad_norm: 2.900751860934747e-11\n",
      "Step: 1340, train/learning_rate: 4.468507177080028e-05\n",
      "Step: 1340, train/epoch: 0.31889575719833374\n",
      "Step: 1350, train/loss: 0.17499999701976776\n",
      "Step: 1350, train/grad_norm: 3.024737438295233e-08\n",
      "Step: 1350, train/learning_rate: 4.464540688786656e-05\n",
      "Step: 1350, train/epoch: 0.32127559185028076\n",
      "Step: 1360, train/loss: 0.0\n",
      "Step: 1360, train/grad_norm: 1.987829591598711e-06\n",
      "Step: 1360, train/learning_rate: 4.4605742004932836e-05\n",
      "Step: 1360, train/epoch: 0.3236553966999054\n",
      "Step: 1370, train/loss: 0.0\n",
      "Step: 1370, train/grad_norm: 2.710773742364836e-06\n",
      "Step: 1370, train/learning_rate: 4.456608075997792e-05\n",
      "Step: 1370, train/epoch: 0.3260352313518524\n",
      "Step: 1380, train/loss: 0.0\n",
      "Step: 1380, train/grad_norm: 6.652023876085877e-05\n",
      "Step: 1380, train/learning_rate: 4.45264158770442e-05\n",
      "Step: 1380, train/epoch: 0.32841503620147705\n",
      "Step: 1390, train/loss: 0.0\n",
      "Step: 1390, train/grad_norm: 0.00021579077292699367\n",
      "Step: 1390, train/learning_rate: 4.448675099411048e-05\n",
      "Step: 1390, train/epoch: 0.3307948708534241\n",
      "Step: 1400, train/loss: 0.0\n",
      "Step: 1400, train/grad_norm: 0.00018191429262515157\n",
      "Step: 1400, train/learning_rate: 4.4447089749155566e-05\n",
      "Step: 1400, train/epoch: 0.3331746757030487\n",
      "Step: 1410, train/loss: 0.0\n",
      "Step: 1410, train/grad_norm: 1.026548019922302e-07\n",
      "Step: 1410, train/learning_rate: 4.4407424866221845e-05\n",
      "Step: 1410, train/epoch: 0.3355545103549957\n",
      "Step: 1420, train/loss: 0.0\n",
      "Step: 1420, train/grad_norm: 2.1689554614567896e-06\n",
      "Step: 1420, train/learning_rate: 4.4367759983288124e-05\n",
      "Step: 1420, train/epoch: 0.33793431520462036\n",
      "Step: 1430, train/loss: 0.18129999935626984\n",
      "Step: 1430, train/grad_norm: 0.000500630761962384\n",
      "Step: 1430, train/learning_rate: 4.432809873833321e-05\n",
      "Step: 1430, train/epoch: 0.3403141498565674\n",
      "Step: 1440, train/loss: 0.0\n",
      "Step: 1440, train/grad_norm: 4.258022272551898e-06\n",
      "Step: 1440, train/learning_rate: 4.428843385539949e-05\n",
      "Step: 1440, train/epoch: 0.342693954706192\n",
      "Step: 1450, train/loss: 0.0\n",
      "Step: 1450, train/grad_norm: 0.00029453440220095217\n",
      "Step: 1450, train/learning_rate: 4.424876897246577e-05\n",
      "Step: 1450, train/epoch: 0.34507375955581665\n",
      "Step: 1460, train/loss: 0.0\n",
      "Step: 1460, train/grad_norm: 0.04475080221891403\n",
      "Step: 1460, train/learning_rate: 4.4209107727510855e-05\n",
      "Step: 1460, train/epoch: 0.34745359420776367\n",
      "Step: 1470, train/loss: 0.00019999999494757503\n",
      "Step: 1470, train/grad_norm: 7.594234574526126e-08\n",
      "Step: 1470, train/learning_rate: 4.4169442844577134e-05\n",
      "Step: 1470, train/epoch: 0.3498333990573883\n",
      "Step: 1480, train/loss: 0.0\n",
      "Step: 1480, train/grad_norm: 1.0180766819856135e-12\n",
      "Step: 1480, train/learning_rate: 4.412977796164341e-05\n",
      "Step: 1480, train/epoch: 0.3522132337093353\n",
      "Step: 1490, train/loss: 0.11879999935626984\n",
      "Step: 1490, train/grad_norm: 1.6114560139612877e-06\n",
      "Step: 1490, train/learning_rate: 4.40901167166885e-05\n",
      "Step: 1490, train/epoch: 0.35459303855895996\n",
      "Step: 1500, train/loss: 0.0\n",
      "Step: 1500, train/grad_norm: 3.3601217808154615e-08\n",
      "Step: 1500, train/learning_rate: 4.405045183375478e-05\n",
      "Step: 1500, train/epoch: 0.356972873210907\n",
      "Step: 1510, train/loss: 0.0\n",
      "Step: 1510, train/grad_norm: 5.706582442144281e-07\n",
      "Step: 1510, train/learning_rate: 4.401078695082106e-05\n",
      "Step: 1510, train/epoch: 0.3593526780605316\n",
      "Step: 1520, train/loss: 0.08900000154972076\n",
      "Step: 1520, train/grad_norm: 3.331585645675659\n",
      "Step: 1520, train/learning_rate: 4.397112570586614e-05\n",
      "Step: 1520, train/epoch: 0.36173251271247864\n",
      "Step: 1530, train/loss: 0.0\n",
      "Step: 1530, train/grad_norm: 2.3778746083280566e-08\n",
      "Step: 1530, train/learning_rate: 4.393146082293242e-05\n",
      "Step: 1530, train/epoch: 0.36411231756210327\n",
      "Step: 1540, train/loss: 0.2651999890804291\n",
      "Step: 1540, train/grad_norm: 1.6008620150387287e-05\n",
      "Step: 1540, train/learning_rate: 4.38917959399987e-05\n",
      "Step: 1540, train/epoch: 0.3664921522140503\n",
      "Step: 1550, train/loss: 0.13220000267028809\n",
      "Step: 1550, train/grad_norm: 0.8399732112884521\n",
      "Step: 1550, train/learning_rate: 4.385213469504379e-05\n",
      "Step: 1550, train/epoch: 0.3688719570636749\n",
      "Step: 1560, train/loss: 0.0\n",
      "Step: 1560, train/grad_norm: 9.318024240201339e-06\n",
      "Step: 1560, train/learning_rate: 4.3812469812110066e-05\n",
      "Step: 1560, train/epoch: 0.37125179171562195\n",
      "Step: 1570, train/loss: 0.0\n",
      "Step: 1570, train/grad_norm: 0.0035763895139098167\n",
      "Step: 1570, train/learning_rate: 4.3772804929176345e-05\n",
      "Step: 1570, train/epoch: 0.3736315965652466\n",
      "Step: 1580, train/loss: 0.05429999902844429\n",
      "Step: 1580, train/grad_norm: 5.515230441233143e-05\n",
      "Step: 1580, train/learning_rate: 4.373314368422143e-05\n",
      "Step: 1580, train/epoch: 0.3760114312171936\n",
      "Step: 1590, train/loss: 9.999999747378752e-05\n",
      "Step: 1590, train/grad_norm: 0.00013110059080645442\n",
      "Step: 1590, train/learning_rate: 4.369347880128771e-05\n",
      "Step: 1590, train/epoch: 0.37839123606681824\n",
      "Step: 1600, train/loss: 0.0\n",
      "Step: 1600, train/grad_norm: 3.722900873981416e-05\n",
      "Step: 1600, train/learning_rate: 4.365381391835399e-05\n",
      "Step: 1600, train/epoch: 0.38077107071876526\n",
      "Step: 1610, train/loss: 0.0\n",
      "Step: 1610, train/grad_norm: 8.570996214984916e-06\n",
      "Step: 1610, train/learning_rate: 4.3614152673399076e-05\n",
      "Step: 1610, train/epoch: 0.3831508755683899\n",
      "Step: 1620, train/loss: 0.0\n",
      "Step: 1620, train/grad_norm: 6.92587391881716e-08\n",
      "Step: 1620, train/learning_rate: 4.3574487790465355e-05\n",
      "Step: 1620, train/epoch: 0.3855307102203369\n",
      "Step: 1630, train/loss: 0.0\n",
      "Step: 1630, train/grad_norm: 8.241559407906607e-05\n",
      "Step: 1630, train/learning_rate: 4.3534822907531634e-05\n",
      "Step: 1630, train/epoch: 0.38791051506996155\n",
      "Step: 1640, train/loss: 0.0\n",
      "Step: 1640, train/grad_norm: 0.0017485626740381122\n",
      "Step: 1640, train/learning_rate: 4.349516166257672e-05\n",
      "Step: 1640, train/epoch: 0.39029034972190857\n",
      "Step: 1650, train/loss: 0.0\n",
      "Step: 1650, train/grad_norm: 3.3502328022905203e-08\n",
      "Step: 1650, train/learning_rate: 4.3455496779643e-05\n",
      "Step: 1650, train/epoch: 0.3926701545715332\n",
      "Step: 1660, train/loss: 0.0\n",
      "Step: 1660, train/grad_norm: 8.000197703950107e-05\n",
      "Step: 1660, train/learning_rate: 4.3415835534688085e-05\n",
      "Step: 1660, train/epoch: 0.3950499892234802\n",
      "Step: 1670, train/loss: 0.07660000026226044\n",
      "Step: 1670, train/grad_norm: 0.00010673264478100464\n",
      "Step: 1670, train/learning_rate: 4.3376170651754364e-05\n",
      "Step: 1670, train/epoch: 0.39742979407310486\n",
      "Step: 1680, train/loss: 0.0\n",
      "Step: 1680, train/grad_norm: 0.0003215813485439867\n",
      "Step: 1680, train/learning_rate: 4.3336505768820643e-05\n",
      "Step: 1680, train/epoch: 0.3998096287250519\n",
      "Step: 1690, train/loss: 0.0019000000320374966\n",
      "Step: 1690, train/grad_norm: 5.754634548793547e-05\n",
      "Step: 1690, train/learning_rate: 4.329684452386573e-05\n",
      "Step: 1690, train/epoch: 0.4021894335746765\n",
      "Step: 1700, train/loss: 0.0\n",
      "Step: 1700, train/grad_norm: 7.571131277472887e-09\n",
      "Step: 1700, train/learning_rate: 4.325717964093201e-05\n",
      "Step: 1700, train/epoch: 0.40456923842430115\n",
      "Step: 1710, train/loss: 0.03889999911189079\n",
      "Step: 1710, train/grad_norm: 0.00027528006467036903\n",
      "Step: 1710, train/learning_rate: 4.321751475799829e-05\n",
      "Step: 1710, train/epoch: 0.40694907307624817\n",
      "Step: 1720, train/loss: 0.0\n",
      "Step: 1720, train/grad_norm: 9.176099524665915e-07\n",
      "Step: 1720, train/learning_rate: 4.3177853513043374e-05\n",
      "Step: 1720, train/epoch: 0.4093288779258728\n",
      "Step: 1730, train/loss: 0.0\n",
      "Step: 1730, train/grad_norm: 0.0014766943641006947\n",
      "Step: 1730, train/learning_rate: 4.313818863010965e-05\n",
      "Step: 1730, train/epoch: 0.4117087125778198\n",
      "Step: 1740, train/loss: 0.0\n",
      "Step: 1740, train/grad_norm: 0.024401415139436722\n",
      "Step: 1740, train/learning_rate: 4.309852374717593e-05\n",
      "Step: 1740, train/epoch: 0.41408851742744446\n",
      "Step: 1750, train/loss: 0.0\n",
      "Step: 1750, train/grad_norm: 9.290769820147204e-10\n",
      "Step: 1750, train/learning_rate: 4.305886250222102e-05\n",
      "Step: 1750, train/epoch: 0.4164683520793915\n",
      "Step: 1760, train/loss: 0.0\n",
      "Step: 1760, train/grad_norm: 9.43928534979932e-06\n",
      "Step: 1760, train/learning_rate: 4.30191976192873e-05\n",
      "Step: 1760, train/epoch: 0.4188481569290161\n",
      "Step: 1770, train/loss: 0.0\n",
      "Step: 1770, train/grad_norm: 2.5738816589182534e-07\n",
      "Step: 1770, train/learning_rate: 4.2979532736353576e-05\n",
      "Step: 1770, train/epoch: 0.42122799158096313\n",
      "Step: 1780, train/loss: 0.0\n",
      "Step: 1780, train/grad_norm: 2.350769534587016e-07\n",
      "Step: 1780, train/learning_rate: 4.293987149139866e-05\n",
      "Step: 1780, train/epoch: 0.42360779643058777\n",
      "Step: 1790, train/loss: 0.0\n",
      "Step: 1790, train/grad_norm: 7.952100089525516e-11\n",
      "Step: 1790, train/learning_rate: 4.290020660846494e-05\n",
      "Step: 1790, train/epoch: 0.4259876310825348\n",
      "Step: 1800, train/loss: 0.0\n",
      "Step: 1800, train/grad_norm: 0.003071761457249522\n",
      "Step: 1800, train/learning_rate: 4.286054172553122e-05\n",
      "Step: 1800, train/epoch: 0.4283674359321594\n",
      "Step: 1810, train/loss: 0.0\n",
      "Step: 1810, train/grad_norm: 1.2069109045498294e-09\n",
      "Step: 1810, train/learning_rate: 4.2820880480576307e-05\n",
      "Step: 1810, train/epoch: 0.43074727058410645\n",
      "Step: 1820, train/loss: 0.3375000059604645\n",
      "Step: 1820, train/grad_norm: 95.16224670410156\n",
      "Step: 1820, train/learning_rate: 4.2781215597642586e-05\n",
      "Step: 1820, train/epoch: 0.4331270754337311\n",
      "Step: 1830, train/loss: 0.0\n",
      "Step: 1830, train/grad_norm: 1.9733807686897475e-13\n",
      "Step: 1830, train/learning_rate: 4.2741550714708865e-05\n",
      "Step: 1830, train/epoch: 0.4355069100856781\n",
      "Step: 1840, train/loss: 0.0\n",
      "Step: 1840, train/grad_norm: 0.05457652360200882\n",
      "Step: 1840, train/learning_rate: 4.270188946975395e-05\n",
      "Step: 1840, train/epoch: 0.43788671493530273\n",
      "Step: 1850, train/loss: 0.0\n",
      "Step: 1850, train/grad_norm: 1.9495848491146717e-08\n",
      "Step: 1850, train/learning_rate: 4.266222458682023e-05\n",
      "Step: 1850, train/epoch: 0.44026654958724976\n",
      "Step: 1860, train/loss: 0.0\n",
      "Step: 1860, train/grad_norm: 3.299124211667004e-13\n",
      "Step: 1860, train/learning_rate: 4.262255970388651e-05\n",
      "Step: 1860, train/epoch: 0.4426463544368744\n",
      "Step: 1870, train/loss: 0.0\n",
      "Step: 1870, train/grad_norm: 4.392497650329563e-12\n",
      "Step: 1870, train/learning_rate: 4.2582898458931595e-05\n",
      "Step: 1870, train/epoch: 0.4450261890888214\n",
      "Step: 1880, train/loss: 0.0\n",
      "Step: 1880, train/grad_norm: 2.1861421006796888e-10\n",
      "Step: 1880, train/learning_rate: 4.2543233575997874e-05\n",
      "Step: 1880, train/epoch: 0.44740599393844604\n",
      "Step: 1890, train/loss: 0.0\n",
      "Step: 1890, train/grad_norm: 3.4034972756780135e-11\n",
      "Step: 1890, train/learning_rate: 4.250356869306415e-05\n",
      "Step: 1890, train/epoch: 0.44978582859039307\n",
      "Step: 1900, train/loss: 0.0\n",
      "Step: 1900, train/grad_norm: 2.1143969206605107e-05\n",
      "Step: 1900, train/learning_rate: 4.246390744810924e-05\n",
      "Step: 1900, train/epoch: 0.4521656334400177\n",
      "Step: 1910, train/loss: 0.0\n",
      "Step: 1910, train/grad_norm: 1.488510577019042e-07\n",
      "Step: 1910, train/learning_rate: 4.242424256517552e-05\n",
      "Step: 1910, train/epoch: 0.4545454680919647\n",
      "Step: 1920, train/loss: 0.0\n",
      "Step: 1920, train/grad_norm: 9.055458748304102e-13\n",
      "Step: 1920, train/learning_rate: 4.23845776822418e-05\n",
      "Step: 1920, train/epoch: 0.45692527294158936\n",
      "Step: 1930, train/loss: 0.0\n",
      "Step: 1930, train/grad_norm: 6.3337370070915e-11\n",
      "Step: 1930, train/learning_rate: 4.2344916437286884e-05\n",
      "Step: 1930, train/epoch: 0.4593051075935364\n",
      "Step: 1940, train/loss: 0.0\n",
      "Step: 1940, train/grad_norm: 1.840328764046717e-06\n",
      "Step: 1940, train/learning_rate: 4.230525155435316e-05\n",
      "Step: 1940, train/epoch: 0.461684912443161\n",
      "Step: 1950, train/loss: 0.0\n",
      "Step: 1950, train/grad_norm: 3.141433077211153e-12\n",
      "Step: 1950, train/learning_rate: 4.226558667141944e-05\n",
      "Step: 1950, train/epoch: 0.46406471729278564\n",
      "Step: 1960, train/loss: 0.0\n",
      "Step: 1960, train/grad_norm: 1.0423335456609612e-15\n",
      "Step: 1960, train/learning_rate: 4.222592542646453e-05\n",
      "Step: 1960, train/epoch: 0.46644455194473267\n",
      "Step: 1970, train/loss: 0.0\n",
      "Step: 1970, train/grad_norm: 6.890230724820867e-05\n",
      "Step: 1970, train/learning_rate: 4.218626054353081e-05\n",
      "Step: 1970, train/epoch: 0.4688243567943573\n",
      "Step: 1980, train/loss: 0.0\n",
      "Step: 1980, train/grad_norm: 1.6630180565968544e-09\n",
      "Step: 1980, train/learning_rate: 4.2146595660597086e-05\n",
      "Step: 1980, train/epoch: 0.4712041914463043\n",
      "Step: 1990, train/loss: 0.08709999918937683\n",
      "Step: 1990, train/grad_norm: 1.4187383212682647e-10\n",
      "Step: 1990, train/learning_rate: 4.210693441564217e-05\n",
      "Step: 1990, train/epoch: 0.47358399629592896\n",
      "Step: 2000, train/loss: 0.0\n",
      "Step: 2000, train/grad_norm: 5.661195512374206e-09\n",
      "Step: 2000, train/learning_rate: 4.206726953270845e-05\n",
      "Step: 2000, train/epoch: 0.475963830947876\n",
      "Step: 2010, train/loss: 0.3594000041484833\n",
      "Step: 2010, train/grad_norm: 1.493405754615651e-08\n",
      "Step: 2010, train/learning_rate: 4.202760464977473e-05\n",
      "Step: 2010, train/epoch: 0.4783436357975006\n",
      "Step: 2020, train/loss: 0.0\n",
      "Step: 2020, train/grad_norm: 9.718744564679582e-08\n",
      "Step: 2020, train/learning_rate: 4.1987943404819816e-05\n",
      "Step: 2020, train/epoch: 0.48072347044944763\n",
      "Step: 2030, train/loss: 0.0\n",
      "Step: 2030, train/grad_norm: 2.311203957106045e-07\n",
      "Step: 2030, train/learning_rate: 4.1948278521886095e-05\n",
      "Step: 2030, train/epoch: 0.48310327529907227\n",
      "Step: 2040, train/loss: 0.0\n",
      "Step: 2040, train/grad_norm: 0.00023250024241860956\n",
      "Step: 2040, train/learning_rate: 4.1908613638952374e-05\n",
      "Step: 2040, train/epoch: 0.4854831099510193\n",
      "Step: 2050, train/loss: 0.0\n",
      "Step: 2050, train/grad_norm: 0.00012624940427485853\n",
      "Step: 2050, train/learning_rate: 4.186895239399746e-05\n",
      "Step: 2050, train/epoch: 0.4878629148006439\n",
      "Step: 2060, train/loss: 0.0\n",
      "Step: 2060, train/grad_norm: 0.00041267421329393983\n",
      "Step: 2060, train/learning_rate: 4.182928751106374e-05\n",
      "Step: 2060, train/epoch: 0.49024274945259094\n",
      "Step: 2070, train/loss: 0.0\n",
      "Step: 2070, train/grad_norm: 0.0002626688510645181\n",
      "Step: 2070, train/learning_rate: 4.178962262813002e-05\n",
      "Step: 2070, train/epoch: 0.4926225543022156\n",
      "Step: 2080, train/loss: 0.0\n",
      "Step: 2080, train/grad_norm: 2.0137081264692824e-06\n",
      "Step: 2080, train/learning_rate: 4.1749961383175105e-05\n",
      "Step: 2080, train/epoch: 0.4950023889541626\n",
      "Step: 2090, train/loss: 0.0\n",
      "Step: 2090, train/grad_norm: 0.001253969967365265\n",
      "Step: 2090, train/learning_rate: 4.1710296500241384e-05\n",
      "Step: 2090, train/epoch: 0.49738219380378723\n",
      "Step: 2100, train/loss: 0.12030000239610672\n",
      "Step: 2100, train/grad_norm: 3.6895603017228495e-08\n",
      "Step: 2100, train/learning_rate: 4.167063161730766e-05\n",
      "Step: 2100, train/epoch: 0.49976202845573425\n",
      "Step: 2110, train/loss: 0.0\n",
      "Step: 2110, train/grad_norm: 0.000814175873529166\n",
      "Step: 2110, train/learning_rate: 4.163097037235275e-05\n",
      "Step: 2110, train/epoch: 0.5021418333053589\n",
      "Step: 2120, train/loss: 0.6812999844551086\n",
      "Step: 2120, train/grad_norm: 187.89027404785156\n",
      "Step: 2120, train/learning_rate: 4.159130548941903e-05\n",
      "Step: 2120, train/epoch: 0.5045216679573059\n",
      "Step: 2130, train/loss: 0.0007999999797903001\n",
      "Step: 2130, train/grad_norm: 0.0001054302483680658\n",
      "Step: 2130, train/learning_rate: 4.155164060648531e-05\n",
      "Step: 2130, train/epoch: 0.5069015026092529\n",
      "Step: 2140, train/loss: 0.0\n",
      "Step: 2140, train/grad_norm: 7.117811037460342e-05\n",
      "Step: 2140, train/learning_rate: 4.151197936153039e-05\n",
      "Step: 2140, train/epoch: 0.5092812776565552\n",
      "Step: 2150, train/loss: 0.0\n",
      "Step: 2150, train/grad_norm: 1.1098240975115914e-06\n",
      "Step: 2150, train/learning_rate: 4.147231447859667e-05\n",
      "Step: 2150, train/epoch: 0.5116611123085022\n",
      "Step: 2160, train/loss: 0.0\n",
      "Step: 2160, train/grad_norm: 1.0984076652675867e-05\n",
      "Step: 2160, train/learning_rate: 4.143264959566295e-05\n",
      "Step: 2160, train/epoch: 0.5140409469604492\n",
      "Step: 2170, train/loss: 0.0\n",
      "Step: 2170, train/grad_norm: 1.3283063537983253e-07\n",
      "Step: 2170, train/learning_rate: 4.139298835070804e-05\n",
      "Step: 2170, train/epoch: 0.5164207816123962\n",
      "Step: 2180, train/loss: 0.0\n",
      "Step: 2180, train/grad_norm: 1.119953765282844e-07\n",
      "Step: 2180, train/learning_rate: 4.135332346777432e-05\n",
      "Step: 2180, train/epoch: 0.5188005566596985\n",
      "Step: 2190, train/loss: 0.0\n",
      "Step: 2190, train/grad_norm: 9.408379355591023e-07\n",
      "Step: 2190, train/learning_rate: 4.1313658584840596e-05\n",
      "Step: 2190, train/epoch: 0.5211803913116455\n",
      "Step: 2200, train/loss: 0.0\n",
      "Step: 2200, train/grad_norm: 0.0008945965673774481\n",
      "Step: 2200, train/learning_rate: 4.127399733988568e-05\n",
      "Step: 2200, train/epoch: 0.5235602259635925\n",
      "Step: 2210, train/loss: 0.0020000000949949026\n",
      "Step: 2210, train/grad_norm: 4.282144860923154e-09\n",
      "Step: 2210, train/learning_rate: 4.123433245695196e-05\n",
      "Step: 2210, train/epoch: 0.5259400010108948\n",
      "Step: 2220, train/loss: 0.0\n",
      "Step: 2220, train/grad_norm: 2.2101797725326833e-08\n",
      "Step: 2220, train/learning_rate: 4.119466757401824e-05\n",
      "Step: 2220, train/epoch: 0.5283198356628418\n",
      "Step: 2230, train/loss: 0.0\n",
      "Step: 2230, train/grad_norm: 0.0031162125524133444\n",
      "Step: 2230, train/learning_rate: 4.1155006329063326e-05\n",
      "Step: 2230, train/epoch: 0.5306996703147888\n",
      "Step: 2240, train/loss: 0.0\n",
      "Step: 2240, train/grad_norm: 1.8412809367873706e-05\n",
      "Step: 2240, train/learning_rate: 4.1115341446129605e-05\n",
      "Step: 2240, train/epoch: 0.5330795049667358\n",
      "Step: 2250, train/loss: 0.0\n",
      "Step: 2250, train/grad_norm: 0.0008782357908785343\n",
      "Step: 2250, train/learning_rate: 4.1075676563195884e-05\n",
      "Step: 2250, train/epoch: 0.5354592800140381\n",
      "Step: 2260, train/loss: 0.0\n",
      "Step: 2260, train/grad_norm: 1.1271679145608005e-09\n",
      "Step: 2260, train/learning_rate: 4.103601531824097e-05\n",
      "Step: 2260, train/epoch: 0.5378391146659851\n",
      "Step: 2270, train/loss: 0.0\n",
      "Step: 2270, train/grad_norm: 2.8772330551873893e-05\n",
      "Step: 2270, train/learning_rate: 4.099635043530725e-05\n",
      "Step: 2270, train/epoch: 0.5402189493179321\n",
      "Step: 2280, train/loss: 0.0\n",
      "Step: 2280, train/grad_norm: 1.755286029947456e-05\n",
      "Step: 2280, train/learning_rate: 4.095668555237353e-05\n",
      "Step: 2280, train/epoch: 0.5425987839698792\n",
      "Step: 2290, train/loss: 0.0\n",
      "Step: 2290, train/grad_norm: 4.1367496095290335e-08\n",
      "Step: 2290, train/learning_rate: 4.0917024307418615e-05\n",
      "Step: 2290, train/epoch: 0.5449785590171814\n",
      "Step: 2300, train/loss: 0.0\n",
      "Step: 2300, train/grad_norm: 1.2017866310998215e-06\n",
      "Step: 2300, train/learning_rate: 4.0877359424484894e-05\n",
      "Step: 2300, train/epoch: 0.5473583936691284\n",
      "Step: 2310, train/loss: 0.0\n",
      "Step: 2310, train/grad_norm: 7.822696716175415e-06\n",
      "Step: 2310, train/learning_rate: 4.083769454155117e-05\n",
      "Step: 2310, train/epoch: 0.5497382283210754\n",
      "Step: 2320, train/loss: 0.0617000013589859\n",
      "Step: 2320, train/grad_norm: 7.3085107032966334e-06\n",
      "Step: 2320, train/learning_rate: 4.079803329659626e-05\n",
      "Step: 2320, train/epoch: 0.5521180629730225\n",
      "Step: 2330, train/loss: 0.0\n",
      "Step: 2330, train/grad_norm: 2.3325295472886864e-08\n",
      "Step: 2330, train/learning_rate: 4.075836841366254e-05\n",
      "Step: 2330, train/epoch: 0.5544978380203247\n",
      "Step: 2340, train/loss: 0.0\n",
      "Step: 2340, train/grad_norm: 6.054600021343504e-08\n",
      "Step: 2340, train/learning_rate: 4.0718707168707624e-05\n",
      "Step: 2340, train/epoch: 0.5568776726722717\n",
      "Step: 2350, train/loss: 0.0\n",
      "Step: 2350, train/grad_norm: 8.365283576949878e-09\n",
      "Step: 2350, train/learning_rate: 4.06790422857739e-05\n",
      "Step: 2350, train/epoch: 0.5592575073242188\n",
      "Step: 2360, train/loss: 0.0\n",
      "Step: 2360, train/grad_norm: 1.6352847751477384e-06\n",
      "Step: 2360, train/learning_rate: 4.063937740284018e-05\n",
      "Step: 2360, train/epoch: 0.5616373419761658\n",
      "Step: 2370, train/loss: 0.0\n",
      "Step: 2370, train/grad_norm: 3.573342288731851e-09\n",
      "Step: 2370, train/learning_rate: 4.059971615788527e-05\n",
      "Step: 2370, train/epoch: 0.564017117023468\n",
      "Step: 2380, train/loss: 0.0\n",
      "Step: 2380, train/grad_norm: 0.003165980102494359\n",
      "Step: 2380, train/learning_rate: 4.056005127495155e-05\n",
      "Step: 2380, train/epoch: 0.566396951675415\n",
      "Step: 2390, train/loss: 0.0\n",
      "Step: 2390, train/grad_norm: 4.811360754253258e-10\n",
      "Step: 2390, train/learning_rate: 4.0520386392017826e-05\n",
      "Step: 2390, train/epoch: 0.5687767863273621\n",
      "Step: 2400, train/loss: 0.0\n",
      "Step: 2400, train/grad_norm: 1.1841093661146829e-09\n",
      "Step: 2400, train/learning_rate: 4.048072514706291e-05\n",
      "Step: 2400, train/epoch: 0.5711566209793091\n",
      "Step: 2410, train/loss: 0.0\n",
      "Step: 2410, train/grad_norm: 3.498376033306272e-10\n",
      "Step: 2410, train/learning_rate: 4.044106026412919e-05\n",
      "Step: 2410, train/epoch: 0.5735363960266113\n",
      "Step: 2420, train/loss: 0.0\n",
      "Step: 2420, train/grad_norm: 7.305199233087478e-06\n",
      "Step: 2420, train/learning_rate: 4.040139538119547e-05\n",
      "Step: 2420, train/epoch: 0.5759162306785583\n",
      "Step: 2430, train/loss: 0.0\n",
      "Step: 2430, train/grad_norm: 2.8142186359048083e-09\n",
      "Step: 2430, train/learning_rate: 4.036173413624056e-05\n",
      "Step: 2430, train/epoch: 0.5782960653305054\n",
      "Step: 2440, train/loss: 0.0\n",
      "Step: 2440, train/grad_norm: 1.0014199885999275e-10\n",
      "Step: 2440, train/learning_rate: 4.0322069253306836e-05\n",
      "Step: 2440, train/epoch: 0.5806758403778076\n",
      "Step: 2450, train/loss: 0.0\n",
      "Step: 2450, train/grad_norm: 6.1490383814089e-05\n",
      "Step: 2450, train/learning_rate: 4.0282404370373115e-05\n",
      "Step: 2450, train/epoch: 0.5830556750297546\n",
      "Step: 2460, train/loss: 0.0\n",
      "Step: 2460, train/grad_norm: 0.0002809478319250047\n",
      "Step: 2460, train/learning_rate: 4.02427431254182e-05\n",
      "Step: 2460, train/epoch: 0.5854355096817017\n",
      "Step: 2470, train/loss: 0.0\n",
      "Step: 2470, train/grad_norm: 3.6743269447470084e-05\n",
      "Step: 2470, train/learning_rate: 4.020307824248448e-05\n",
      "Step: 2470, train/epoch: 0.5878153443336487\n",
      "Step: 2480, train/loss: 0.0\n",
      "Step: 2480, train/grad_norm: 2.1529988458723892e-08\n",
      "Step: 2480, train/learning_rate: 4.016341335955076e-05\n",
      "Step: 2480, train/epoch: 0.5901951193809509\n",
      "Step: 2490, train/loss: 0.0\n",
      "Step: 2490, train/grad_norm: 2.2120073595033318e-07\n",
      "Step: 2490, train/learning_rate: 4.0123752114595845e-05\n",
      "Step: 2490, train/epoch: 0.592574954032898\n",
      "Step: 2500, train/loss: 0.0\n",
      "Step: 2500, train/grad_norm: 1.3920036130710134e-11\n",
      "Step: 2500, train/learning_rate: 4.0084087231662124e-05\n",
      "Step: 2500, train/epoch: 0.594954788684845\n",
      "Step: 2510, train/loss: 0.0\n",
      "Step: 2510, train/grad_norm: 4.279248125554219e-13\n",
      "Step: 2510, train/learning_rate: 4.0044422348728403e-05\n",
      "Step: 2510, train/epoch: 0.597334623336792\n",
      "Step: 2520, train/loss: 0.051600001752376556\n",
      "Step: 2520, train/grad_norm: 4.541988729567237e-10\n",
      "Step: 2520, train/learning_rate: 4.000476110377349e-05\n",
      "Step: 2520, train/epoch: 0.5997143983840942\n",
      "Step: 2530, train/loss: 0.0\n",
      "Step: 2530, train/grad_norm: 4.796396058104335e-10\n",
      "Step: 2530, train/learning_rate: 3.996509622083977e-05\n",
      "Step: 2530, train/epoch: 0.6020942330360413\n",
      "Step: 2540, train/loss: 0.0\n",
      "Step: 2540, train/grad_norm: 2.2498208851473578e-10\n",
      "Step: 2540, train/learning_rate: 3.992543133790605e-05\n",
      "Step: 2540, train/epoch: 0.6044740676879883\n",
      "Step: 2550, train/loss: 0.0\n",
      "Step: 2550, train/grad_norm: 7.427793909524122e-16\n",
      "Step: 2550, train/learning_rate: 3.9885770092951134e-05\n",
      "Step: 2550, train/epoch: 0.6068539023399353\n",
      "Step: 2560, train/loss: 0.06599999964237213\n",
      "Step: 2560, train/grad_norm: 3.245314444694919e-13\n",
      "Step: 2560, train/learning_rate: 3.984610521001741e-05\n",
      "Step: 2560, train/epoch: 0.6092336773872375\n",
      "Step: 2570, train/loss: 0.5248000025749207\n",
      "Step: 2570, train/grad_norm: 2.1749124379179426e-14\n",
      "Step: 2570, train/learning_rate: 3.980644032708369e-05\n",
      "Step: 2570, train/epoch: 0.6116135120391846\n",
      "Step: 2580, train/loss: 0.0\n",
      "Step: 2580, train/grad_norm: 6.935452239635945e-14\n",
      "Step: 2580, train/learning_rate: 3.976677908212878e-05\n",
      "Step: 2580, train/epoch: 0.6139933466911316\n",
      "Step: 2590, train/loss: 0.0\n",
      "Step: 2590, train/grad_norm: 7.553200287446771e-10\n",
      "Step: 2590, train/learning_rate: 3.972711419919506e-05\n",
      "Step: 2590, train/epoch: 0.6163731813430786\n",
      "Step: 2600, train/loss: 0.0\n",
      "Step: 2600, train/grad_norm: 1.2955502989853152e-11\n",
      "Step: 2600, train/learning_rate: 3.9687449316261336e-05\n",
      "Step: 2600, train/epoch: 0.6187529563903809\n",
      "Step: 2610, train/loss: 0.0\n",
      "Step: 2610, train/grad_norm: 2.6103912365016413e-09\n",
      "Step: 2610, train/learning_rate: 3.964778807130642e-05\n",
      "Step: 2610, train/epoch: 0.6211327910423279\n",
      "Step: 2620, train/loss: 0.0\n",
      "Step: 2620, train/grad_norm: 1.653253867139881e-09\n",
      "Step: 2620, train/learning_rate: 3.96081231883727e-05\n",
      "Step: 2620, train/epoch: 0.6235126256942749\n",
      "Step: 2630, train/loss: 9.999999747378752e-05\n",
      "Step: 2630, train/grad_norm: 0.004151008557528257\n",
      "Step: 2630, train/learning_rate: 3.956845830543898e-05\n",
      "Step: 2630, train/epoch: 0.6258924603462219\n",
      "Step: 2640, train/loss: 0.0\n",
      "Step: 2640, train/grad_norm: 1.5666258438087888e-12\n",
      "Step: 2640, train/learning_rate: 3.9528797060484067e-05\n",
      "Step: 2640, train/epoch: 0.6282722353935242\n",
      "Step: 2650, train/loss: 0.0\n",
      "Step: 2650, train/grad_norm: 2.705309032347003e-10\n",
      "Step: 2650, train/learning_rate: 3.9489132177550346e-05\n",
      "Step: 2650, train/epoch: 0.6306520700454712\n",
      "Step: 2660, train/loss: 0.0\n",
      "Step: 2660, train/grad_norm: 6.881388969759428e-08\n",
      "Step: 2660, train/learning_rate: 3.9449467294616625e-05\n",
      "Step: 2660, train/epoch: 0.6330319046974182\n",
      "Step: 2670, train/loss: 0.08630000054836273\n",
      "Step: 2670, train/grad_norm: 3.1937115174685005e-09\n",
      "Step: 2670, train/learning_rate: 3.940980604966171e-05\n",
      "Step: 2670, train/epoch: 0.6354116797447205\n",
      "Step: 2680, train/loss: 0.1103999987244606\n",
      "Step: 2680, train/grad_norm: 1.4159091961118975e-06\n",
      "Step: 2680, train/learning_rate: 3.937014116672799e-05\n",
      "Step: 2680, train/epoch: 0.6377915143966675\n",
      "Step: 2690, train/loss: 0.0\n",
      "Step: 2690, train/grad_norm: 1.1994670146009412e-08\n",
      "Step: 2690, train/learning_rate: 3.933047628379427e-05\n",
      "Step: 2690, train/epoch: 0.6401713490486145\n",
      "Step: 2700, train/loss: 0.0\n",
      "Step: 2700, train/grad_norm: 0.00034787238109856844\n",
      "Step: 2700, train/learning_rate: 3.9290815038839355e-05\n",
      "Step: 2700, train/epoch: 0.6425511837005615\n",
      "Step: 2710, train/loss: 0.0\n",
      "Step: 2710, train/grad_norm: 1.4241131607306556e-09\n",
      "Step: 2710, train/learning_rate: 3.9251150155905634e-05\n",
      "Step: 2710, train/epoch: 0.6449309587478638\n",
      "Step: 2720, train/loss: 0.0\n",
      "Step: 2720, train/grad_norm: 2.5102863787651586e-07\n",
      "Step: 2720, train/learning_rate: 3.921148527297191e-05\n",
      "Step: 2720, train/epoch: 0.6473107933998108\n",
      "Step: 2730, train/loss: 0.0\n",
      "Step: 2730, train/grad_norm: 3.776654267095836e-10\n",
      "Step: 2730, train/learning_rate: 3.9171824028017e-05\n",
      "Step: 2730, train/epoch: 0.6496906280517578\n",
      "Step: 2740, train/loss: 0.0\n",
      "Step: 2740, train/grad_norm: 5.129788593905005e-09\n",
      "Step: 2740, train/learning_rate: 3.913215914508328e-05\n",
      "Step: 2740, train/epoch: 0.6520704627037048\n",
      "Step: 2750, train/loss: 0.0\n",
      "Step: 2750, train/grad_norm: 2.453454362694174e-05\n",
      "Step: 2750, train/learning_rate: 3.909249426214956e-05\n",
      "Step: 2750, train/epoch: 0.6544502377510071\n",
      "Step: 2760, train/loss: 0.0\n",
      "Step: 2760, train/grad_norm: 1.3483840710648565e-09\n",
      "Step: 2760, train/learning_rate: 3.9052833017194644e-05\n",
      "Step: 2760, train/epoch: 0.6568300724029541\n",
      "Step: 2770, train/loss: 0.0\n",
      "Step: 2770, train/grad_norm: 1.0775019765318916e-10\n",
      "Step: 2770, train/learning_rate: 3.901316813426092e-05\n",
      "Step: 2770, train/epoch: 0.6592099070549011\n",
      "Step: 2780, train/loss: 0.0003000000142492354\n",
      "Step: 2780, train/grad_norm: 1.9018007516860962\n",
      "Step: 2780, train/learning_rate: 3.89735032513272e-05\n",
      "Step: 2780, train/epoch: 0.6615897417068481\n",
      "Step: 2790, train/loss: 0.0\n",
      "Step: 2790, train/grad_norm: 2.4390348074554424e-10\n",
      "Step: 2790, train/learning_rate: 3.893384200637229e-05\n",
      "Step: 2790, train/epoch: 0.6639695167541504\n",
      "Step: 2800, train/loss: 0.0\n",
      "Step: 2800, train/grad_norm: 5.36944355644664e-09\n",
      "Step: 2800, train/learning_rate: 3.889417712343857e-05\n",
      "Step: 2800, train/epoch: 0.6663493514060974\n",
      "Step: 2810, train/loss: 0.0\n",
      "Step: 2810, train/grad_norm: 9.412374213368935e-10\n",
      "Step: 2810, train/learning_rate: 3.8854512240504846e-05\n",
      "Step: 2810, train/epoch: 0.6687291860580444\n",
      "Step: 2820, train/loss: 0.0\n",
      "Step: 2820, train/grad_norm: 9.051397569237452e-07\n",
      "Step: 2820, train/learning_rate: 3.881485099554993e-05\n",
      "Step: 2820, train/epoch: 0.6711090207099915\n",
      "Step: 2830, train/loss: 0.0\n",
      "Step: 2830, train/grad_norm: 3.778327680565262e-14\n",
      "Step: 2830, train/learning_rate: 3.877518611261621e-05\n",
      "Step: 2830, train/epoch: 0.6734887957572937\n",
      "Step: 2840, train/loss: 0.0\n",
      "Step: 2840, train/grad_norm: 1.0843454845144151e-07\n",
      "Step: 2840, train/learning_rate: 3.873552122968249e-05\n",
      "Step: 2840, train/epoch: 0.6758686304092407\n",
      "Step: 2850, train/loss: 0.0\n",
      "Step: 2850, train/grad_norm: 6.666341784145047e-11\n",
      "Step: 2850, train/learning_rate: 3.8695859984727576e-05\n",
      "Step: 2850, train/epoch: 0.6782484650611877\n",
      "Step: 2860, train/loss: 0.0\n",
      "Step: 2860, train/grad_norm: 2.64805466443363e-09\n",
      "Step: 2860, train/learning_rate: 3.8656195101793855e-05\n",
      "Step: 2860, train/epoch: 0.6806282997131348\n",
      "Step: 2870, train/loss: 0.32190001010894775\n",
      "Step: 2870, train/grad_norm: 0.00047838754835538566\n",
      "Step: 2870, train/learning_rate: 3.8616530218860134e-05\n",
      "Step: 2870, train/epoch: 0.683008074760437\n",
      "Step: 2880, train/loss: 0.18279999494552612\n",
      "Step: 2880, train/grad_norm: 1.3796083386807823e-08\n",
      "Step: 2880, train/learning_rate: 3.857686897390522e-05\n",
      "Step: 2880, train/epoch: 0.685387909412384\n",
      "Step: 2890, train/loss: 0.0\n",
      "Step: 2890, train/grad_norm: 1.9712131232862973e-11\n",
      "Step: 2890, train/learning_rate: 3.85372040909715e-05\n",
      "Step: 2890, train/epoch: 0.687767744064331\n",
      "Step: 2900, train/loss: 0.0\n",
      "Step: 2900, train/grad_norm: 1.3329498393943595e-08\n",
      "Step: 2900, train/learning_rate: 3.849753920803778e-05\n",
      "Step: 2900, train/epoch: 0.6901475191116333\n",
      "Step: 2910, train/loss: 0.0\n",
      "Step: 2910, train/grad_norm: 1.3383734225047572e-11\n",
      "Step: 2910, train/learning_rate: 3.8457877963082865e-05\n",
      "Step: 2910, train/epoch: 0.6925273537635803\n",
      "Step: 2920, train/loss: 0.0\n",
      "Step: 2920, train/grad_norm: 4.348665039177746e-10\n",
      "Step: 2920, train/learning_rate: 3.8418213080149144e-05\n",
      "Step: 2920, train/epoch: 0.6949071884155273\n",
      "Step: 2930, train/loss: 0.0\n",
      "Step: 2930, train/grad_norm: 6.909910976737876e-10\n",
      "Step: 2930, train/learning_rate: 3.837854819721542e-05\n",
      "Step: 2930, train/epoch: 0.6972870230674744\n",
      "Step: 2940, train/loss: 0.0\n",
      "Step: 2940, train/grad_norm: 2.7667546032006385e-10\n",
      "Step: 2940, train/learning_rate: 3.833888695226051e-05\n",
      "Step: 2940, train/epoch: 0.6996667981147766\n",
      "Step: 2950, train/loss: 9.999999747378752e-05\n",
      "Step: 2950, train/grad_norm: 4.645043460982379e-08\n",
      "Step: 2950, train/learning_rate: 3.829922206932679e-05\n",
      "Step: 2950, train/epoch: 0.7020466327667236\n",
      "Step: 2960, train/loss: 0.09650000184774399\n",
      "Step: 2960, train/grad_norm: 2.9661030576111358e-12\n",
      "Step: 2960, train/learning_rate: 3.825955718639307e-05\n",
      "Step: 2960, train/epoch: 0.7044264674186707\n",
      "Step: 2970, train/loss: 0.0\n",
      "Step: 2970, train/grad_norm: 3.52063933561908e-10\n",
      "Step: 2970, train/learning_rate: 3.821989594143815e-05\n",
      "Step: 2970, train/epoch: 0.7068063020706177\n",
      "Step: 2980, train/loss: 0.07069999724626541\n",
      "Step: 2980, train/grad_norm: 1.3248147370803487e-11\n",
      "Step: 2980, train/learning_rate: 3.818023105850443e-05\n",
      "Step: 2980, train/epoch: 0.7091860771179199\n",
      "Step: 2990, train/loss: 0.0\n",
      "Step: 2990, train/grad_norm: 1.6369169486551982e-07\n",
      "Step: 2990, train/learning_rate: 3.814056617557071e-05\n",
      "Step: 2990, train/epoch: 0.7115659117698669\n",
      "Step: 3000, train/loss: 0.0\n",
      "Step: 3000, train/grad_norm: 3.1556331236970436e-07\n",
      "Step: 3000, train/learning_rate: 3.81009049306158e-05\n",
      "Step: 3000, train/epoch: 0.713945746421814\n",
      "Step: 3010, train/loss: 0.0\n",
      "Step: 3010, train/grad_norm: 2.2106637354024627e-10\n",
      "Step: 3010, train/learning_rate: 3.806124004768208e-05\n",
      "Step: 3010, train/epoch: 0.716325581073761\n",
      "Step: 3020, train/loss: 0.0\n",
      "Step: 3020, train/grad_norm: 1.0103359272761736e-05\n",
      "Step: 3020, train/learning_rate: 3.802157880272716e-05\n",
      "Step: 3020, train/epoch: 0.7187053561210632\n",
      "Step: 3030, train/loss: 0.0\n",
      "Step: 3030, train/grad_norm: 0.005071596242487431\n",
      "Step: 3030, train/learning_rate: 3.798191391979344e-05\n",
      "Step: 3030, train/epoch: 0.7210851907730103\n",
      "Step: 3040, train/loss: 0.0\n",
      "Step: 3040, train/grad_norm: 1.6570919636471615e-10\n",
      "Step: 3040, train/learning_rate: 3.794224903685972e-05\n",
      "Step: 3040, train/epoch: 0.7234650254249573\n",
      "Step: 3050, train/loss: 0.0\n",
      "Step: 3050, train/grad_norm: 6.717236544773186e-08\n",
      "Step: 3050, train/learning_rate: 3.790258779190481e-05\n",
      "Step: 3050, train/epoch: 0.7258448600769043\n",
      "Step: 3060, train/loss: 0.0\n",
      "Step: 3060, train/grad_norm: 5.147852088181748e-13\n",
      "Step: 3060, train/learning_rate: 3.7862922908971086e-05\n",
      "Step: 3060, train/epoch: 0.7282246351242065\n",
      "Step: 3070, train/loss: 0.0\n",
      "Step: 3070, train/grad_norm: 7.286443150233968e-12\n",
      "Step: 3070, train/learning_rate: 3.7823258026037365e-05\n",
      "Step: 3070, train/epoch: 0.7306044697761536\n",
      "Step: 3080, train/loss: 0.0\n",
      "Step: 3080, train/grad_norm: 1.5669442632315622e-08\n",
      "Step: 3080, train/learning_rate: 3.778359678108245e-05\n",
      "Step: 3080, train/epoch: 0.7329843044281006\n",
      "Step: 3090, train/loss: 0.0\n",
      "Step: 3090, train/grad_norm: 1.8660963399530495e-11\n",
      "Step: 3090, train/learning_rate: 3.774393189814873e-05\n",
      "Step: 3090, train/epoch: 0.7353641390800476\n",
      "Step: 3100, train/loss: 0.0\n",
      "Step: 3100, train/grad_norm: 2.427023154932123e-13\n",
      "Step: 3100, train/learning_rate: 3.770426701521501e-05\n",
      "Step: 3100, train/epoch: 0.7377439141273499\n",
      "Step: 3110, train/loss: 0.0\n",
      "Step: 3110, train/grad_norm: 1.91936408100446e-07\n",
      "Step: 3110, train/learning_rate: 3.7664605770260096e-05\n",
      "Step: 3110, train/epoch: 0.7401237487792969\n",
      "Step: 3120, train/loss: 0.0\n",
      "Step: 3120, train/grad_norm: 1.6071172836973346e-08\n",
      "Step: 3120, train/learning_rate: 3.7624940887326375e-05\n",
      "Step: 3120, train/epoch: 0.7425035834312439\n",
      "Step: 3130, train/loss: 0.0\n",
      "Step: 3130, train/grad_norm: 3.4512870339398205e-10\n",
      "Step: 3130, train/learning_rate: 3.7585276004392654e-05\n",
      "Step: 3130, train/epoch: 0.7448834180831909\n",
      "Step: 3140, train/loss: 0.0\n",
      "Step: 3140, train/grad_norm: 1.1983283343397488e-07\n",
      "Step: 3140, train/learning_rate: 3.754561475943774e-05\n",
      "Step: 3140, train/epoch: 0.7472631931304932\n",
      "Step: 3150, train/loss: 0.0\n",
      "Step: 3150, train/grad_norm: 1.5297360960175865e-06\n",
      "Step: 3150, train/learning_rate: 3.750594987650402e-05\n",
      "Step: 3150, train/epoch: 0.7496430277824402\n",
      "Step: 3160, train/loss: 0.0\n",
      "Step: 3160, train/grad_norm: 6.580896418084681e-12\n",
      "Step: 3160, train/learning_rate: 3.74662849935703e-05\n",
      "Step: 3160, train/epoch: 0.7520228624343872\n",
      "Step: 3170, train/loss: 0.0\n",
      "Step: 3170, train/grad_norm: 7.046922767539598e-11\n",
      "Step: 3170, train/learning_rate: 3.7426623748615384e-05\n",
      "Step: 3170, train/epoch: 0.7544026374816895\n",
      "Step: 3180, train/loss: 0.0\n",
      "Step: 3180, train/grad_norm: 1.1255956583466897e-10\n",
      "Step: 3180, train/learning_rate: 3.738695886568166e-05\n",
      "Step: 3180, train/epoch: 0.7567824721336365\n",
      "Step: 3190, train/loss: 0.0\n",
      "Step: 3190, train/grad_norm: 8.568915542406241e-12\n",
      "Step: 3190, train/learning_rate: 3.734729398274794e-05\n",
      "Step: 3190, train/epoch: 0.7591623067855835\n",
      "Step: 3200, train/loss: 0.0\n",
      "Step: 3200, train/grad_norm: 3.0479150453510684e-11\n",
      "Step: 3200, train/learning_rate: 3.730763273779303e-05\n",
      "Step: 3200, train/epoch: 0.7615421414375305\n",
      "Step: 3210, train/loss: 0.17810000479221344\n",
      "Step: 3210, train/grad_norm: 6.495494631053589e-08\n",
      "Step: 3210, train/learning_rate: 3.726796785485931e-05\n",
      "Step: 3210, train/epoch: 0.7639219164848328\n",
      "Step: 3220, train/loss: 0.0\n",
      "Step: 3220, train/grad_norm: 1.8562690939916138e-08\n",
      "Step: 3220, train/learning_rate: 3.7228302971925586e-05\n",
      "Step: 3220, train/epoch: 0.7663017511367798\n",
      "Step: 3230, train/loss: 0.0\n",
      "Step: 3230, train/grad_norm: 2.038741175880432e-09\n",
      "Step: 3230, train/learning_rate: 3.718864172697067e-05\n",
      "Step: 3230, train/epoch: 0.7686815857887268\n",
      "Step: 3240, train/loss: 0.0\n",
      "Step: 3240, train/grad_norm: 7.02448721412452e-09\n",
      "Step: 3240, train/learning_rate: 3.714897684403695e-05\n",
      "Step: 3240, train/epoch: 0.7710614204406738\n",
      "Step: 3250, train/loss: 0.1242000013589859\n",
      "Step: 3250, train/grad_norm: 2.9235815190986614e-07\n",
      "Step: 3250, train/learning_rate: 3.710931196110323e-05\n",
      "Step: 3250, train/epoch: 0.7734411954879761\n",
      "Step: 3260, train/loss: 0.0\n",
      "Step: 3260, train/grad_norm: 2.1564824237430003e-06\n",
      "Step: 3260, train/learning_rate: 3.706965071614832e-05\n",
      "Step: 3260, train/epoch: 0.7758210301399231\n",
      "Step: 3270, train/loss: 0.4226999878883362\n",
      "Step: 3270, train/grad_norm: 2.7763633170252433e-06\n",
      "Step: 3270, train/learning_rate: 3.7029985833214596e-05\n",
      "Step: 3270, train/epoch: 0.7782008647918701\n",
      "Step: 3280, train/loss: 0.0\n",
      "Step: 3280, train/grad_norm: 1.6518124539288692e-05\n",
      "Step: 3280, train/learning_rate: 3.6990320950280875e-05\n",
      "Step: 3280, train/epoch: 0.7805806994438171\n",
      "Step: 3290, train/loss: 0.0\n",
      "Step: 3290, train/grad_norm: 2.0707311705336906e-05\n",
      "Step: 3290, train/learning_rate: 3.695065970532596e-05\n",
      "Step: 3290, train/epoch: 0.7829604744911194\n",
      "Step: 3300, train/loss: 0.0\n",
      "Step: 3300, train/grad_norm: 1.832664020184893e-05\n",
      "Step: 3300, train/learning_rate: 3.691099482239224e-05\n",
      "Step: 3300, train/epoch: 0.7853403091430664\n",
      "Step: 3310, train/loss: 0.00019999999494757503\n",
      "Step: 3310, train/grad_norm: 0.0747796818614006\n",
      "Step: 3310, train/learning_rate: 3.687132993945852e-05\n",
      "Step: 3310, train/epoch: 0.7877201437950134\n",
      "Step: 3320, train/loss: 0.0\n",
      "Step: 3320, train/grad_norm: 0.00013163533003535122\n",
      "Step: 3320, train/learning_rate: 3.6831668694503605e-05\n",
      "Step: 3320, train/epoch: 0.7900999784469604\n",
      "Step: 3330, train/loss: 0.0\n",
      "Step: 3330, train/grad_norm: 4.5497714040720894e-08\n",
      "Step: 3330, train/learning_rate: 3.6792003811569884e-05\n",
      "Step: 3330, train/epoch: 0.7924797534942627\n",
      "Step: 3340, train/loss: 0.0\n",
      "Step: 3340, train/grad_norm: 5.989639340064912e-10\n",
      "Step: 3340, train/learning_rate: 3.6752338928636163e-05\n",
      "Step: 3340, train/epoch: 0.7948595881462097\n",
      "Step: 3350, train/loss: 0.0\n",
      "Step: 3350, train/grad_norm: 6.807481867099341e-08\n",
      "Step: 3350, train/learning_rate: 3.671267768368125e-05\n",
      "Step: 3350, train/epoch: 0.7972394227981567\n",
      "Step: 3360, train/loss: 0.0\n",
      "Step: 3360, train/grad_norm: 1.3018547351251186e-09\n",
      "Step: 3360, train/learning_rate: 3.667301280074753e-05\n",
      "Step: 3360, train/epoch: 0.7996192574501038\n",
      "Step: 3370, train/loss: 0.0\n",
      "Step: 3370, train/grad_norm: 5.246320600171828e-10\n",
      "Step: 3370, train/learning_rate: 3.663334791781381e-05\n",
      "Step: 3370, train/epoch: 0.801999032497406\n",
      "Step: 3380, train/loss: 0.0\n",
      "Step: 3380, train/grad_norm: 4.194343761199093e-10\n",
      "Step: 3380, train/learning_rate: 3.6593686672858894e-05\n",
      "Step: 3380, train/epoch: 0.804378867149353\n",
      "Step: 3390, train/loss: 0.0\n",
      "Step: 3390, train/grad_norm: 3.20175779222609e-08\n",
      "Step: 3390, train/learning_rate: 3.655402178992517e-05\n",
      "Step: 3390, train/epoch: 0.8067587018013\n",
      "Step: 3400, train/loss: 0.007199999876320362\n",
      "Step: 3400, train/grad_norm: 1.0753457013734646e-09\n",
      "Step: 3400, train/learning_rate: 3.651435690699145e-05\n",
      "Step: 3400, train/epoch: 0.8091384768486023\n",
      "Step: 3410, train/loss: 0.0\n",
      "Step: 3410, train/grad_norm: 1.1434172719759772e-08\n",
      "Step: 3410, train/learning_rate: 3.647469566203654e-05\n",
      "Step: 3410, train/epoch: 0.8115183115005493\n",
      "Step: 3420, train/loss: 0.0020000000949949026\n",
      "Step: 3420, train/grad_norm: 7.990083261688596e-09\n",
      "Step: 3420, train/learning_rate: 3.643503077910282e-05\n",
      "Step: 3420, train/epoch: 0.8138981461524963\n",
      "Step: 3430, train/loss: 0.0\n",
      "Step: 3430, train/grad_norm: 2.3179184438504308e-07\n",
      "Step: 3430, train/learning_rate: 3.6395365896169096e-05\n",
      "Step: 3430, train/epoch: 0.8162779808044434\n",
      "Step: 3440, train/loss: 0.10939999669790268\n",
      "Step: 3440, train/grad_norm: 1.4213709675914288e-07\n",
      "Step: 3440, train/learning_rate: 3.635570465121418e-05\n",
      "Step: 3440, train/epoch: 0.8186577558517456\n",
      "Step: 3450, train/loss: 0.0\n",
      "Step: 3450, train/grad_norm: 1.9647575300041353e-07\n",
      "Step: 3450, train/learning_rate: 3.631603976828046e-05\n",
      "Step: 3450, train/epoch: 0.8210375905036926\n",
      "Step: 3460, train/loss: 0.0005000000237487257\n",
      "Step: 3460, train/grad_norm: 2.7234671115875244\n",
      "Step: 3460, train/learning_rate: 3.627637488534674e-05\n",
      "Step: 3460, train/epoch: 0.8234174251556396\n",
      "Step: 3470, train/loss: 0.0\n",
      "Step: 3470, train/grad_norm: 0.0019133251626044512\n",
      "Step: 3470, train/learning_rate: 3.623671364039183e-05\n",
      "Step: 3470, train/epoch: 0.8257972598075867\n",
      "Step: 3480, train/loss: 0.0\n",
      "Step: 3480, train/grad_norm: 6.770037655479122e-12\n",
      "Step: 3480, train/learning_rate: 3.6197048757458106e-05\n",
      "Step: 3480, train/epoch: 0.8281770348548889\n",
      "Step: 3490, train/loss: 0.0\n",
      "Step: 3490, train/grad_norm: 3.149590099837951e-07\n",
      "Step: 3490, train/learning_rate: 3.6157383874524385e-05\n",
      "Step: 3490, train/epoch: 0.8305568695068359\n",
      "Step: 3500, train/loss: 0.0\n",
      "Step: 3500, train/grad_norm: 9.16876782830478e-11\n",
      "Step: 3500, train/learning_rate: 3.611772262956947e-05\n",
      "Step: 3500, train/epoch: 0.832936704158783\n",
      "Step: 3510, train/loss: 0.0\n",
      "Step: 3510, train/grad_norm: 1.175864738911514e-08\n",
      "Step: 3510, train/learning_rate: 3.607805774663575e-05\n",
      "Step: 3510, train/epoch: 0.83531653881073\n",
      "Step: 3520, train/loss: 0.0\n",
      "Step: 3520, train/grad_norm: 3.979413065735571e-08\n",
      "Step: 3520, train/learning_rate: 3.603839286370203e-05\n",
      "Step: 3520, train/epoch: 0.8376963138580322\n",
      "Step: 3530, train/loss: 0.0\n",
      "Step: 3530, train/grad_norm: 8.902617141745561e-13\n",
      "Step: 3530, train/learning_rate: 3.5998731618747115e-05\n",
      "Step: 3530, train/epoch: 0.8400761485099792\n",
      "Step: 3540, train/loss: 0.0\n",
      "Step: 3540, train/grad_norm: 0.00019465707009658217\n",
      "Step: 3540, train/learning_rate: 3.5959066735813394e-05\n",
      "Step: 3540, train/epoch: 0.8424559831619263\n",
      "Step: 3550, train/loss: 0.0\n",
      "Step: 3550, train/grad_norm: 4.397857633620106e-10\n",
      "Step: 3550, train/learning_rate: 3.591940185287967e-05\n",
      "Step: 3550, train/epoch: 0.8448358178138733\n",
      "Step: 3560, train/loss: 0.0\n",
      "Step: 3560, train/grad_norm: 8.195434020308312e-07\n",
      "Step: 3560, train/learning_rate: 3.587974060792476e-05\n",
      "Step: 3560, train/epoch: 0.8472155928611755\n",
      "Step: 3570, train/loss: 0.0\n",
      "Step: 3570, train/grad_norm: 1.4720779933571904e-12\n",
      "Step: 3570, train/learning_rate: 3.584007572499104e-05\n",
      "Step: 3570, train/epoch: 0.8495954275131226\n",
      "Step: 3580, train/loss: 0.09489999711513519\n",
      "Step: 3580, train/grad_norm: 220.3634490966797\n",
      "Step: 3580, train/learning_rate: 3.580041084205732e-05\n",
      "Step: 3580, train/epoch: 0.8519752621650696\n",
      "Step: 3590, train/loss: 0.0\n",
      "Step: 3590, train/grad_norm: 1.2466990106929643e-08\n",
      "Step: 3590, train/learning_rate: 3.5760749597102404e-05\n",
      "Step: 3590, train/epoch: 0.8543550968170166\n",
      "Step: 3600, train/loss: 0.0\n",
      "Step: 3600, train/grad_norm: 4.418768961844677e-10\n",
      "Step: 3600, train/learning_rate: 3.572108471416868e-05\n",
      "Step: 3600, train/epoch: 0.8567348718643188\n",
      "Step: 3610, train/loss: 0.0\n",
      "Step: 3610, train/grad_norm: 1.469639405830836e-12\n",
      "Step: 3610, train/learning_rate: 3.568141983123496e-05\n",
      "Step: 3610, train/epoch: 0.8591147065162659\n",
      "Step: 3620, train/loss: 0.0\n",
      "Step: 3620, train/grad_norm: 2.1874220490492036e-10\n",
      "Step: 3620, train/learning_rate: 3.564175858628005e-05\n",
      "Step: 3620, train/epoch: 0.8614945411682129\n",
      "Step: 3630, train/loss: 0.0\n",
      "Step: 3630, train/grad_norm: 2.6203572645044915e-09\n",
      "Step: 3630, train/learning_rate: 3.560209370334633e-05\n",
      "Step: 3630, train/epoch: 0.8638743162155151\n",
      "Step: 3640, train/loss: 0.0\n",
      "Step: 3640, train/grad_norm: 3.965162616650919e-13\n",
      "Step: 3640, train/learning_rate: 3.5562428820412606e-05\n",
      "Step: 3640, train/epoch: 0.8662541508674622\n",
      "Step: 3650, train/loss: 0.0\n",
      "Step: 3650, train/grad_norm: 6.583307232688185e-10\n",
      "Step: 3650, train/learning_rate: 3.552276757545769e-05\n",
      "Step: 3650, train/epoch: 0.8686339855194092\n",
      "Step: 3660, train/loss: 0.0\n",
      "Step: 3660, train/grad_norm: 2.2586813062247524e-12\n",
      "Step: 3660, train/learning_rate: 3.548310269252397e-05\n",
      "Step: 3660, train/epoch: 0.8710138201713562\n",
      "Step: 3670, train/loss: 0.8687999844551086\n",
      "Step: 3670, train/grad_norm: 6.177445470711973e-07\n",
      "Step: 3670, train/learning_rate: 3.544344144756906e-05\n",
      "Step: 3670, train/epoch: 0.8733935952186584\n",
      "Step: 3680, train/loss: 0.0\n",
      "Step: 3680, train/grad_norm: 1.9970610082964413e-05\n",
      "Step: 3680, train/learning_rate: 3.5403776564635336e-05\n",
      "Step: 3680, train/epoch: 0.8757734298706055\n",
      "Step: 3690, train/loss: 0.0\n",
      "Step: 3690, train/grad_norm: 0.0003725055430550128\n",
      "Step: 3690, train/learning_rate: 3.5364111681701615e-05\n",
      "Step: 3690, train/epoch: 0.8781532645225525\n",
      "Step: 3700, train/loss: 9.999999747378752e-05\n",
      "Step: 3700, train/grad_norm: 0.0025417932774871588\n",
      "Step: 3700, train/learning_rate: 3.53244504367467e-05\n",
      "Step: 3700, train/epoch: 0.8805330991744995\n",
      "Step: 3710, train/loss: 0.0729999989271164\n",
      "Step: 3710, train/grad_norm: 1.3256142494810774e-07\n",
      "Step: 3710, train/learning_rate: 3.528478555381298e-05\n",
      "Step: 3710, train/epoch: 0.8829128742218018\n",
      "Step: 3720, train/loss: 0.0\n",
      "Step: 3720, train/grad_norm: 1.5340958725573728e-06\n",
      "Step: 3720, train/learning_rate: 3.524512067087926e-05\n",
      "Step: 3720, train/epoch: 0.8852927088737488\n",
      "Step: 3730, train/loss: 0.011699999682605267\n",
      "Step: 3730, train/grad_norm: 2.230807005787483e-08\n",
      "Step: 3730, train/learning_rate: 3.5205459425924346e-05\n",
      "Step: 3730, train/epoch: 0.8876725435256958\n",
      "Step: 3740, train/loss: 0.0\n",
      "Step: 3740, train/grad_norm: 8.733719369047321e-06\n",
      "Step: 3740, train/learning_rate: 3.5165794542990625e-05\n",
      "Step: 3740, train/epoch: 0.8900523781776428\n",
      "Step: 3750, train/loss: 0.0\n",
      "Step: 3750, train/grad_norm: 5.077368925299197e-08\n",
      "Step: 3750, train/learning_rate: 3.5126129660056904e-05\n",
      "Step: 3750, train/epoch: 0.8924321532249451\n",
      "Step: 3760, train/loss: 0.0\n",
      "Step: 3760, train/grad_norm: 1.14400779693824e-06\n",
      "Step: 3760, train/learning_rate: 3.508646841510199e-05\n",
      "Step: 3760, train/epoch: 0.8948119878768921\n",
      "Step: 3770, train/loss: 0.008700000122189522\n",
      "Step: 3770, train/grad_norm: 1.3231997719742594e-09\n",
      "Step: 3770, train/learning_rate: 3.504680353216827e-05\n",
      "Step: 3770, train/epoch: 0.8971918225288391\n",
      "Step: 3780, train/loss: 0.0\n",
      "Step: 3780, train/grad_norm: 7.984457539578216e-08\n",
      "Step: 3780, train/learning_rate: 3.500713864923455e-05\n",
      "Step: 3780, train/epoch: 0.8995716571807861\n",
      "Step: 3790, train/loss: 0.0\n",
      "Step: 3790, train/grad_norm: 1.3798885367677372e-09\n",
      "Step: 3790, train/learning_rate: 3.4967477404279634e-05\n",
      "Step: 3790, train/epoch: 0.9019514322280884\n",
      "Step: 3800, train/loss: 0.0\n",
      "Step: 3800, train/grad_norm: 2.4861616765292638e-08\n",
      "Step: 3800, train/learning_rate: 3.4927812521345913e-05\n",
      "Step: 3800, train/epoch: 0.9043312668800354\n",
      "Step: 3810, train/loss: 0.051600001752376556\n",
      "Step: 3810, train/grad_norm: 1.1543322671059286e-06\n",
      "Step: 3810, train/learning_rate: 3.488814763841219e-05\n",
      "Step: 3810, train/epoch: 0.9067111015319824\n",
      "Step: 3820, train/loss: 0.25940001010894775\n",
      "Step: 3820, train/grad_norm: 1.2052485942604108e-07\n",
      "Step: 3820, train/learning_rate: 3.484848639345728e-05\n",
      "Step: 3820, train/epoch: 0.9090909361839294\n",
      "Step: 3830, train/loss: 0.0\n",
      "Step: 3830, train/grad_norm: 5.977558430458885e-07\n",
      "Step: 3830, train/learning_rate: 3.480882151052356e-05\n",
      "Step: 3830, train/epoch: 0.9114707112312317\n",
      "Step: 3840, train/loss: 0.16249999403953552\n",
      "Step: 3840, train/grad_norm: 190.94313049316406\n",
      "Step: 3840, train/learning_rate: 3.476915662758984e-05\n",
      "Step: 3840, train/epoch: 0.9138505458831787\n",
      "Step: 3850, train/loss: 0.0\n",
      "Step: 3850, train/grad_norm: 3.005307860348694e-07\n",
      "Step: 3850, train/learning_rate: 3.472949538263492e-05\n",
      "Step: 3850, train/epoch: 0.9162303805351257\n",
      "Step: 3860, train/loss: 0.0\n",
      "Step: 3860, train/grad_norm: 7.868243301345501e-08\n",
      "Step: 3860, train/learning_rate: 3.46898304997012e-05\n",
      "Step: 3860, train/epoch: 0.9186102151870728\n",
      "Step: 3870, train/loss: 0.0\n",
      "Step: 3870, train/grad_norm: 6.754650030416087e-07\n",
      "Step: 3870, train/learning_rate: 3.465016561676748e-05\n",
      "Step: 3870, train/epoch: 0.920989990234375\n",
      "Step: 3880, train/loss: 0.0\n",
      "Step: 3880, train/grad_norm: 1.126201993884024e-07\n",
      "Step: 3880, train/learning_rate: 3.461050437181257e-05\n",
      "Step: 3880, train/epoch: 0.923369824886322\n",
      "Step: 3890, train/loss: 0.06840000301599503\n",
      "Step: 3890, train/grad_norm: 2.7335950107953977e-06\n",
      "Step: 3890, train/learning_rate: 3.4570839488878846e-05\n",
      "Step: 3890, train/epoch: 0.925749659538269\n",
      "Step: 3900, train/loss: 0.25369998812675476\n",
      "Step: 3900, train/grad_norm: 3.1766649044584483e-06\n",
      "Step: 3900, train/learning_rate: 3.4531174605945125e-05\n",
      "Step: 3900, train/epoch: 0.9281294345855713\n",
      "Step: 3910, train/loss: 0.0\n",
      "Step: 3910, train/grad_norm: 0.00039043574361130595\n",
      "Step: 3910, train/learning_rate: 3.449151336099021e-05\n",
      "Step: 3910, train/epoch: 0.9305092692375183\n",
      "Step: 3920, train/loss: 0.0007999999797903001\n",
      "Step: 3920, train/grad_norm: 1.888188592147344e-07\n",
      "Step: 3920, train/learning_rate: 3.445184847805649e-05\n",
      "Step: 3920, train/epoch: 0.9328891038894653\n",
      "Step: 3930, train/loss: 9.999999747378752e-05\n",
      "Step: 3930, train/grad_norm: 5.282417987473309e-07\n",
      "Step: 3930, train/learning_rate: 3.441218359512277e-05\n",
      "Step: 3930, train/epoch: 0.9352689385414124\n",
      "Step: 3940, train/loss: 0.0\n",
      "Step: 3940, train/grad_norm: 3.061212394683821e-11\n",
      "Step: 3940, train/learning_rate: 3.4372522350167856e-05\n",
      "Step: 3940, train/epoch: 0.9376487135887146\n",
      "Step: 3950, train/loss: 0.0\n",
      "Step: 3950, train/grad_norm: 1.8855288164143502e-10\n",
      "Step: 3950, train/learning_rate: 3.4332857467234135e-05\n",
      "Step: 3950, train/epoch: 0.9400285482406616\n",
      "Step: 3960, train/loss: 0.0\n",
      "Step: 3960, train/grad_norm: 7.033555161906069e-12\n",
      "Step: 3960, train/learning_rate: 3.4293192584300414e-05\n",
      "Step: 3960, train/epoch: 0.9424083828926086\n",
      "Step: 3970, train/loss: 0.0\n",
      "Step: 3970, train/grad_norm: 3.3478117500429505e-10\n",
      "Step: 3970, train/learning_rate: 3.42535313393455e-05\n",
      "Step: 3970, train/epoch: 0.9447882175445557\n",
      "Step: 3980, train/loss: 0.0\n",
      "Step: 3980, train/grad_norm: 1.4386148938783094e-09\n",
      "Step: 3980, train/learning_rate: 3.421386645641178e-05\n",
      "Step: 3980, train/epoch: 0.9471679925918579\n",
      "Step: 3990, train/loss: 0.0\n",
      "Step: 3990, train/grad_norm: 2.6756123983062707e-09\n",
      "Step: 3990, train/learning_rate: 3.417420157347806e-05\n",
      "Step: 3990, train/epoch: 0.9495478272438049\n",
      "Step: 4000, train/loss: 0.0\n",
      "Step: 4000, train/grad_norm: 4.629548047002663e-12\n",
      "Step: 4000, train/learning_rate: 3.4134540328523144e-05\n",
      "Step: 4000, train/epoch: 0.951927661895752\n",
      "Step: 4010, train/loss: 0.0\n",
      "Step: 4010, train/grad_norm: 8.1391702266842e-09\n",
      "Step: 4010, train/learning_rate: 3.409487544558942e-05\n",
      "Step: 4010, train/epoch: 0.954307496547699\n",
      "Step: 4020, train/loss: 0.0\n",
      "Step: 4020, train/grad_norm: 8.853657362806189e-09\n",
      "Step: 4020, train/learning_rate: 3.40552105626557e-05\n",
      "Step: 4020, train/epoch: 0.9566872715950012\n",
      "Step: 4030, train/loss: 0.0\n",
      "Step: 4030, train/grad_norm: 2.3944417648635863e-07\n",
      "Step: 4030, train/learning_rate: 3.401554931770079e-05\n",
      "Step: 4030, train/epoch: 0.9590671062469482\n",
      "Step: 4040, train/loss: 0.0\n",
      "Step: 4040, train/grad_norm: 1.0377391307381245e-10\n",
      "Step: 4040, train/learning_rate: 3.397588443476707e-05\n",
      "Step: 4040, train/epoch: 0.9614469408988953\n",
      "Step: 4050, train/loss: 0.0\n",
      "Step: 4050, train/grad_norm: 9.335444417502003e-08\n",
      "Step: 4050, train/learning_rate: 3.3936219551833346e-05\n",
      "Step: 4050, train/epoch: 0.9638267755508423\n",
      "Step: 4060, train/loss: 0.0\n",
      "Step: 4060, train/grad_norm: 1.6009186865062475e-08\n",
      "Step: 4060, train/learning_rate: 3.389655830687843e-05\n",
      "Step: 4060, train/epoch: 0.9662065505981445\n",
      "Step: 4070, train/loss: 0.0\n",
      "Step: 4070, train/grad_norm: 2.054773773352281e-08\n",
      "Step: 4070, train/learning_rate: 3.385689342394471e-05\n",
      "Step: 4070, train/epoch: 0.9685863852500916\n",
      "Step: 4080, train/loss: 0.0\n",
      "Step: 4080, train/grad_norm: 6.073823388685895e-11\n",
      "Step: 4080, train/learning_rate: 3.381722854101099e-05\n",
      "Step: 4080, train/epoch: 0.9709662199020386\n",
      "Step: 4090, train/loss: 0.0\n",
      "Step: 4090, train/grad_norm: 1.5790303786733517e-10\n",
      "Step: 4090, train/learning_rate: 3.377756729605608e-05\n",
      "Step: 4090, train/epoch: 0.9733460545539856\n",
      "Step: 4100, train/loss: 0.0\n",
      "Step: 4100, train/grad_norm: 3.5827218969330943e-10\n",
      "Step: 4100, train/learning_rate: 3.3737902413122356e-05\n",
      "Step: 4100, train/epoch: 0.9757258296012878\n",
      "Step: 4110, train/loss: 0.0\n",
      "Step: 4110, train/grad_norm: 6.274261492300592e-12\n",
      "Step: 4110, train/learning_rate: 3.3698237530188635e-05\n",
      "Step: 4110, train/epoch: 0.9781056642532349\n",
      "Step: 4120, train/loss: 0.0\n",
      "Step: 4120, train/grad_norm: 3.9238591542645196e-11\n",
      "Step: 4120, train/learning_rate: 3.365857628523372e-05\n",
      "Step: 4120, train/epoch: 0.9804854989051819\n",
      "Step: 4130, train/loss: 0.18440000712871552\n",
      "Step: 4130, train/grad_norm: 1.6660625590247946e-07\n",
      "Step: 4130, train/learning_rate: 3.36189114023e-05\n",
      "Step: 4130, train/epoch: 0.9828652739524841\n",
      "Step: 4140, train/loss: 0.0\n",
      "Step: 4140, train/grad_norm: 4.469380243676824e-08\n",
      "Step: 4140, train/learning_rate: 3.357924651936628e-05\n",
      "Step: 4140, train/epoch: 0.9852451086044312\n",
      "Step: 4150, train/loss: 0.0\n",
      "Step: 4150, train/grad_norm: 0.009226524271070957\n",
      "Step: 4150, train/learning_rate: 3.3539585274411365e-05\n",
      "Step: 4150, train/epoch: 0.9876249432563782\n",
      "Step: 4160, train/loss: 0.002899999963119626\n",
      "Step: 4160, train/grad_norm: 1.3627769703816739e-06\n",
      "Step: 4160, train/learning_rate: 3.3499920391477644e-05\n",
      "Step: 4160, train/epoch: 0.9900047779083252\n",
      "Step: 4170, train/loss: 0.0\n",
      "Step: 4170, train/grad_norm: 1.1722807620462294e-11\n",
      "Step: 4170, train/learning_rate: 3.3460255508543923e-05\n",
      "Step: 4170, train/epoch: 0.9923845529556274\n",
      "Step: 4180, train/loss: 0.0\n",
      "Step: 4180, train/grad_norm: 1.0702772001991434e-11\n",
      "Step: 4180, train/learning_rate: 3.342059426358901e-05\n",
      "Step: 4180, train/epoch: 0.9947643876075745\n",
      "Step: 4190, train/loss: 0.3921999931335449\n",
      "Step: 4190, train/grad_norm: 1.4146046313501248e-11\n",
      "Step: 4190, train/learning_rate: 3.338092938065529e-05\n",
      "Step: 4190, train/epoch: 0.9971442222595215\n",
      "Step: 4200, train/loss: 0.0\n",
      "Step: 4200, train/grad_norm: 1.9602828615461476e-05\n",
      "Step: 4200, train/learning_rate: 3.334126449772157e-05\n",
      "Step: 4200, train/epoch: 0.9995240569114685\n",
      "Step: 4202, eval/loss: 0.008016631007194519\n",
      "Step: 4202, eval/accuracy: 0.9993058443069458\n",
      "Step: 4202, eval/f1: 0.9992669820785522\n",
      "Step: 4202, eval/runtime: 6245.5087890625\n",
      "Step: 4202, eval/samples_per_second: 1.152999997138977\n",
      "Step: 4202, eval/steps_per_second: 0.14399999380111694\n",
      "Step: 4202, train/epoch: 1.0\n",
      "Step: 4210, train/loss: 0.0\n",
      "Step: 4210, train/grad_norm: 5.418697037384845e-05\n",
      "Step: 4210, train/learning_rate: 3.3301603252766654e-05\n",
      "Step: 4210, train/epoch: 1.0019038915634155\n",
      "Step: 4220, train/loss: 0.006300000008195639\n",
      "Step: 4220, train/grad_norm: 7.562959467577457e-07\n",
      "Step: 4220, train/learning_rate: 3.326193836983293e-05\n",
      "Step: 4220, train/epoch: 1.0042836666107178\n",
      "Step: 4230, train/loss: 0.0\n",
      "Step: 4230, train/grad_norm: 6.18169792687695e-07\n",
      "Step: 4230, train/learning_rate: 3.322227348689921e-05\n",
      "Step: 4230, train/epoch: 1.00666344165802\n",
      "Step: 4240, train/loss: 0.0\n",
      "Step: 4240, train/grad_norm: 6.593681689537334e-08\n",
      "Step: 4240, train/learning_rate: 3.31826122419443e-05\n",
      "Step: 4240, train/epoch: 1.0090433359146118\n",
      "Step: 4250, train/loss: 0.0\n",
      "Step: 4250, train/grad_norm: 2.2080869257479208e-06\n",
      "Step: 4250, train/learning_rate: 3.314294735901058e-05\n",
      "Step: 4250, train/epoch: 1.011423110961914\n",
      "Step: 4260, train/loss: 0.0\n",
      "Step: 4260, train/grad_norm: 6.786945050407667e-07\n",
      "Step: 4260, train/learning_rate: 3.3103282476076856e-05\n",
      "Step: 4260, train/epoch: 1.0138030052185059\n",
      "Step: 4270, train/loss: 0.0\n",
      "Step: 4270, train/grad_norm: 7.902889898048215e-09\n",
      "Step: 4270, train/learning_rate: 3.306362123112194e-05\n",
      "Step: 4270, train/epoch: 1.016182780265808\n",
      "Step: 4280, train/loss: 0.0\n",
      "Step: 4280, train/grad_norm: 7.798535506253756e-08\n",
      "Step: 4280, train/learning_rate: 3.302395634818822e-05\n",
      "Step: 4280, train/epoch: 1.0185625553131104\n",
      "Step: 4290, train/loss: 0.0\n",
      "Step: 4290, train/grad_norm: 2.8821903197417953e-11\n",
      "Step: 4290, train/learning_rate: 3.29842914652545e-05\n",
      "Step: 4290, train/epoch: 1.0209424495697021\n",
      "Step: 4300, train/loss: 0.0\n",
      "Step: 4300, train/grad_norm: 2.4173840529329027e-07\n",
      "Step: 4300, train/learning_rate: 3.294463022029959e-05\n",
      "Step: 4300, train/epoch: 1.0233222246170044\n",
      "Step: 4310, train/loss: 0.0\n",
      "Step: 4310, train/grad_norm: 4.653586438507773e-05\n",
      "Step: 4310, train/learning_rate: 3.2904965337365866e-05\n",
      "Step: 4310, train/epoch: 1.0257019996643066\n",
      "Step: 4320, train/loss: 0.0005000000237487257\n",
      "Step: 4320, train/grad_norm: 1.0463929028814434e-10\n",
      "Step: 4320, train/learning_rate: 3.2865300454432145e-05\n",
      "Step: 4320, train/epoch: 1.0280818939208984\n",
      "Step: 4330, train/loss: 0.0\n",
      "Step: 4330, train/grad_norm: 2.060883019794346e-09\n",
      "Step: 4330, train/learning_rate: 3.282563920947723e-05\n",
      "Step: 4330, train/epoch: 1.0304616689682007\n",
      "Step: 4340, train/loss: 0.0\n",
      "Step: 4340, train/grad_norm: 1.850741136877332e-05\n",
      "Step: 4340, train/learning_rate: 3.278597432654351e-05\n",
      "Step: 4340, train/epoch: 1.0328415632247925\n",
      "Step: 4350, train/loss: 0.0\n",
      "Step: 4350, train/grad_norm: 1.7794194718590006e-05\n",
      "Step: 4350, train/learning_rate: 3.2746313081588596e-05\n",
      "Step: 4350, train/epoch: 1.0352213382720947\n",
      "Step: 4360, train/loss: 0.0\n",
      "Step: 4360, train/grad_norm: 2.974302804048534e-09\n",
      "Step: 4360, train/learning_rate: 3.2706648198654875e-05\n",
      "Step: 4360, train/epoch: 1.037601113319397\n",
      "Step: 4370, train/loss: 0.0\n",
      "Step: 4370, train/grad_norm: 8.124667605358127e-10\n",
      "Step: 4370, train/learning_rate: 3.2666983315721154e-05\n",
      "Step: 4370, train/epoch: 1.0399810075759888\n",
      "Step: 4380, train/loss: 0.0\n",
      "Step: 4380, train/grad_norm: 5.149333082954399e-05\n",
      "Step: 4380, train/learning_rate: 3.262732207076624e-05\n",
      "Step: 4380, train/epoch: 1.042360782623291\n",
      "Step: 4390, train/loss: 0.0\n",
      "Step: 4390, train/grad_norm: 1.034165848068369e-06\n",
      "Step: 4390, train/learning_rate: 3.258765718783252e-05\n",
      "Step: 4390, train/epoch: 1.0447405576705933\n",
      "Step: 4400, train/loss: 0.0\n",
      "Step: 4400, train/grad_norm: 1.493965783083695e-06\n",
      "Step: 4400, train/learning_rate: 3.25479923048988e-05\n",
      "Step: 4400, train/epoch: 1.047120451927185\n",
      "Step: 4410, train/loss: 0.0\n",
      "Step: 4410, train/grad_norm: 2.551191036204159e-09\n",
      "Step: 4410, train/learning_rate: 3.2508331059943885e-05\n",
      "Step: 4410, train/epoch: 1.0495002269744873\n",
      "Step: 4420, train/loss: 0.14380000531673431\n",
      "Step: 4420, train/grad_norm: 343.1187744140625\n",
      "Step: 4420, train/learning_rate: 3.2468666177010164e-05\n",
      "Step: 4420, train/epoch: 1.0518800020217896\n",
      "Step: 4430, train/loss: 0.0\n",
      "Step: 4430, train/grad_norm: 3.754782249010269e-11\n",
      "Step: 4430, train/learning_rate: 3.242900129407644e-05\n",
      "Step: 4430, train/epoch: 1.0542598962783813\n",
      "Step: 4440, train/loss: 0.0\n",
      "Step: 4440, train/grad_norm: 2.880320266740455e-07\n",
      "Step: 4440, train/learning_rate: 3.238934004912153e-05\n",
      "Step: 4440, train/epoch: 1.0566396713256836\n",
      "Step: 4450, train/loss: 0.20000000298023224\n",
      "Step: 4450, train/grad_norm: 0.00010737597040133551\n",
      "Step: 4450, train/learning_rate: 3.234967516618781e-05\n",
      "Step: 4450, train/epoch: 1.0590195655822754\n",
      "Step: 4460, train/loss: 0.0\n",
      "Step: 4460, train/grad_norm: 7.338323371186561e-08\n",
      "Step: 4460, train/learning_rate: 3.231001028325409e-05\n",
      "Step: 4460, train/epoch: 1.0613993406295776\n",
      "Step: 4470, train/loss: 0.0\n",
      "Step: 4470, train/grad_norm: 8.762804645812139e-05\n",
      "Step: 4470, train/learning_rate: 3.227034903829917e-05\n",
      "Step: 4470, train/epoch: 1.0637791156768799\n",
      "Step: 4480, train/loss: 0.0\n",
      "Step: 4480, train/grad_norm: 1.784045593922201e-06\n",
      "Step: 4480, train/learning_rate: 3.223068415536545e-05\n",
      "Step: 4480, train/epoch: 1.0661590099334717\n",
      "Step: 4490, train/loss: 0.0\n",
      "Step: 4490, train/grad_norm: 0.00029219116549938917\n",
      "Step: 4490, train/learning_rate: 3.219101927243173e-05\n",
      "Step: 4490, train/epoch: 1.068538784980774\n",
      "Step: 4500, train/loss: 0.0\n",
      "Step: 4500, train/grad_norm: 1.908254125737585e-05\n",
      "Step: 4500, train/learning_rate: 3.215135802747682e-05\n",
      "Step: 4500, train/epoch: 1.0709185600280762\n",
      "Step: 4510, train/loss: 0.0\n",
      "Step: 4510, train/grad_norm: 2.418787232727482e-07\n",
      "Step: 4510, train/learning_rate: 3.2111693144543096e-05\n",
      "Step: 4510, train/epoch: 1.073298454284668\n",
      "Step: 4520, train/loss: 0.0\n",
      "Step: 4520, train/grad_norm: 2.2956994598644087e-06\n",
      "Step: 4520, train/learning_rate: 3.2072028261609375e-05\n",
      "Step: 4520, train/epoch: 1.0756782293319702\n",
      "Step: 4530, train/loss: 0.0\n",
      "Step: 4530, train/grad_norm: 2.5551642011123477e-06\n",
      "Step: 4530, train/learning_rate: 3.203236701665446e-05\n",
      "Step: 4530, train/epoch: 1.078058123588562\n",
      "Step: 4540, train/loss: 0.0\n",
      "Step: 4540, train/grad_norm: 3.33973512169905e-05\n",
      "Step: 4540, train/learning_rate: 3.199270213372074e-05\n",
      "Step: 4540, train/epoch: 1.0804378986358643\n",
      "Step: 4550, train/loss: 0.0\n",
      "Step: 4550, train/grad_norm: 5.1518145482987165e-05\n",
      "Step: 4550, train/learning_rate: 3.195303725078702e-05\n",
      "Step: 4550, train/epoch: 1.0828176736831665\n",
      "Step: 4560, train/loss: 0.0\n",
      "Step: 4560, train/grad_norm: 1.2800308013538597e-06\n",
      "Step: 4560, train/learning_rate: 3.1913376005832106e-05\n",
      "Step: 4560, train/epoch: 1.0851975679397583\n",
      "Step: 4570, train/loss: 0.0\n",
      "Step: 4570, train/grad_norm: 1.3671246961166617e-05\n",
      "Step: 4570, train/learning_rate: 3.1873711122898385e-05\n",
      "Step: 4570, train/epoch: 1.0875773429870605\n",
      "Step: 4580, train/loss: 0.05490000173449516\n",
      "Step: 4580, train/grad_norm: 1.946657501861182e-09\n",
      "Step: 4580, train/learning_rate: 3.1834046239964664e-05\n",
      "Step: 4580, train/epoch: 1.0899571180343628\n",
      "Step: 4590, train/loss: 0.0\n",
      "Step: 4590, train/grad_norm: 1.9027114106506815e-08\n",
      "Step: 4590, train/learning_rate: 3.179438499500975e-05\n",
      "Step: 4590, train/epoch: 1.0923370122909546\n",
      "Step: 4600, train/loss: 0.0\n",
      "Step: 4600, train/grad_norm: 0.00298235728405416\n",
      "Step: 4600, train/learning_rate: 3.175472011207603e-05\n",
      "Step: 4600, train/epoch: 1.0947167873382568\n",
      "Step: 4610, train/loss: 0.0\n",
      "Step: 4610, train/grad_norm: 2.3008814853398007e-09\n",
      "Step: 4610, train/learning_rate: 3.171505522914231e-05\n",
      "Step: 4610, train/epoch: 1.097096562385559\n",
      "Step: 4620, train/loss: 0.0\n",
      "Step: 4620, train/grad_norm: 3.6042783335688e-07\n",
      "Step: 4620, train/learning_rate: 3.1675393984187394e-05\n",
      "Step: 4620, train/epoch: 1.0994764566421509\n",
      "Step: 4630, train/loss: 0.0\n",
      "Step: 4630, train/grad_norm: 1.809030841792847e-10\n",
      "Step: 4630, train/learning_rate: 3.1635729101253673e-05\n",
      "Step: 4630, train/epoch: 1.1018562316894531\n",
      "Step: 4640, train/loss: 0.0\n",
      "Step: 4640, train/grad_norm: 4.600227399009782e-08\n",
      "Step: 4640, train/learning_rate: 3.159606421831995e-05\n",
      "Step: 4640, train/epoch: 1.104236125946045\n",
      "Step: 4650, train/loss: 0.0\n",
      "Step: 4650, train/grad_norm: 1.2663654125333323e-08\n",
      "Step: 4650, train/learning_rate: 3.155640297336504e-05\n",
      "Step: 4650, train/epoch: 1.1066159009933472\n",
      "Step: 4660, train/loss: 0.0\n",
      "Step: 4660, train/grad_norm: 0.0035760074388235807\n",
      "Step: 4660, train/learning_rate: 3.151673809043132e-05\n",
      "Step: 4660, train/epoch: 1.1089956760406494\n",
      "Step: 4670, train/loss: 0.0\n",
      "Step: 4670, train/grad_norm: 2.486404810042586e-06\n",
      "Step: 4670, train/learning_rate: 3.14770732074976e-05\n",
      "Step: 4670, train/epoch: 1.1113755702972412\n",
      "Step: 4680, train/loss: 0.0\n",
      "Step: 4680, train/grad_norm: 2.6171373956884736e-09\n",
      "Step: 4680, train/learning_rate: 3.143741196254268e-05\n",
      "Step: 4680, train/epoch: 1.1137553453445435\n",
      "Step: 4690, train/loss: 0.0\n",
      "Step: 4690, train/grad_norm: 2.3795465153853e-08\n",
      "Step: 4690, train/learning_rate: 3.139774707960896e-05\n",
      "Step: 4690, train/epoch: 1.1161351203918457\n",
      "Step: 4700, train/loss: 0.0\n",
      "Step: 4700, train/grad_norm: 2.960779852401174e-07\n",
      "Step: 4700, train/learning_rate: 3.135808219667524e-05\n",
      "Step: 4700, train/epoch: 1.1185150146484375\n",
      "Step: 4710, train/loss: 0.0\n",
      "Step: 4710, train/grad_norm: 1.0475108558338775e-09\n",
      "Step: 4710, train/learning_rate: 3.131842095172033e-05\n",
      "Step: 4710, train/epoch: 1.1208947896957397\n",
      "Step: 4720, train/loss: 0.0\n",
      "Step: 4720, train/grad_norm: 2.0705854808511504e-08\n",
      "Step: 4720, train/learning_rate: 3.1278756068786606e-05\n",
      "Step: 4720, train/epoch: 1.1232746839523315\n",
      "Step: 4730, train/loss: 0.0\n",
      "Step: 4730, train/grad_norm: 2.5522677304934405e-08\n",
      "Step: 4730, train/learning_rate: 3.1239091185852885e-05\n",
      "Step: 4730, train/epoch: 1.1256544589996338\n",
      "Step: 4740, train/loss: 0.0\n",
      "Step: 4740, train/grad_norm: 3.152567629172154e-08\n",
      "Step: 4740, train/learning_rate: 3.119942994089797e-05\n",
      "Step: 4740, train/epoch: 1.128034234046936\n",
      "Step: 4750, train/loss: 0.0\n",
      "Step: 4750, train/grad_norm: 9.871566675201393e-08\n",
      "Step: 4750, train/learning_rate: 3.115976505796425e-05\n",
      "Step: 4750, train/epoch: 1.1304141283035278\n",
      "Step: 4760, train/loss: 0.0\n",
      "Step: 4760, train/grad_norm: 1.15905329778343e-08\n",
      "Step: 4760, train/learning_rate: 3.112010017503053e-05\n",
      "Step: 4760, train/epoch: 1.13279390335083\n",
      "Step: 4770, train/loss: 0.0\n",
      "Step: 4770, train/grad_norm: 0.00020173986558802426\n",
      "Step: 4770, train/learning_rate: 3.1080438930075616e-05\n",
      "Step: 4770, train/epoch: 1.1351736783981323\n",
      "Step: 4780, train/loss: 0.0\n",
      "Step: 4780, train/grad_norm: 1.5102531506272499e-05\n",
      "Step: 4780, train/learning_rate: 3.1040774047141895e-05\n",
      "Step: 4780, train/epoch: 1.1375535726547241\n",
      "Step: 4790, train/loss: 0.0\n",
      "Step: 4790, train/grad_norm: 2.633486246850225e-06\n",
      "Step: 4790, train/learning_rate: 3.1001109164208174e-05\n",
      "Step: 4790, train/epoch: 1.1399333477020264\n",
      "Step: 4800, train/loss: 0.0\n",
      "Step: 4800, train/grad_norm: 9.45989328537955e-11\n",
      "Step: 4800, train/learning_rate: 3.096144791925326e-05\n",
      "Step: 4800, train/epoch: 1.1423132419586182\n",
      "Step: 4810, train/loss: 0.0\n",
      "Step: 4810, train/grad_norm: 5.319457585528653e-08\n",
      "Step: 4810, train/learning_rate: 3.092178303631954e-05\n",
      "Step: 4810, train/epoch: 1.1446930170059204\n",
      "Step: 4820, train/loss: 0.0\n",
      "Step: 4820, train/grad_norm: 2.6929111385243232e-08\n",
      "Step: 4820, train/learning_rate: 3.088211815338582e-05\n",
      "Step: 4820, train/epoch: 1.1470727920532227\n",
      "Step: 4830, train/loss: 0.0\n",
      "Step: 4830, train/grad_norm: 7.597259626956898e-11\n",
      "Step: 4830, train/learning_rate: 3.0842456908430904e-05\n",
      "Step: 4830, train/epoch: 1.1494526863098145\n",
      "Step: 4840, train/loss: 0.0\n",
      "Step: 4840, train/grad_norm: 6.74284501656075e-06\n",
      "Step: 4840, train/learning_rate: 3.080279202549718e-05\n",
      "Step: 4840, train/epoch: 1.1518324613571167\n",
      "Step: 4850, train/loss: 0.0\n",
      "Step: 4850, train/grad_norm: 1.9269083395556663e-07\n",
      "Step: 4850, train/learning_rate: 3.076312714256346e-05\n",
      "Step: 4850, train/epoch: 1.154212236404419\n",
      "Step: 4860, train/loss: 0.0\n",
      "Step: 4860, train/grad_norm: 1.1270763877746504e-08\n",
      "Step: 4860, train/learning_rate: 3.072346589760855e-05\n",
      "Step: 4860, train/epoch: 1.1565921306610107\n",
      "Step: 4870, train/loss: 0.0\n",
      "Step: 4870, train/grad_norm: 1.0524647223064676e-05\n",
      "Step: 4870, train/learning_rate: 3.068380101467483e-05\n",
      "Step: 4870, train/epoch: 1.158971905708313\n",
      "Step: 4880, train/loss: 0.0\n",
      "Step: 4880, train/grad_norm: 2.186051460739691e-06\n",
      "Step: 4880, train/learning_rate: 3.0644136131741107e-05\n",
      "Step: 4880, train/epoch: 1.1613516807556152\n",
      "Step: 4890, train/loss: 0.0\n",
      "Step: 4890, train/grad_norm: 3.961152827969272e-08\n",
      "Step: 4890, train/learning_rate: 3.060447488678619e-05\n",
      "Step: 4890, train/epoch: 1.163731575012207\n",
      "Step: 4900, train/loss: 0.0\n",
      "Step: 4900, train/grad_norm: 2.2550618723471416e-06\n",
      "Step: 4900, train/learning_rate: 3.056481000385247e-05\n",
      "Step: 4900, train/epoch: 1.1661113500595093\n",
      "Step: 4910, train/loss: 0.0\n",
      "Step: 4910, train/grad_norm: 6.752895131967307e-08\n",
      "Step: 4910, train/learning_rate: 3.052514512091875e-05\n",
      "Step: 4910, train/epoch: 1.168491244316101\n",
      "Step: 4920, train/loss: 0.0\n",
      "Step: 4920, train/grad_norm: 8.348988558282144e-06\n",
      "Step: 4920, train/learning_rate: 3.0485483875963837e-05\n",
      "Step: 4920, train/epoch: 1.1708710193634033\n",
      "Step: 4930, train/loss: 0.0\n",
      "Step: 4930, train/grad_norm: 8.649814753347584e-10\n",
      "Step: 4930, train/learning_rate: 3.0445818993030116e-05\n",
      "Step: 4930, train/epoch: 1.1732507944107056\n",
      "Step: 4940, train/loss: 0.0\n",
      "Step: 4940, train/grad_norm: 9.016689617169504e-10\n",
      "Step: 4940, train/learning_rate: 3.04061559290858e-05\n",
      "Step: 4940, train/epoch: 1.1756306886672974\n",
      "Step: 4950, train/loss: 0.0\n",
      "Step: 4950, train/grad_norm: 9.094653849461842e-12\n",
      "Step: 4950, train/learning_rate: 3.036649286514148e-05\n",
      "Step: 4950, train/epoch: 1.1780104637145996\n",
      "Step: 4960, train/loss: 0.0\n",
      "Step: 4960, train/grad_norm: 1.8896349374131205e-08\n",
      "Step: 4960, train/learning_rate: 3.032682798220776e-05\n",
      "Step: 4960, train/epoch: 1.1803902387619019\n",
      "Step: 4970, train/loss: 0.0\n",
      "Step: 4970, train/grad_norm: 5.095190758197532e-08\n",
      "Step: 4970, train/learning_rate: 3.0287164918263443e-05\n",
      "Step: 4970, train/epoch: 1.1827701330184937\n",
      "Step: 4980, train/loss: 0.0\n",
      "Step: 4980, train/grad_norm: 7.0990969179562935e-12\n",
      "Step: 4980, train/learning_rate: 3.0247501854319125e-05\n",
      "Step: 4980, train/epoch: 1.185149908065796\n",
      "Step: 4990, train/loss: 0.0\n",
      "Step: 4990, train/grad_norm: 1.451456316248212e-11\n",
      "Step: 4990, train/learning_rate: 3.0207836971385404e-05\n",
      "Step: 4990, train/epoch: 1.1875298023223877\n",
      "Step: 5000, train/loss: 0.0\n",
      "Step: 5000, train/grad_norm: 6.221488324520408e-10\n",
      "Step: 5000, train/learning_rate: 3.0168173907441087e-05\n",
      "Step: 5000, train/epoch: 1.18990957736969\n",
      "Step: 5010, train/loss: 0.0\n",
      "Step: 5010, train/grad_norm: 2.617571714935707e-09\n",
      "Step: 5010, train/learning_rate: 3.012851084349677e-05\n",
      "Step: 5010, train/epoch: 1.1922893524169922\n",
      "Step: 5020, train/loss: 0.0\n",
      "Step: 5020, train/grad_norm: 4.564022423880765e-11\n",
      "Step: 5020, train/learning_rate: 3.008884596056305e-05\n",
      "Step: 5020, train/epoch: 1.194669246673584\n",
      "Step: 5030, train/loss: 0.0\n",
      "Step: 5030, train/grad_norm: 0.004026609007269144\n",
      "Step: 5030, train/learning_rate: 3.004918289661873e-05\n",
      "Step: 5030, train/epoch: 1.1970490217208862\n",
      "Step: 5040, train/loss: 0.0\n",
      "Step: 5040, train/grad_norm: 1.917302761000883e-08\n",
      "Step: 5040, train/learning_rate: 3.0009519832674414e-05\n",
      "Step: 5040, train/epoch: 1.1994287967681885\n",
      "Step: 5050, train/loss: 0.0\n",
      "Step: 5050, train/grad_norm: 1.1693668255929879e-09\n",
      "Step: 5050, train/learning_rate: 2.9969854949740693e-05\n",
      "Step: 5050, train/epoch: 1.2018086910247803\n",
      "Step: 5060, train/loss: 0.0\n",
      "Step: 5060, train/grad_norm: 2.951202588974411e-07\n",
      "Step: 5060, train/learning_rate: 2.9930191885796376e-05\n",
      "Step: 5060, train/epoch: 1.2041884660720825\n",
      "Step: 5070, train/loss: 0.0\n",
      "Step: 5070, train/grad_norm: 1.3809002439302276e-06\n",
      "Step: 5070, train/learning_rate: 2.9890528821852058e-05\n",
      "Step: 5070, train/epoch: 1.2065683603286743\n",
      "Step: 5080, train/loss: 0.0\n",
      "Step: 5080, train/grad_norm: 1.7271284377784468e-05\n",
      "Step: 5080, train/learning_rate: 2.9850863938918337e-05\n",
      "Step: 5080, train/epoch: 1.2089481353759766\n",
      "Step: 5090, train/loss: 0.0\n",
      "Step: 5090, train/grad_norm: 8.982665278267632e-09\n",
      "Step: 5090, train/learning_rate: 2.981120087497402e-05\n",
      "Step: 5090, train/epoch: 1.2113279104232788\n",
      "Step: 5100, train/loss: 0.0\n",
      "Step: 5100, train/grad_norm: 7.743246897007339e-07\n",
      "Step: 5100, train/learning_rate: 2.9771537811029702e-05\n",
      "Step: 5100, train/epoch: 1.2137078046798706\n",
      "Step: 5110, train/loss: 0.0\n",
      "Step: 5110, train/grad_norm: 4.75015582424021e-11\n",
      "Step: 5110, train/learning_rate: 2.973187292809598e-05\n",
      "Step: 5110, train/epoch: 1.2160875797271729\n",
      "Step: 5120, train/loss: 0.0\n",
      "Step: 5120, train/grad_norm: 1.893365464411545e-08\n",
      "Step: 5120, train/learning_rate: 2.9692209864151664e-05\n",
      "Step: 5120, train/epoch: 1.218467354774475\n",
      "Step: 5130, train/loss: 0.0\n",
      "Step: 5130, train/grad_norm: 1.1243487563206145e-07\n",
      "Step: 5130, train/learning_rate: 2.9652546800207347e-05\n",
      "Step: 5130, train/epoch: 1.220847249031067\n",
      "Step: 5140, train/loss: 0.0\n",
      "Step: 5140, train/grad_norm: 1.3253073083419054e-09\n",
      "Step: 5140, train/learning_rate: 2.9612881917273626e-05\n",
      "Step: 5140, train/epoch: 1.2232270240783691\n",
      "Step: 5150, train/loss: 0.0\n",
      "Step: 5150, train/grad_norm: 3.3781510865082964e-05\n",
      "Step: 5150, train/learning_rate: 2.957321885332931e-05\n",
      "Step: 5150, train/epoch: 1.2256067991256714\n",
      "Step: 5160, train/loss: 0.0\n",
      "Step: 5160, train/grad_norm: 1.6926948731565972e-08\n",
      "Step: 5160, train/learning_rate: 2.953355578938499e-05\n",
      "Step: 5160, train/epoch: 1.2279866933822632\n",
      "Step: 5170, train/loss: 0.0\n",
      "Step: 5170, train/grad_norm: 1.4778444210605812e-06\n",
      "Step: 5170, train/learning_rate: 2.949389090645127e-05\n",
      "Step: 5170, train/epoch: 1.2303664684295654\n",
      "Step: 5180, train/loss: 0.0\n",
      "Step: 5180, train/grad_norm: 7.404734674310021e-07\n",
      "Step: 5180, train/learning_rate: 2.9454227842506953e-05\n",
      "Step: 5180, train/epoch: 1.2327463626861572\n",
      "Step: 5190, train/loss: 0.0\n",
      "Step: 5190, train/grad_norm: 3.5285250415961755e-11\n",
      "Step: 5190, train/learning_rate: 2.9414564778562635e-05\n",
      "Step: 5190, train/epoch: 1.2351261377334595\n",
      "Step: 5200, train/loss: 0.0\n",
      "Step: 5200, train/grad_norm: 0.00021271378500387073\n",
      "Step: 5200, train/learning_rate: 2.9374901714618318e-05\n",
      "Step: 5200, train/epoch: 1.2375059127807617\n",
      "Step: 5210, train/loss: 0.0\n",
      "Step: 5210, train/grad_norm: 2.676908366083808e-07\n",
      "Step: 5210, train/learning_rate: 2.9335236831684597e-05\n",
      "Step: 5210, train/epoch: 1.2398858070373535\n",
      "Step: 5220, train/loss: 0.0\n",
      "Step: 5220, train/grad_norm: 2.512945185273452e-09\n",
      "Step: 5220, train/learning_rate: 2.929557376774028e-05\n",
      "Step: 5220, train/epoch: 1.2422655820846558\n",
      "Step: 5230, train/loss: 0.0\n",
      "Step: 5230, train/grad_norm: 5.1952796638943255e-05\n",
      "Step: 5230, train/learning_rate: 2.9255910703795962e-05\n",
      "Step: 5230, train/epoch: 1.244645357131958\n",
      "Step: 5240, train/loss: 0.0\n",
      "Step: 5240, train/grad_norm: 2.7116620060496643e-09\n",
      "Step: 5240, train/learning_rate: 2.921624582086224e-05\n",
      "Step: 5240, train/epoch: 1.2470252513885498\n",
      "Step: 5250, train/loss: 0.0\n",
      "Step: 5250, train/grad_norm: 2.534805844334187e-06\n",
      "Step: 5250, train/learning_rate: 2.9176582756917924e-05\n",
      "Step: 5250, train/epoch: 1.249405026435852\n",
      "Step: 5260, train/loss: 0.0\n",
      "Step: 5260, train/grad_norm: 1.6104308087960817e-05\n",
      "Step: 5260, train/learning_rate: 2.9136919692973606e-05\n",
      "Step: 5260, train/epoch: 1.2517849206924438\n",
      "Step: 5270, train/loss: 0.0\n",
      "Step: 5270, train/grad_norm: 6.766623261711402e-09\n",
      "Step: 5270, train/learning_rate: 2.9097254810039885e-05\n",
      "Step: 5270, train/epoch: 1.254164695739746\n",
      "Step: 5280, train/loss: 0.0\n",
      "Step: 5280, train/grad_norm: 1.3904614206694532e-06\n",
      "Step: 5280, train/learning_rate: 2.9057591746095568e-05\n",
      "Step: 5280, train/epoch: 1.2565444707870483\n",
      "Step: 5290, train/loss: 0.0\n",
      "Step: 5290, train/grad_norm: 9.726433347623242e-08\n",
      "Step: 5290, train/learning_rate: 2.901792868215125e-05\n",
      "Step: 5290, train/epoch: 1.2589243650436401\n",
      "Step: 5300, train/loss: 0.0\n",
      "Step: 5300, train/grad_norm: 5.713378072869091e-07\n",
      "Step: 5300, train/learning_rate: 2.897826379921753e-05\n",
      "Step: 5300, train/epoch: 1.2613041400909424\n",
      "Step: 5310, train/loss: 0.0\n",
      "Step: 5310, train/grad_norm: 7.849009442395527e-09\n",
      "Step: 5310, train/learning_rate: 2.8938600735273212e-05\n",
      "Step: 5310, train/epoch: 1.2636839151382446\n",
      "Step: 5320, train/loss: 0.0\n",
      "Step: 5320, train/grad_norm: 1.0318141363541145e-07\n",
      "Step: 5320, train/learning_rate: 2.8898937671328895e-05\n",
      "Step: 5320, train/epoch: 1.2660638093948364\n",
      "Step: 5330, train/loss: 0.0\n",
      "Step: 5330, train/grad_norm: 1.7593693257822451e-07\n",
      "Step: 5330, train/learning_rate: 2.8859272788395174e-05\n",
      "Step: 5330, train/epoch: 1.2684435844421387\n",
      "Step: 5340, train/loss: 0.0\n",
      "Step: 5340, train/grad_norm: 1.9396453332376495e-09\n",
      "Step: 5340, train/learning_rate: 2.8819609724450856e-05\n",
      "Step: 5340, train/epoch: 1.270823359489441\n",
      "Step: 5350, train/loss: 0.0\n",
      "Step: 5350, train/grad_norm: 1.8761578530757106e-06\n",
      "Step: 5350, train/learning_rate: 2.877994666050654e-05\n",
      "Step: 5350, train/epoch: 1.2732032537460327\n",
      "Step: 5360, train/loss: 0.0\n",
      "Step: 5360, train/grad_norm: 1.3893691752642212e-11\n",
      "Step: 5360, train/learning_rate: 2.8740281777572818e-05\n",
      "Step: 5360, train/epoch: 1.275583028793335\n",
      "Step: 5370, train/loss: 0.0\n",
      "Step: 5370, train/grad_norm: 2.7003288494142907e-09\n",
      "Step: 5370, train/learning_rate: 2.87006187136285e-05\n",
      "Step: 5370, train/epoch: 1.2779629230499268\n",
      "Step: 5380, train/loss: 0.0\n",
      "Step: 5380, train/grad_norm: 0.0007485946989618242\n",
      "Step: 5380, train/learning_rate: 2.8660955649684183e-05\n",
      "Step: 5380, train/epoch: 1.280342698097229\n",
      "Step: 5390, train/loss: 0.0\n",
      "Step: 5390, train/grad_norm: 5.2193044552950596e-08\n",
      "Step: 5390, train/learning_rate: 2.8621290766750462e-05\n",
      "Step: 5390, train/epoch: 1.2827224731445312\n",
      "Step: 5400, train/loss: 0.0\n",
      "Step: 5400, train/grad_norm: 9.108125711976278e-12\n",
      "Step: 5400, train/learning_rate: 2.8581627702806145e-05\n",
      "Step: 5400, train/epoch: 1.285102367401123\n",
      "Step: 5410, train/loss: 0.0\n",
      "Step: 5410, train/grad_norm: 1.4388711555568534e-08\n",
      "Step: 5410, train/learning_rate: 2.8541964638861828e-05\n",
      "Step: 5410, train/epoch: 1.2874821424484253\n",
      "Step: 5420, train/loss: 0.0\n",
      "Step: 5420, train/grad_norm: 9.382241046296258e-07\n",
      "Step: 5420, train/learning_rate: 2.8502299755928107e-05\n",
      "Step: 5420, train/epoch: 1.2898619174957275\n",
      "Step: 5430, train/loss: 0.0\n",
      "Step: 5430, train/grad_norm: 2.5923980501829647e-05\n",
      "Step: 5430, train/learning_rate: 2.846263669198379e-05\n",
      "Step: 5430, train/epoch: 1.2922418117523193\n",
      "Step: 5440, train/loss: 0.0\n",
      "Step: 5440, train/grad_norm: 1.7292782761302306e-09\n",
      "Step: 5440, train/learning_rate: 2.8422973628039472e-05\n",
      "Step: 5440, train/epoch: 1.2946215867996216\n",
      "Step: 5450, train/loss: 0.0\n",
      "Step: 5450, train/grad_norm: 8.605884754819826e-11\n",
      "Step: 5450, train/learning_rate: 2.838330874510575e-05\n",
      "Step: 5450, train/epoch: 1.2970014810562134\n",
      "Step: 5460, train/loss: 0.0\n",
      "Step: 5460, train/grad_norm: 6.479322287034073e-13\n",
      "Step: 5460, train/learning_rate: 2.8343645681161433e-05\n",
      "Step: 5460, train/epoch: 1.2993812561035156\n",
      "Step: 5470, train/loss: 0.0\n",
      "Step: 5470, train/grad_norm: 4.362966521398448e-08\n",
      "Step: 5470, train/learning_rate: 2.8303982617217116e-05\n",
      "Step: 5470, train/epoch: 1.3017610311508179\n",
      "Step: 5480, train/loss: 0.0\n",
      "Step: 5480, train/grad_norm: 8.019688246818646e-10\n",
      "Step: 5480, train/learning_rate: 2.8264317734283395e-05\n",
      "Step: 5480, train/epoch: 1.3041409254074097\n",
      "Step: 5490, train/loss: 0.0\n",
      "Step: 5490, train/grad_norm: 9.300109127252654e-09\n",
      "Step: 5490, train/learning_rate: 2.8224654670339078e-05\n",
      "Step: 5490, train/epoch: 1.306520700454712\n",
      "Step: 5500, train/loss: 0.0\n",
      "Step: 5500, train/grad_norm: 5.615423788185581e-07\n",
      "Step: 5500, train/learning_rate: 2.818499160639476e-05\n",
      "Step: 5500, train/epoch: 1.3089004755020142\n",
      "Step: 5510, train/loss: 0.0\n",
      "Step: 5510, train/grad_norm: 6.539345731582102e-10\n",
      "Step: 5510, train/learning_rate: 2.814532672346104e-05\n",
      "Step: 5510, train/epoch: 1.311280369758606\n",
      "Step: 5520, train/loss: 0.0\n",
      "Step: 5520, train/grad_norm: 1.9896780403816372e-11\n",
      "Step: 5520, train/learning_rate: 2.8105663659516722e-05\n",
      "Step: 5520, train/epoch: 1.3136601448059082\n",
      "Step: 5530, train/loss: 0.0\n",
      "Step: 5530, train/grad_norm: 9.880721796307057e-10\n",
      "Step: 5530, train/learning_rate: 2.8066000595572405e-05\n",
      "Step: 5530, train/epoch: 1.3160400390625\n",
      "Step: 5540, train/loss: 0.0\n",
      "Step: 5540, train/grad_norm: 4.6341863679799644e-09\n",
      "Step: 5540, train/learning_rate: 2.8026337531628087e-05\n",
      "Step: 5540, train/epoch: 1.3184198141098022\n",
      "Step: 5550, train/loss: 0.0\n",
      "Step: 5550, train/grad_norm: 1.6751243947510375e-08\n",
      "Step: 5550, train/learning_rate: 2.7986672648694366e-05\n",
      "Step: 5550, train/epoch: 1.3207995891571045\n",
      "Step: 5560, train/loss: 0.0\n",
      "Step: 5560, train/grad_norm: 9.237202114320553e-09\n",
      "Step: 5560, train/learning_rate: 2.794700958475005e-05\n",
      "Step: 5560, train/epoch: 1.3231794834136963\n",
      "Step: 5570, train/loss: 0.13279999792575836\n",
      "Step: 5570, train/grad_norm: 9.388728358317167e-05\n",
      "Step: 5570, train/learning_rate: 2.790734652080573e-05\n",
      "Step: 5570, train/epoch: 1.3255592584609985\n",
      "Step: 5580, train/loss: 0.0\n",
      "Step: 5580, train/grad_norm: 9.736419136208152e-12\n",
      "Step: 5580, train/learning_rate: 2.786768163787201e-05\n",
      "Step: 5580, train/epoch: 1.3279390335083008\n",
      "Step: 5590, train/loss: 0.0\n",
      "Step: 5590, train/grad_norm: 4.073043291530354e-10\n",
      "Step: 5590, train/learning_rate: 2.7828018573927693e-05\n",
      "Step: 5590, train/epoch: 1.3303189277648926\n",
      "Step: 5600, train/loss: 0.0\n",
      "Step: 5600, train/grad_norm: 3.0366312242513516e-10\n",
      "Step: 5600, train/learning_rate: 2.7788355509983376e-05\n",
      "Step: 5600, train/epoch: 1.3326987028121948\n",
      "Step: 5610, train/loss: 0.0\n",
      "Step: 5610, train/grad_norm: 1.020324180700527e-08\n",
      "Step: 5610, train/learning_rate: 2.7748690627049655e-05\n",
      "Step: 5610, train/epoch: 1.335078477859497\n",
      "Step: 5620, train/loss: 0.0\n",
      "Step: 5620, train/grad_norm: 6.713440825478756e-07\n",
      "Step: 5620, train/learning_rate: 2.7709027563105337e-05\n",
      "Step: 5620, train/epoch: 1.3374583721160889\n",
      "Step: 5630, train/loss: 0.0\n",
      "Step: 5630, train/grad_norm: 3.881305693509418e-11\n",
      "Step: 5630, train/learning_rate: 2.766936449916102e-05\n",
      "Step: 5630, train/epoch: 1.3398381471633911\n",
      "Step: 5640, train/loss: 0.0\n",
      "Step: 5640, train/grad_norm: 6.618182624151814e-08\n",
      "Step: 5640, train/learning_rate: 2.76296996162273e-05\n",
      "Step: 5640, train/epoch: 1.342218041419983\n",
      "Step: 5650, train/loss: 0.052299998700618744\n",
      "Step: 5650, train/grad_norm: 1.9246966985519975e-05\n",
      "Step: 5650, train/learning_rate: 2.759003655228298e-05\n",
      "Step: 5650, train/epoch: 1.3445978164672852\n",
      "Step: 5660, train/loss: 0.0\n",
      "Step: 5660, train/grad_norm: 1.0451271792444317e-10\n",
      "Step: 5660, train/learning_rate: 2.7550373488338664e-05\n",
      "Step: 5660, train/epoch: 1.3469775915145874\n",
      "Step: 5670, train/loss: 0.0\n",
      "Step: 5670, train/grad_norm: 0.0009209593990817666\n",
      "Step: 5670, train/learning_rate: 2.7510708605404943e-05\n",
      "Step: 5670, train/epoch: 1.3493574857711792\n",
      "Step: 5680, train/loss: 0.0\n",
      "Step: 5680, train/grad_norm: 0.00047412700951099396\n",
      "Step: 5680, train/learning_rate: 2.7471045541460626e-05\n",
      "Step: 5680, train/epoch: 1.3517372608184814\n",
      "Step: 5690, train/loss: 0.0\n",
      "Step: 5690, train/grad_norm: 2.4461756765958853e-05\n",
      "Step: 5690, train/learning_rate: 2.743138247751631e-05\n",
      "Step: 5690, train/epoch: 1.3541170358657837\n",
      "Step: 5700, train/loss: 0.0\n",
      "Step: 5700, train/grad_norm: 2.4587221147953642e-11\n",
      "Step: 5700, train/learning_rate: 2.7391717594582587e-05\n",
      "Step: 5700, train/epoch: 1.3564969301223755\n",
      "Step: 5710, train/loss: 0.0\n",
      "Step: 5710, train/grad_norm: 8.681236977281515e-06\n",
      "Step: 5710, train/learning_rate: 2.735205453063827e-05\n",
      "Step: 5710, train/epoch: 1.3588767051696777\n",
      "Step: 5720, train/loss: 9.999999747378752e-05\n",
      "Step: 5720, train/grad_norm: 9.560388036788936e-10\n",
      "Step: 5720, train/learning_rate: 2.7312391466693953e-05\n",
      "Step: 5720, train/epoch: 1.3612565994262695\n",
      "Step: 5730, train/loss: 0.0\n",
      "Step: 5730, train/grad_norm: 9.122348274104297e-06\n",
      "Step: 5730, train/learning_rate: 2.7272726583760232e-05\n",
      "Step: 5730, train/epoch: 1.3636363744735718\n",
      "Step: 5740, train/loss: 0.0\n",
      "Step: 5740, train/grad_norm: 2.6506074846111005e-07\n",
      "Step: 5740, train/learning_rate: 2.7233063519815914e-05\n",
      "Step: 5740, train/epoch: 1.366016149520874\n",
      "Step: 5750, train/loss: 0.0\n",
      "Step: 5750, train/grad_norm: 2.875964577597756e-09\n",
      "Step: 5750, train/learning_rate: 2.7193400455871597e-05\n",
      "Step: 5750, train/epoch: 1.3683960437774658\n",
      "Step: 5760, train/loss: 0.0\n",
      "Step: 5760, train/grad_norm: 1.0619928936250744e-08\n",
      "Step: 5760, train/learning_rate: 2.7153735572937876e-05\n",
      "Step: 5760, train/epoch: 1.370775818824768\n",
      "Step: 5770, train/loss: 0.0\n",
      "Step: 5770, train/grad_norm: 1.9922717989229177e-12\n",
      "Step: 5770, train/learning_rate: 2.711407250899356e-05\n",
      "Step: 5770, train/epoch: 1.3731555938720703\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 1.7531399862491526e-05\n",
      "Step: 5780, train/learning_rate: 2.707440944504924e-05\n",
      "Step: 5780, train/epoch: 1.375535488128662\n",
      "Step: 5790, train/loss: 0.0\n",
      "Step: 5790, train/grad_norm: 2.5663096536732155e-08\n",
      "Step: 5790, train/learning_rate: 2.703474456211552e-05\n",
      "Step: 5790, train/epoch: 1.3779152631759644\n",
      "Step: 5800, train/loss: 0.0\n",
      "Step: 5800, train/grad_norm: 9.155177949082827e-09\n",
      "Step: 5800, train/learning_rate: 2.6995081498171203e-05\n",
      "Step: 5800, train/epoch: 1.3802950382232666\n",
      "Step: 5810, train/loss: 0.0\n",
      "Step: 5810, train/grad_norm: 6.280857778317994e-12\n",
      "Step: 5810, train/learning_rate: 2.6955418434226885e-05\n",
      "Step: 5810, train/epoch: 1.3826749324798584\n",
      "Step: 5820, train/loss: 0.0\n",
      "Step: 5820, train/grad_norm: 1.0786375241877977e-06\n",
      "Step: 5820, train/learning_rate: 2.6915753551293164e-05\n",
      "Step: 5820, train/epoch: 1.3850547075271606\n",
      "Step: 5830, train/loss: 0.14059999585151672\n",
      "Step: 5830, train/grad_norm: 2.6538552901911316e-06\n",
      "Step: 5830, train/learning_rate: 2.6876090487348847e-05\n",
      "Step: 5830, train/epoch: 1.3874346017837524\n",
      "Step: 5840, train/loss: 0.0\n",
      "Step: 5840, train/grad_norm: 1.783348402284446e-08\n",
      "Step: 5840, train/learning_rate: 2.683642742340453e-05\n",
      "Step: 5840, train/epoch: 1.3898143768310547\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 8.702123159309849e-05\n",
      "Step: 5850, train/learning_rate: 2.679676254047081e-05\n",
      "Step: 5850, train/epoch: 1.392194151878357\n",
      "Step: 5860, train/loss: 0.0\n",
      "Step: 5860, train/grad_norm: 0.0001685632742010057\n",
      "Step: 5860, train/learning_rate: 2.675709947652649e-05\n",
      "Step: 5860, train/epoch: 1.3945740461349487\n",
      "Step: 5870, train/loss: 0.0\n",
      "Step: 5870, train/grad_norm: 6.106057844590396e-05\n",
      "Step: 5870, train/learning_rate: 2.6717436412582174e-05\n",
      "Step: 5870, train/epoch: 1.396953821182251\n",
      "Step: 5880, train/loss: 0.00039999998989515007\n",
      "Step: 5880, train/grad_norm: 2.0221168739453788e-08\n",
      "Step: 5880, train/learning_rate: 2.6677773348637857e-05\n",
      "Step: 5880, train/epoch: 1.3993335962295532\n",
      "Step: 5890, train/loss: 0.0\n",
      "Step: 5890, train/grad_norm: 1.1569073699035926e-08\n",
      "Step: 5890, train/learning_rate: 2.6638108465704136e-05\n",
      "Step: 5890, train/epoch: 1.401713490486145\n",
      "Step: 5900, train/loss: 0.0\n",
      "Step: 5900, train/grad_norm: 6.204903344603219e-12\n",
      "Step: 5900, train/learning_rate: 2.6598445401759818e-05\n",
      "Step: 5900, train/epoch: 1.4040932655334473\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 1.9437682800133027e-12\n",
      "Step: 5910, train/learning_rate: 2.65587823378155e-05\n",
      "Step: 5910, train/epoch: 1.406473159790039\n",
      "Step: 5920, train/loss: 0.0\n",
      "Step: 5920, train/grad_norm: 0.0005343725206330419\n",
      "Step: 5920, train/learning_rate: 2.651911745488178e-05\n",
      "Step: 5920, train/epoch: 1.4088529348373413\n",
      "Step: 5930, train/loss: 0.0\n",
      "Step: 5930, train/grad_norm: 1.1294433444053453e-10\n",
      "Step: 5930, train/learning_rate: 2.6479454390937462e-05\n",
      "Step: 5930, train/epoch: 1.4112327098846436\n",
      "Step: 5940, train/loss: 0.0\n",
      "Step: 5940, train/grad_norm: 7.856552315388399e-07\n",
      "Step: 5940, train/learning_rate: 2.6439791326993145e-05\n",
      "Step: 5940, train/epoch: 1.4136126041412354\n",
      "Step: 5950, train/loss: 0.0\n",
      "Step: 5950, train/grad_norm: 3.8861195150730055e-08\n",
      "Step: 5950, train/learning_rate: 2.6400126444059424e-05\n",
      "Step: 5950, train/epoch: 1.4159923791885376\n",
      "Step: 5960, train/loss: 0.0\n",
      "Step: 5960, train/grad_norm: 6.124429319243063e-08\n",
      "Step: 5960, train/learning_rate: 2.6360463380115107e-05\n",
      "Step: 5960, train/epoch: 1.4183721542358398\n",
      "Step: 5970, train/loss: 0.0\n",
      "Step: 5970, train/grad_norm: 0.00021393090719357133\n",
      "Step: 5970, train/learning_rate: 2.632080031617079e-05\n",
      "Step: 5970, train/epoch: 1.4207520484924316\n",
      "Step: 5980, train/loss: 0.0\n",
      "Step: 5980, train/grad_norm: 1.335634802757113e-08\n",
      "Step: 5980, train/learning_rate: 2.628113543323707e-05\n",
      "Step: 5980, train/epoch: 1.4231318235397339\n",
      "Step: 5990, train/loss: 0.0\n",
      "Step: 5990, train/grad_norm: 1.950278829099261e-06\n",
      "Step: 5990, train/learning_rate: 2.624147236929275e-05\n",
      "Step: 5990, train/epoch: 1.4255117177963257\n",
      "Step: 6000, train/loss: 0.0\n",
      "Step: 6000, train/grad_norm: 2.307961981251605e-12\n",
      "Step: 6000, train/learning_rate: 2.6201809305348434e-05\n",
      "Step: 6000, train/epoch: 1.427891492843628\n",
      "Step: 6010, train/loss: 0.0\n",
      "Step: 6010, train/grad_norm: 1.0028865758682226e-11\n",
      "Step: 6010, train/learning_rate: 2.6162144422414713e-05\n",
      "Step: 6010, train/epoch: 1.4302712678909302\n",
      "Step: 6020, train/loss: 0.0\n",
      "Step: 6020, train/grad_norm: 2.4965394335163538e-12\n",
      "Step: 6020, train/learning_rate: 2.6122481358470395e-05\n",
      "Step: 6020, train/epoch: 1.432651162147522\n",
      "Step: 6030, train/loss: 0.0\n",
      "Step: 6030, train/grad_norm: 3.7739521794102893e-13\n",
      "Step: 6030, train/learning_rate: 2.6082818294526078e-05\n",
      "Step: 6030, train/epoch: 1.4350309371948242\n",
      "Step: 6040, train/loss: 0.0\n",
      "Step: 6040, train/grad_norm: 4.037841675602616e-12\n",
      "Step: 6040, train/learning_rate: 2.6043153411592357e-05\n",
      "Step: 6040, train/epoch: 1.4374107122421265\n",
      "Step: 6050, train/loss: 0.0\n",
      "Step: 6050, train/grad_norm: 2.2278846822931797e-10\n",
      "Step: 6050, train/learning_rate: 2.600349034764804e-05\n",
      "Step: 6050, train/epoch: 1.4397906064987183\n",
      "Step: 6060, train/loss: 0.0\n",
      "Step: 6060, train/grad_norm: 1.1366571017745741e-09\n",
      "Step: 6060, train/learning_rate: 2.5963827283703722e-05\n",
      "Step: 6060, train/epoch: 1.4421703815460205\n",
      "Step: 6070, train/loss: 0.0\n",
      "Step: 6070, train/grad_norm: 8.289390279969666e-06\n",
      "Step: 6070, train/learning_rate: 2.592416240077e-05\n",
      "Step: 6070, train/epoch: 1.4445501565933228\n",
      "Step: 6080, train/loss: 0.0\n",
      "Step: 6080, train/grad_norm: 1.307790542526277e-09\n",
      "Step: 6080, train/learning_rate: 2.5884499336825684e-05\n",
      "Step: 6080, train/epoch: 1.4469300508499146\n",
      "Step: 6090, train/loss: 0.0\n",
      "Step: 6090, train/grad_norm: 2.095709161764603e-09\n",
      "Step: 6090, train/learning_rate: 2.5844836272881366e-05\n",
      "Step: 6090, train/epoch: 1.4493098258972168\n",
      "Step: 6100, train/loss: 0.0\n",
      "Step: 6100, train/grad_norm: 8.659742434247164e-08\n",
      "Step: 6100, train/learning_rate: 2.5805171389947645e-05\n",
      "Step: 6100, train/epoch: 1.4516897201538086\n",
      "Step: 6110, train/loss: 0.0\n",
      "Step: 6110, train/grad_norm: 9.069886175439024e-09\n",
      "Step: 6110, train/learning_rate: 2.5765508326003328e-05\n",
      "Step: 6110, train/epoch: 1.4540694952011108\n",
      "Step: 6120, train/loss: 0.0\n",
      "Step: 6120, train/grad_norm: 2.049801127446127e-11\n",
      "Step: 6120, train/learning_rate: 2.572584526205901e-05\n",
      "Step: 6120, train/epoch: 1.456449270248413\n",
      "Step: 6130, train/loss: 0.0\n",
      "Step: 6130, train/grad_norm: 1.5840455613869153e-09\n",
      "Step: 6130, train/learning_rate: 2.568618037912529e-05\n",
      "Step: 6130, train/epoch: 1.4588291645050049\n",
      "Step: 6140, train/loss: 0.0\n",
      "Step: 6140, train/grad_norm: 4.057405106339118e-11\n",
      "Step: 6140, train/learning_rate: 2.5646517315180972e-05\n",
      "Step: 6140, train/epoch: 1.4612089395523071\n",
      "Step: 6150, train/loss: 0.0\n",
      "Step: 6150, train/grad_norm: 5.792460955333922e-10\n",
      "Step: 6150, train/learning_rate: 2.5606854251236655e-05\n",
      "Step: 6150, train/epoch: 1.4635887145996094\n",
      "Step: 6160, train/loss: 0.0\n",
      "Step: 6160, train/grad_norm: 1.52198142977511e-10\n",
      "Step: 6160, train/learning_rate: 2.5567189368302934e-05\n",
      "Step: 6160, train/epoch: 1.4659686088562012\n",
      "Step: 6170, train/loss: 0.0\n",
      "Step: 6170, train/grad_norm: 7.918337751981752e-14\n",
      "Step: 6170, train/learning_rate: 2.5527526304358616e-05\n",
      "Step: 6170, train/epoch: 1.4683483839035034\n",
      "Step: 6180, train/loss: 0.0\n",
      "Step: 6180, train/grad_norm: 8.814715992712385e-13\n",
      "Step: 6180, train/learning_rate: 2.54878632404143e-05\n",
      "Step: 6180, train/epoch: 1.4707282781600952\n",
      "Step: 6190, train/loss: 0.0\n",
      "Step: 6190, train/grad_norm: 4.419759989104932e-07\n",
      "Step: 6190, train/learning_rate: 2.544820017646998e-05\n",
      "Step: 6190, train/epoch: 1.4731080532073975\n",
      "Step: 6200, train/loss: 0.0\n",
      "Step: 6200, train/grad_norm: 4.715412435518651e-11\n",
      "Step: 6200, train/learning_rate: 2.540853529353626e-05\n",
      "Step: 6200, train/epoch: 1.4754878282546997\n",
      "Step: 6210, train/loss: 0.0\n",
      "Step: 6210, train/grad_norm: 3.4936469717194996e-08\n",
      "Step: 6210, train/learning_rate: 2.5368872229591943e-05\n",
      "Step: 6210, train/epoch: 1.4778677225112915\n",
      "Step: 6220, train/loss: 0.0\n",
      "Step: 6220, train/grad_norm: 4.091548974627379e-11\n",
      "Step: 6220, train/learning_rate: 2.5329209165647626e-05\n",
      "Step: 6220, train/epoch: 1.4802474975585938\n",
      "Step: 6230, train/loss: 0.0\n",
      "Step: 6230, train/grad_norm: 1.7876321950494045e-11\n",
      "Step: 6230, train/learning_rate: 2.5289544282713905e-05\n",
      "Step: 6230, train/epoch: 1.482627272605896\n",
      "Step: 6240, train/loss: 0.0\n",
      "Step: 6240, train/grad_norm: 5.7500650074704396e-11\n",
      "Step: 6240, train/learning_rate: 2.5249881218769588e-05\n",
      "Step: 6240, train/epoch: 1.4850071668624878\n",
      "Step: 6250, train/loss: 0.00800000037997961\n",
      "Step: 6250, train/grad_norm: 1.6049221673863912e-10\n",
      "Step: 6250, train/learning_rate: 2.521021815482527e-05\n",
      "Step: 6250, train/epoch: 1.48738694190979\n",
      "Step: 6260, train/loss: 0.0\n",
      "Step: 6260, train/grad_norm: 2.029591383845286e-09\n",
      "Step: 6260, train/learning_rate: 2.517055327189155e-05\n",
      "Step: 6260, train/epoch: 1.4897668361663818\n",
      "Step: 6270, train/loss: 9.999999747378752e-05\n",
      "Step: 6270, train/grad_norm: 1.020803556328533e-15\n",
      "Step: 6270, train/learning_rate: 2.5130890207947232e-05\n",
      "Step: 6270, train/epoch: 1.492146611213684\n",
      "Step: 6280, train/loss: 0.06520000100135803\n",
      "Step: 6280, train/grad_norm: 7.06688922491594e-07\n",
      "Step: 6280, train/learning_rate: 2.5091227144002914e-05\n",
      "Step: 6280, train/epoch: 1.4945263862609863\n",
      "Step: 6290, train/loss: 0.0\n",
      "Step: 6290, train/grad_norm: 6.414711606339552e-08\n",
      "Step: 6290, train/learning_rate: 2.5051562261069193e-05\n",
      "Step: 6290, train/epoch: 1.4969062805175781\n",
      "Step: 6300, train/loss: 0.0\n",
      "Step: 6300, train/grad_norm: 3.2615396494684035e-11\n",
      "Step: 6300, train/learning_rate: 2.5011899197124876e-05\n",
      "Step: 6300, train/epoch: 1.4992860555648804\n",
      "Step: 6310, train/loss: 0.0\n",
      "Step: 6310, train/grad_norm: 4.709937968527811e-07\n",
      "Step: 6310, train/learning_rate: 2.497223613318056e-05\n",
      "Step: 6310, train/epoch: 1.5016658306121826\n",
      "Step: 6320, train/loss: 0.0\n",
      "Step: 6320, train/grad_norm: 2.4658967959112488e-05\n",
      "Step: 6320, train/learning_rate: 2.4932571250246838e-05\n",
      "Step: 6320, train/epoch: 1.5040457248687744\n",
      "Step: 6330, train/loss: 0.0\n",
      "Step: 6330, train/grad_norm: 0.0027483648154884577\n",
      "Step: 6330, train/learning_rate: 2.489290818630252e-05\n",
      "Step: 6330, train/epoch: 1.5064254999160767\n",
      "Step: 6340, train/loss: 0.0\n",
      "Step: 6340, train/grad_norm: 7.016460763092924e-12\n",
      "Step: 6340, train/learning_rate: 2.4853245122358203e-05\n",
      "Step: 6340, train/epoch: 1.508805274963379\n",
      "Step: 6350, train/loss: 0.0\n",
      "Step: 6350, train/grad_norm: 1.4568023232186533e-08\n",
      "Step: 6350, train/learning_rate: 2.4813580239424482e-05\n",
      "Step: 6350, train/epoch: 1.5111851692199707\n",
      "Step: 6360, train/loss: 0.0\n",
      "Step: 6360, train/grad_norm: 1.4370690193388214e-09\n",
      "Step: 6360, train/learning_rate: 2.4773917175480165e-05\n",
      "Step: 6360, train/epoch: 1.513564944267273\n",
      "Step: 6370, train/loss: 0.0\n",
      "Step: 6370, train/grad_norm: 8.721185622562189e-06\n",
      "Step: 6370, train/learning_rate: 2.4734254111535847e-05\n",
      "Step: 6370, train/epoch: 1.5159448385238647\n",
      "Step: 6380, train/loss: 0.0\n",
      "Step: 6380, train/grad_norm: 1.0785969556739716e-12\n",
      "Step: 6380, train/learning_rate: 2.4694589228602126e-05\n",
      "Step: 6380, train/epoch: 1.518324613571167\n",
      "Step: 6390, train/loss: 0.0\n",
      "Step: 6390, train/grad_norm: 6.951520845177583e-06\n",
      "Step: 6390, train/learning_rate: 2.465492616465781e-05\n",
      "Step: 6390, train/epoch: 1.5207043886184692\n",
      "Step: 6400, train/loss: 0.0\n",
      "Step: 6400, train/grad_norm: 7.451452788131974e-10\n",
      "Step: 6400, train/learning_rate: 2.461526310071349e-05\n",
      "Step: 6400, train/epoch: 1.523084282875061\n",
      "Step: 6410, train/loss: 0.0\n",
      "Step: 6410, train/grad_norm: 5.164712546523731e-10\n",
      "Step: 6410, train/learning_rate: 2.457559821777977e-05\n",
      "Step: 6410, train/epoch: 1.5254640579223633\n",
      "Step: 6420, train/loss: 0.0\n",
      "Step: 6420, train/grad_norm: 1.5625474816260976e-06\n",
      "Step: 6420, train/learning_rate: 2.4535935153835453e-05\n",
      "Step: 6420, train/epoch: 1.5278438329696655\n",
      "Step: 6430, train/loss: 0.0\n",
      "Step: 6430, train/grad_norm: 1.2814671990213355e-08\n",
      "Step: 6430, train/learning_rate: 2.4496272089891136e-05\n",
      "Step: 6430, train/epoch: 1.5302237272262573\n",
      "Step: 6440, train/loss: 0.0\n",
      "Step: 6440, train/grad_norm: 1.4422067806663108e-07\n",
      "Step: 6440, train/learning_rate: 2.4456607206957415e-05\n",
      "Step: 6440, train/epoch: 1.5326035022735596\n",
      "Step: 6450, train/loss: 0.0\n",
      "Step: 6450, train/grad_norm: 1.107917614717735e-05\n",
      "Step: 6450, train/learning_rate: 2.4416944143013097e-05\n",
      "Step: 6450, train/epoch: 1.5349833965301514\n",
      "Step: 6460, train/loss: 0.0\n",
      "Step: 6460, train/grad_norm: 1.8042296687781345e-06\n",
      "Step: 6460, train/learning_rate: 2.437728107906878e-05\n",
      "Step: 6460, train/epoch: 1.5373631715774536\n",
      "Step: 6470, train/loss: 0.0\n",
      "Step: 6470, train/grad_norm: 2.5735780582181178e-05\n",
      "Step: 6470, train/learning_rate: 2.433761619613506e-05\n",
      "Step: 6470, train/epoch: 1.5397429466247559\n",
      "Step: 6480, train/loss: 0.0\n",
      "Step: 6480, train/grad_norm: 1.4360621580777888e-09\n",
      "Step: 6480, train/learning_rate: 2.429795313219074e-05\n",
      "Step: 6480, train/epoch: 1.5421228408813477\n",
      "Step: 6490, train/loss: 0.0\n",
      "Step: 6490, train/grad_norm: 1.865847387705344e-08\n",
      "Step: 6490, train/learning_rate: 2.4258290068246424e-05\n",
      "Step: 6490, train/epoch: 1.54450261592865\n",
      "Step: 6500, train/loss: 0.0\n",
      "Step: 6500, train/grad_norm: 9.668966072240437e-08\n",
      "Step: 6500, train/learning_rate: 2.4218625185312703e-05\n",
      "Step: 6500, train/epoch: 1.5468823909759521\n",
      "Step: 6510, train/loss: 0.0\n",
      "Step: 6510, train/grad_norm: 1.3935955678334722e-07\n",
      "Step: 6510, train/learning_rate: 2.4178962121368386e-05\n",
      "Step: 6510, train/epoch: 1.549262285232544\n",
      "Step: 6520, train/loss: 0.0\n",
      "Step: 6520, train/grad_norm: 2.498819640095462e-06\n",
      "Step: 6520, train/learning_rate: 2.413929905742407e-05\n",
      "Step: 6520, train/epoch: 1.5516420602798462\n",
      "Step: 6530, train/loss: 0.0\n",
      "Step: 6530, train/grad_norm: 1.2549534176287125e-06\n",
      "Step: 6530, train/learning_rate: 2.409963599347975e-05\n",
      "Step: 6530, train/epoch: 1.5540218353271484\n",
      "Step: 6540, train/loss: 0.0\n",
      "Step: 6540, train/grad_norm: 2.6001002240150228e-08\n",
      "Step: 6540, train/learning_rate: 2.405997111054603e-05\n",
      "Step: 6540, train/epoch: 1.5564017295837402\n",
      "Step: 6550, train/loss: 9.999999747378752e-05\n",
      "Step: 6550, train/grad_norm: 1.260883072973229e-05\n",
      "Step: 6550, train/learning_rate: 2.4020308046601713e-05\n",
      "Step: 6550, train/epoch: 1.5587815046310425\n",
      "Step: 6560, train/loss: 0.0\n",
      "Step: 6560, train/grad_norm: 5.805946279302532e-10\n",
      "Step: 6560, train/learning_rate: 2.3980644982657395e-05\n",
      "Step: 6560, train/epoch: 1.5611613988876343\n",
      "Step: 6570, train/loss: 0.0\n",
      "Step: 6570, train/grad_norm: 8.723711197866635e-10\n",
      "Step: 6570, train/learning_rate: 2.3940980099723674e-05\n",
      "Step: 6570, train/epoch: 1.5635411739349365\n",
      "Step: 6580, train/loss: 0.0\n",
      "Step: 6580, train/grad_norm: 3.0858450001416916e-15\n",
      "Step: 6580, train/learning_rate: 2.3901317035779357e-05\n",
      "Step: 6580, train/epoch: 1.5659209489822388\n",
      "Step: 6590, train/loss: 0.0\n",
      "Step: 6590, train/grad_norm: 1.0437791742035074e-09\n",
      "Step: 6590, train/learning_rate: 2.386165397183504e-05\n",
      "Step: 6590, train/epoch: 1.5683008432388306\n",
      "Step: 6600, train/loss: 0.0\n",
      "Step: 6600, train/grad_norm: 1.4339802956175873e-10\n",
      "Step: 6600, train/learning_rate: 2.382198908890132e-05\n",
      "Step: 6600, train/epoch: 1.5706806182861328\n",
      "Step: 6610, train/loss: 0.0\n",
      "Step: 6610, train/grad_norm: 1.1856270088969612e-12\n",
      "Step: 6610, train/learning_rate: 2.3782326024957e-05\n",
      "Step: 6610, train/epoch: 1.573060393333435\n",
      "Step: 6620, train/loss: 0.0\n",
      "Step: 6620, train/grad_norm: 3.5399052142539037e-20\n",
      "Step: 6620, train/learning_rate: 2.3742662961012684e-05\n",
      "Step: 6620, train/epoch: 1.5754402875900269\n",
      "Step: 6630, train/loss: 0.0\n",
      "Step: 6630, train/grad_norm: 1.13438633351148e-14\n",
      "Step: 6630, train/learning_rate: 2.3702998078078963e-05\n",
      "Step: 6630, train/epoch: 1.577820062637329\n",
      "Step: 6640, train/loss: 0.0\n",
      "Step: 6640, train/grad_norm: 5.807455880326904e-16\n",
      "Step: 6640, train/learning_rate: 2.3663335014134645e-05\n",
      "Step: 6640, train/epoch: 1.580199956893921\n",
      "Step: 6650, train/loss: 0.0\n",
      "Step: 6650, train/grad_norm: 2.0270611855721654e-09\n",
      "Step: 6650, train/learning_rate: 2.3623671950190328e-05\n",
      "Step: 6650, train/epoch: 1.5825797319412231\n",
      "Step: 6660, train/loss: 0.0\n",
      "Step: 6660, train/grad_norm: 2.3698624659895806e-11\n",
      "Step: 6660, train/learning_rate: 2.3584007067256607e-05\n",
      "Step: 6660, train/epoch: 1.5849595069885254\n",
      "Step: 6670, train/loss: 0.0\n",
      "Step: 6670, train/grad_norm: 9.80697586783208e-06\n",
      "Step: 6670, train/learning_rate: 2.354434400331229e-05\n",
      "Step: 6670, train/epoch: 1.5873394012451172\n",
      "Step: 6680, train/loss: 0.0\n",
      "Step: 6680, train/grad_norm: 1.458133169762732e-09\n",
      "Step: 6680, train/learning_rate: 2.3504680939367972e-05\n",
      "Step: 6680, train/epoch: 1.5897191762924194\n",
      "Step: 6690, train/loss: 0.17270000278949738\n",
      "Step: 6690, train/grad_norm: 1.9395317296666548e-10\n",
      "Step: 6690, train/learning_rate: 2.346501605643425e-05\n",
      "Step: 6690, train/epoch: 1.5920989513397217\n",
      "Step: 6700, train/loss: 0.0\n",
      "Step: 6700, train/grad_norm: 2.596348167571705e-07\n",
      "Step: 6700, train/learning_rate: 2.3425352992489934e-05\n",
      "Step: 6700, train/epoch: 1.5944788455963135\n",
      "Step: 6710, train/loss: 0.0\n",
      "Step: 6710, train/grad_norm: 2.8998218112974428e-05\n",
      "Step: 6710, train/learning_rate: 2.3385689928545617e-05\n",
      "Step: 6710, train/epoch: 1.5968586206436157\n",
      "Step: 6720, train/loss: 0.21559999883174896\n",
      "Step: 6720, train/grad_norm: 72.99868774414062\n",
      "Step: 6720, train/learning_rate: 2.3346025045611896e-05\n",
      "Step: 6720, train/epoch: 1.5992385149002075\n",
      "Step: 6730, train/loss: 0.0\n",
      "Step: 6730, train/grad_norm: 6.0976921290034625e-09\n",
      "Step: 6730, train/learning_rate: 2.3306361981667578e-05\n",
      "Step: 6730, train/epoch: 1.6016182899475098\n",
      "Step: 6740, train/loss: 0.0\n",
      "Step: 6740, train/grad_norm: 1.2805968196971662e-07\n",
      "Step: 6740, train/learning_rate: 2.326669891772326e-05\n",
      "Step: 6740, train/epoch: 1.603998064994812\n",
      "Step: 6750, train/loss: 0.0\n",
      "Step: 6750, train/grad_norm: 2.9002147905465847e-11\n",
      "Step: 6750, train/learning_rate: 2.322703403478954e-05\n",
      "Step: 6750, train/epoch: 1.6063779592514038\n",
      "Step: 6760, train/loss: 0.0\n",
      "Step: 6760, train/grad_norm: 7.792201606182658e-11\n",
      "Step: 6760, train/learning_rate: 2.3187370970845222e-05\n",
      "Step: 6760, train/epoch: 1.608757734298706\n",
      "Step: 6770, train/loss: 0.0\n",
      "Step: 6770, train/grad_norm: 1.651537386666499e-13\n",
      "Step: 6770, train/learning_rate: 2.3147707906900905e-05\n",
      "Step: 6770, train/epoch: 1.6111375093460083\n",
      "Step: 6780, train/loss: 0.0\n",
      "Step: 6780, train/grad_norm: 6.575636035677235e-09\n",
      "Step: 6780, train/learning_rate: 2.3108043023967184e-05\n",
      "Step: 6780, train/epoch: 1.6135174036026\n",
      "Step: 6790, train/loss: 0.0\n",
      "Step: 6790, train/grad_norm: 2.1626793511586584e-07\n",
      "Step: 6790, train/learning_rate: 2.3068379960022867e-05\n",
      "Step: 6790, train/epoch: 1.6158971786499023\n",
      "Step: 6800, train/loss: 0.0\n",
      "Step: 6800, train/grad_norm: 4.1440085141175587e-08\n",
      "Step: 6800, train/learning_rate: 2.302871689607855e-05\n",
      "Step: 6800, train/epoch: 1.6182769536972046\n",
      "Step: 6810, train/loss: 0.0\n",
      "Step: 6810, train/grad_norm: 1.1311241526756888e-10\n",
      "Step: 6810, train/learning_rate: 2.298905201314483e-05\n",
      "Step: 6810, train/epoch: 1.6206568479537964\n",
      "Step: 6820, train/loss: 0.0\n",
      "Step: 6820, train/grad_norm: 2.7478696651428436e-08\n",
      "Step: 6820, train/learning_rate: 2.294938894920051e-05\n",
      "Step: 6820, train/epoch: 1.6230366230010986\n",
      "Step: 6830, train/loss: 0.0\n",
      "Step: 6830, train/grad_norm: 5.4542219629638566e-08\n",
      "Step: 6830, train/learning_rate: 2.2909725885256194e-05\n",
      "Step: 6830, train/epoch: 1.6254165172576904\n",
      "Step: 6840, train/loss: 0.0\n",
      "Step: 6840, train/grad_norm: 1.100761892303126e-05\n",
      "Step: 6840, train/learning_rate: 2.2870061002322473e-05\n",
      "Step: 6840, train/epoch: 1.6277962923049927\n",
      "Step: 6850, train/loss: 0.0\n",
      "Step: 6850, train/grad_norm: 2.4681421351147037e-08\n",
      "Step: 6850, train/learning_rate: 2.2830397938378155e-05\n",
      "Step: 6850, train/epoch: 1.630176067352295\n",
      "Step: 6860, train/loss: 0.0\n",
      "Step: 6860, train/grad_norm: 4.0227669728665205e-07\n",
      "Step: 6860, train/learning_rate: 2.2790734874433838e-05\n",
      "Step: 6860, train/epoch: 1.6325559616088867\n",
      "Step: 6870, train/loss: 0.0\n",
      "Step: 6870, train/grad_norm: 6.631400850665159e-08\n",
      "Step: 6870, train/learning_rate: 2.275107181048952e-05\n",
      "Step: 6870, train/epoch: 1.634935736656189\n",
      "Step: 6880, train/loss: 0.0\n",
      "Step: 6880, train/grad_norm: 2.3150642514124797e-10\n",
      "Step: 6880, train/learning_rate: 2.27114069275558e-05\n",
      "Step: 6880, train/epoch: 1.6373155117034912\n",
      "Step: 6890, train/loss: 0.0\n",
      "Step: 6890, train/grad_norm: 1.0033604169734645e-08\n",
      "Step: 6890, train/learning_rate: 2.2671743863611482e-05\n",
      "Step: 6890, train/epoch: 1.639695405960083\n",
      "Step: 6900, train/loss: 0.0\n",
      "Step: 6900, train/grad_norm: 2.297620600721917e-11\n",
      "Step: 6900, train/learning_rate: 2.2632080799667165e-05\n",
      "Step: 6900, train/epoch: 1.6420751810073853\n",
      "Step: 6910, train/loss: 0.04650000110268593\n",
      "Step: 6910, train/grad_norm: 3.180547594183736e-07\n",
      "Step: 6910, train/learning_rate: 2.2592415916733444e-05\n",
      "Step: 6910, train/epoch: 1.644455075263977\n",
      "Step: 6920, train/loss: 0.0\n",
      "Step: 6920, train/grad_norm: 6.822282472285224e-08\n",
      "Step: 6920, train/learning_rate: 2.2552752852789126e-05\n",
      "Step: 6920, train/epoch: 1.6468348503112793\n",
      "Step: 6930, train/loss: 0.0\n",
      "Step: 6930, train/grad_norm: 5.320170970435356e-08\n",
      "Step: 6930, train/learning_rate: 2.251308978884481e-05\n",
      "Step: 6930, train/epoch: 1.6492146253585815\n",
      "Step: 6940, train/loss: 0.0\n",
      "Step: 6940, train/grad_norm: 7.656397428945638e-06\n",
      "Step: 6940, train/learning_rate: 2.2473424905911088e-05\n",
      "Step: 6940, train/epoch: 1.6515945196151733\n",
      "Step: 6950, train/loss: 0.0\n",
      "Step: 6950, train/grad_norm: 5.870013808362273e-09\n",
      "Step: 6950, train/learning_rate: 2.243376184196677e-05\n",
      "Step: 6950, train/epoch: 1.6539742946624756\n",
      "Step: 6960, train/loss: 0.0\n",
      "Step: 6960, train/grad_norm: 1.8414764781482518e-05\n",
      "Step: 6960, train/learning_rate: 2.2394098778022453e-05\n",
      "Step: 6960, train/epoch: 1.6563540697097778\n",
      "Step: 6970, train/loss: 0.06210000067949295\n",
      "Step: 6970, train/grad_norm: 8.101460480247624e-07\n",
      "Step: 6970, train/learning_rate: 2.2354433895088732e-05\n",
      "Step: 6970, train/epoch: 1.6587339639663696\n",
      "Step: 6980, train/loss: 0.0\n",
      "Step: 6980, train/grad_norm: 6.121125295521779e-08\n",
      "Step: 6980, train/learning_rate: 2.2314770831144415e-05\n",
      "Step: 6980, train/epoch: 1.6611137390136719\n",
      "Step: 6990, train/loss: 9.999999747378752e-05\n",
      "Step: 6990, train/grad_norm: 1.8875076648328104e-06\n",
      "Step: 6990, train/learning_rate: 2.2275107767200097e-05\n",
      "Step: 6990, train/epoch: 1.6634936332702637\n",
      "Step: 7000, train/loss: 0.4099999964237213\n",
      "Step: 7000, train/grad_norm: 3.7937246588626294e-07\n",
      "Step: 7000, train/learning_rate: 2.2235442884266376e-05\n",
      "Step: 7000, train/epoch: 1.665873408317566\n",
      "Step: 7010, train/loss: 0.0\n",
      "Step: 7010, train/grad_norm: 1.134485160037002e-06\n",
      "Step: 7010, train/learning_rate: 2.219577982032206e-05\n",
      "Step: 7010, train/epoch: 1.6682531833648682\n",
      "Step: 7020, train/loss: 0.10000000149011612\n",
      "Step: 7020, train/grad_norm: 4.0318737931643867e-11\n",
      "Step: 7020, train/learning_rate: 2.2156116756377742e-05\n",
      "Step: 7020, train/epoch: 1.67063307762146\n",
      "Step: 7030, train/loss: 0.0\n",
      "Step: 7030, train/grad_norm: 4.1893336130867376e-13\n",
      "Step: 7030, train/learning_rate: 2.211645187344402e-05\n",
      "Step: 7030, train/epoch: 1.6730128526687622\n",
      "Step: 7040, train/loss: 0.24690000712871552\n",
      "Step: 7040, train/grad_norm: 161.5388946533203\n",
      "Step: 7040, train/learning_rate: 2.2076788809499703e-05\n",
      "Step: 7040, train/epoch: 1.6753926277160645\n",
      "Step: 7050, train/loss: 0.0\n",
      "Step: 7050, train/grad_norm: 8.823608510510894e-12\n",
      "Step: 7050, train/learning_rate: 2.2037125745555386e-05\n",
      "Step: 7050, train/epoch: 1.6777725219726562\n",
      "Step: 7060, train/loss: 0.0\n",
      "Step: 7060, train/grad_norm: 1.3067220638873778e-09\n",
      "Step: 7060, train/learning_rate: 2.1997460862621665e-05\n",
      "Step: 7060, train/epoch: 1.6801522970199585\n",
      "Step: 7070, train/loss: 0.0\n",
      "Step: 7070, train/grad_norm: 7.525086781999535e-14\n",
      "Step: 7070, train/learning_rate: 2.1957797798677348e-05\n",
      "Step: 7070, train/epoch: 1.6825320720672607\n",
      "Step: 7080, train/loss: 0.0\n",
      "Step: 7080, train/grad_norm: 3.1797702249036774e-09\n",
      "Step: 7080, train/learning_rate: 2.191813473473303e-05\n",
      "Step: 7080, train/epoch: 1.6849119663238525\n",
      "Step: 7090, train/loss: 0.0\n",
      "Step: 7090, train/grad_norm: 3.3635477736382313e-10\n",
      "Step: 7090, train/learning_rate: 2.187846985179931e-05\n",
      "Step: 7090, train/epoch: 1.6872917413711548\n",
      "Step: 7100, train/loss: 0.0\n",
      "Step: 7100, train/grad_norm: 2.4167793100104973e-08\n",
      "Step: 7100, train/learning_rate: 2.1838806787854992e-05\n",
      "Step: 7100, train/epoch: 1.6896716356277466\n",
      "Step: 7110, train/loss: 0.0\n",
      "Step: 7110, train/grad_norm: 2.695914602668381e-10\n",
      "Step: 7110, train/learning_rate: 2.1799143723910674e-05\n",
      "Step: 7110, train/epoch: 1.6920514106750488\n",
      "Step: 7120, train/loss: 0.0\n",
      "Step: 7120, train/grad_norm: 1.231993063122161e-09\n",
      "Step: 7120, train/learning_rate: 2.1759478840976954e-05\n",
      "Step: 7120, train/epoch: 1.694431185722351\n",
      "Step: 7130, train/loss: 0.0\n",
      "Step: 7130, train/grad_norm: 5.793516274360533e-12\n",
      "Step: 7130, train/learning_rate: 2.1719815777032636e-05\n",
      "Step: 7130, train/epoch: 1.6968110799789429\n",
      "Step: 7140, train/loss: 0.0\n",
      "Step: 7140, train/grad_norm: 1.5417250809335314e-09\n",
      "Step: 7140, train/learning_rate: 2.168015271308832e-05\n",
      "Step: 7140, train/epoch: 1.6991908550262451\n",
      "Step: 7150, train/loss: 0.0\n",
      "Step: 7150, train/grad_norm: 0.004672834649682045\n",
      "Step: 7150, train/learning_rate: 2.1640487830154598e-05\n",
      "Step: 7150, train/epoch: 1.7015706300735474\n",
      "Step: 7160, train/loss: 0.0\n",
      "Step: 7160, train/grad_norm: 2.7413005199150575e-10\n",
      "Step: 7160, train/learning_rate: 2.160082476621028e-05\n",
      "Step: 7160, train/epoch: 1.7039505243301392\n",
      "Step: 7170, train/loss: 0.0\n",
      "Step: 7170, train/grad_norm: 2.2558213075285494e-09\n",
      "Step: 7170, train/learning_rate: 2.1561161702265963e-05\n",
      "Step: 7170, train/epoch: 1.7063302993774414\n",
      "Step: 7180, train/loss: 0.0\n",
      "Step: 7180, train/grad_norm: 8.012374652643928e-10\n",
      "Step: 7180, train/learning_rate: 2.1521496819332242e-05\n",
      "Step: 7180, train/epoch: 1.7087101936340332\n",
      "Step: 7190, train/loss: 0.0\n",
      "Step: 7190, train/grad_norm: 3.815645322902128e-05\n",
      "Step: 7190, train/learning_rate: 2.1481833755387925e-05\n",
      "Step: 7190, train/epoch: 1.7110899686813354\n",
      "Step: 7200, train/loss: 0.0\n",
      "Step: 7200, train/grad_norm: 5.5547715732684466e-11\n",
      "Step: 7200, train/learning_rate: 2.1442170691443607e-05\n",
      "Step: 7200, train/epoch: 1.7134697437286377\n",
      "Step: 7210, train/loss: 0.0\n",
      "Step: 7210, train/grad_norm: 1.8582717829784823e-12\n",
      "Step: 7210, train/learning_rate: 2.140250762749929e-05\n",
      "Step: 7210, train/epoch: 1.7158496379852295\n",
      "Step: 7220, train/loss: 0.0\n",
      "Step: 7220, train/grad_norm: 1.1937606458900518e-09\n",
      "Step: 7220, train/learning_rate: 2.136284274456557e-05\n",
      "Step: 7220, train/epoch: 1.7182294130325317\n",
      "Step: 7230, train/loss: 0.0\n",
      "Step: 7230, train/grad_norm: 2.8859808640091522e-11\n",
      "Step: 7230, train/learning_rate: 2.132317968062125e-05\n",
      "Step: 7230, train/epoch: 1.720609188079834\n",
      "Step: 7240, train/loss: 0.00019999999494757503\n",
      "Step: 7240, train/grad_norm: 2.9397688710436354e-12\n",
      "Step: 7240, train/learning_rate: 2.1283516616676934e-05\n",
      "Step: 7240, train/epoch: 1.7229890823364258\n",
      "Step: 7250, train/loss: 9.999999747378752e-05\n",
      "Step: 7250, train/grad_norm: 1.2976758512195374e-07\n",
      "Step: 7250, train/learning_rate: 2.1243851733743213e-05\n",
      "Step: 7250, train/epoch: 1.725368857383728\n",
      "Step: 7260, train/loss: 0.0\n",
      "Step: 7260, train/grad_norm: 1.3587286957772449e-06\n",
      "Step: 7260, train/learning_rate: 2.1204188669798896e-05\n",
      "Step: 7260, train/epoch: 1.7277486324310303\n",
      "Step: 7270, train/loss: 0.0\n",
      "Step: 7270, train/grad_norm: 7.142661502257397e-07\n",
      "Step: 7270, train/learning_rate: 2.116452560585458e-05\n",
      "Step: 7270, train/epoch: 1.730128526687622\n",
      "Step: 7280, train/loss: 0.1234000027179718\n",
      "Step: 7280, train/grad_norm: 75.81040954589844\n",
      "Step: 7280, train/learning_rate: 2.1124860722920857e-05\n",
      "Step: 7280, train/epoch: 1.7325083017349243\n",
      "Step: 7290, train/loss: 0.0\n",
      "Step: 7290, train/grad_norm: 0.00013764486357104033\n",
      "Step: 7290, train/learning_rate: 2.108519765897654e-05\n",
      "Step: 7290, train/epoch: 1.7348881959915161\n",
      "Step: 7300, train/loss: 0.0\n",
      "Step: 7300, train/grad_norm: 8.728537977731321e-06\n",
      "Step: 7300, train/learning_rate: 2.1045534595032223e-05\n",
      "Step: 7300, train/epoch: 1.7372679710388184\n",
      "Step: 7310, train/loss: 0.0\n",
      "Step: 7310, train/grad_norm: 3.2048927067762634e-08\n",
      "Step: 7310, train/learning_rate: 2.10058697120985e-05\n",
      "Step: 7310, train/epoch: 1.7396477460861206\n",
      "Step: 7320, train/loss: 0.0\n",
      "Step: 7320, train/grad_norm: 0.26203155517578125\n",
      "Step: 7320, train/learning_rate: 2.0966206648154184e-05\n",
      "Step: 7320, train/epoch: 1.7420276403427124\n",
      "Step: 7330, train/loss: 0.0\n",
      "Step: 7330, train/grad_norm: 8.436064490524586e-06\n",
      "Step: 7330, train/learning_rate: 2.0926543584209867e-05\n",
      "Step: 7330, train/epoch: 1.7444074153900146\n",
      "Step: 7340, train/loss: 0.0\n",
      "Step: 7340, train/grad_norm: 3.709251517047818e-10\n",
      "Step: 7340, train/learning_rate: 2.0886878701276146e-05\n",
      "Step: 7340, train/epoch: 1.746787190437317\n",
      "Step: 7350, train/loss: 0.0\n",
      "Step: 7350, train/grad_norm: 7.985415322764311e-06\n",
      "Step: 7350, train/learning_rate: 2.084721563733183e-05\n",
      "Step: 7350, train/epoch: 1.7491670846939087\n",
      "Step: 7360, train/loss: 0.05079999938607216\n",
      "Step: 7360, train/grad_norm: 1.8069847556034802e-06\n",
      "Step: 7360, train/learning_rate: 2.080755257338751e-05\n",
      "Step: 7360, train/epoch: 1.751546859741211\n",
      "Step: 7370, train/loss: 0.0\n",
      "Step: 7370, train/grad_norm: 1.1856053248535114e-12\n",
      "Step: 7370, train/learning_rate: 2.076788769045379e-05\n",
      "Step: 7370, train/epoch: 1.7539267539978027\n",
      "Step: 7380, train/loss: 0.0\n",
      "Step: 7380, train/grad_norm: 1.3161894685254083e-08\n",
      "Step: 7380, train/learning_rate: 2.0728224626509473e-05\n",
      "Step: 7380, train/epoch: 1.756306529045105\n",
      "Step: 7390, train/loss: 0.0\n",
      "Step: 7390, train/grad_norm: 4.0191884542739587e-11\n",
      "Step: 7390, train/learning_rate: 2.0688561562565155e-05\n",
      "Step: 7390, train/epoch: 1.7586863040924072\n",
      "Step: 7400, train/loss: 0.0\n",
      "Step: 7400, train/grad_norm: 1.3785035667211037e-13\n",
      "Step: 7400, train/learning_rate: 2.0648896679631434e-05\n",
      "Step: 7400, train/epoch: 1.761066198348999\n",
      "Step: 7410, train/loss: 0.0\n",
      "Step: 7410, train/grad_norm: 1.2796201598330725e-10\n",
      "Step: 7410, train/learning_rate: 2.0609233615687117e-05\n",
      "Step: 7410, train/epoch: 1.7634459733963013\n",
      "Step: 7420, train/loss: 0.0\n",
      "Step: 7420, train/grad_norm: 3.5821895449927865e-10\n",
      "Step: 7420, train/learning_rate: 2.05695705517428e-05\n",
      "Step: 7420, train/epoch: 1.7658257484436035\n",
      "Step: 7430, train/loss: 0.0\n",
      "Step: 7430, train/grad_norm: 5.513830281245191e-09\n",
      "Step: 7430, train/learning_rate: 2.052990566880908e-05\n",
      "Step: 7430, train/epoch: 1.7682056427001953\n",
      "Step: 7440, train/loss: 0.0\n",
      "Step: 7440, train/grad_norm: 3.003873538798274e-11\n",
      "Step: 7440, train/learning_rate: 2.049024260486476e-05\n",
      "Step: 7440, train/epoch: 1.7705854177474976\n",
      "Step: 7450, train/loss: 0.0\n",
      "Step: 7450, train/grad_norm: 6.230533955431383e-08\n",
      "Step: 7450, train/learning_rate: 2.0450579540920444e-05\n",
      "Step: 7450, train/epoch: 1.7729653120040894\n",
      "Step: 7460, train/loss: 0.0\n",
      "Step: 7460, train/grad_norm: 8.046487738264108e-14\n",
      "Step: 7460, train/learning_rate: 2.0410914657986723e-05\n",
      "Step: 7460, train/epoch: 1.7753450870513916\n",
      "Step: 7470, train/loss: 0.0\n",
      "Step: 7470, train/grad_norm: 5.821626953292705e-14\n",
      "Step: 7470, train/learning_rate: 2.0371251594042405e-05\n",
      "Step: 7470, train/epoch: 1.7777248620986938\n",
      "Step: 7480, train/loss: 0.0\n",
      "Step: 7480, train/grad_norm: 0.001464484608732164\n",
      "Step: 7480, train/learning_rate: 2.0331588530098088e-05\n",
      "Step: 7480, train/epoch: 1.7801047563552856\n",
      "Step: 7490, train/loss: 0.0\n",
      "Step: 7490, train/grad_norm: 5.929566733453573e-14\n",
      "Step: 7490, train/learning_rate: 2.0291923647164367e-05\n",
      "Step: 7490, train/epoch: 1.782484531402588\n",
      "Step: 7500, train/loss: 0.0\n",
      "Step: 7500, train/grad_norm: 7.710445129593957e-12\n",
      "Step: 7500, train/learning_rate: 2.025226058322005e-05\n",
      "Step: 7500, train/epoch: 1.7848643064498901\n",
      "Step: 7510, train/loss: 0.0\n",
      "Step: 7510, train/grad_norm: 4.4282039515265714e-13\n",
      "Step: 7510, train/learning_rate: 2.0212597519275732e-05\n",
      "Step: 7510, train/epoch: 1.787244200706482\n",
      "Step: 7520, train/loss: 0.0\n",
      "Step: 7520, train/grad_norm: 6.724066303753773e-10\n",
      "Step: 7520, train/learning_rate: 2.017293263634201e-05\n",
      "Step: 7520, train/epoch: 1.7896239757537842\n",
      "Step: 7530, train/loss: 0.00019999999494757503\n",
      "Step: 7530, train/grad_norm: 2.388005842178363e-16\n",
      "Step: 7530, train/learning_rate: 2.0133269572397694e-05\n",
      "Step: 7530, train/epoch: 1.7920037508010864\n",
      "Step: 7540, train/loss: 0.0\n",
      "Step: 7540, train/grad_norm: 3.174941863234859e-13\n",
      "Step: 7540, train/learning_rate: 2.0093606508453377e-05\n",
      "Step: 7540, train/epoch: 1.7943836450576782\n",
      "Step: 7550, train/loss: 0.0\n",
      "Step: 7550, train/grad_norm: 3.117128954954751e-08\n",
      "Step: 7550, train/learning_rate: 2.005394344450906e-05\n",
      "Step: 7550, train/epoch: 1.7967634201049805\n",
      "Step: 7560, train/loss: 0.0\n",
      "Step: 7560, train/grad_norm: 1.5142219922381628e-08\n",
      "Step: 7560, train/learning_rate: 2.0014278561575338e-05\n",
      "Step: 7560, train/epoch: 1.7991433143615723\n",
      "Step: 7570, train/loss: 0.0\n",
      "Step: 7570, train/grad_norm: 2.802476783622665e-12\n",
      "Step: 7570, train/learning_rate: 1.997461549763102e-05\n",
      "Step: 7570, train/epoch: 1.8015230894088745\n",
      "Step: 7580, train/loss: 0.0\n",
      "Step: 7580, train/grad_norm: 3.957651755279049e-15\n",
      "Step: 7580, train/learning_rate: 1.9934952433686703e-05\n",
      "Step: 7580, train/epoch: 1.8039028644561768\n",
      "Step: 7590, train/loss: 0.0\n",
      "Step: 7590, train/grad_norm: 3.0615963323571416e-12\n",
      "Step: 7590, train/learning_rate: 1.9895287550752982e-05\n",
      "Step: 7590, train/epoch: 1.8062827587127686\n",
      "Step: 7600, train/loss: 0.0\n",
      "Step: 7600, train/grad_norm: 6.122905730609335e-14\n",
      "Step: 7600, train/learning_rate: 1.9855624486808665e-05\n",
      "Step: 7600, train/epoch: 1.8086625337600708\n",
      "Step: 7610, train/loss: 0.0\n",
      "Step: 7610, train/grad_norm: 2.117746082663796e-11\n",
      "Step: 7610, train/learning_rate: 1.9815961422864348e-05\n",
      "Step: 7610, train/epoch: 1.811042308807373\n",
      "Step: 7620, train/loss: 0.0\n",
      "Step: 7620, train/grad_norm: 1.2066997179260852e-08\n",
      "Step: 7620, train/learning_rate: 1.9776296539930627e-05\n",
      "Step: 7620, train/epoch: 1.8134222030639648\n",
      "Step: 7630, train/loss: 0.0\n",
      "Step: 7630, train/grad_norm: 2.114102214687344e-13\n",
      "Step: 7630, train/learning_rate: 1.973663347598631e-05\n",
      "Step: 7630, train/epoch: 1.815801978111267\n",
      "Step: 7640, train/loss: 0.0\n",
      "Step: 7640, train/grad_norm: 5.4014705035132504e-12\n",
      "Step: 7640, train/learning_rate: 1.9696970412041992e-05\n",
      "Step: 7640, train/epoch: 1.8181818723678589\n",
      "Step: 7650, train/loss: 0.0\n",
      "Step: 7650, train/grad_norm: 0.008103904314339161\n",
      "Step: 7650, train/learning_rate: 1.965730552910827e-05\n",
      "Step: 7650, train/epoch: 1.8205616474151611\n",
      "Step: 7660, train/loss: 0.0\n",
      "Step: 7660, train/grad_norm: 1.0477706077977392e-14\n",
      "Step: 7660, train/learning_rate: 1.9617642465163954e-05\n",
      "Step: 7660, train/epoch: 1.8229414224624634\n",
      "Step: 7670, train/loss: 0.0\n",
      "Step: 7670, train/grad_norm: 1.0572413344789311e-07\n",
      "Step: 7670, train/learning_rate: 1.9577979401219636e-05\n",
      "Step: 7670, train/epoch: 1.8253213167190552\n",
      "Step: 7680, train/loss: 0.0\n",
      "Step: 7680, train/grad_norm: 1.0120549953640534e-09\n",
      "Step: 7680, train/learning_rate: 1.9538314518285915e-05\n",
      "Step: 7680, train/epoch: 1.8277010917663574\n",
      "Step: 7690, train/loss: 0.0\n",
      "Step: 7690, train/grad_norm: 1.0261211880191468e-09\n",
      "Step: 7690, train/learning_rate: 1.9498651454341598e-05\n",
      "Step: 7690, train/epoch: 1.8300808668136597\n",
      "Step: 7700, train/loss: 0.0\n",
      "Step: 7700, train/grad_norm: 5.5161379563601004e-17\n",
      "Step: 7700, train/learning_rate: 1.945898839039728e-05\n",
      "Step: 7700, train/epoch: 1.8324607610702515\n",
      "Step: 7710, train/loss: 0.06599999964237213\n",
      "Step: 7710, train/grad_norm: 6.321112383522731e-11\n",
      "Step: 7710, train/learning_rate: 1.941932350746356e-05\n",
      "Step: 7710, train/epoch: 1.8348405361175537\n",
      "Step: 7720, train/loss: 0.0\n",
      "Step: 7720, train/grad_norm: 3.7016087972574496e-09\n",
      "Step: 7720, train/learning_rate: 1.9379660443519242e-05\n",
      "Step: 7720, train/epoch: 1.8372204303741455\n",
      "Step: 7730, train/loss: 0.0\n",
      "Step: 7730, train/grad_norm: 3.648215596285809e-08\n",
      "Step: 7730, train/learning_rate: 1.9339997379574925e-05\n",
      "Step: 7730, train/epoch: 1.8396002054214478\n",
      "Step: 7740, train/loss: 0.0\n",
      "Step: 7740, train/grad_norm: 8.184064537175573e-10\n",
      "Step: 7740, train/learning_rate: 1.9300332496641204e-05\n",
      "Step: 7740, train/epoch: 1.84197998046875\n",
      "Step: 7750, train/loss: 0.0\n",
      "Step: 7750, train/grad_norm: 8.873354828686786e-11\n",
      "Step: 7750, train/learning_rate: 1.9260669432696886e-05\n",
      "Step: 7750, train/epoch: 1.8443598747253418\n",
      "Step: 7760, train/loss: 0.0003000000142492354\n",
      "Step: 7760, train/grad_norm: 9.69681650531129e-07\n",
      "Step: 7760, train/learning_rate: 1.922100636875257e-05\n",
      "Step: 7760, train/epoch: 1.846739649772644\n",
      "Step: 7770, train/loss: 0.0\n",
      "Step: 7770, train/grad_norm: 1.326857727690367e-06\n",
      "Step: 7770, train/learning_rate: 1.9181341485818848e-05\n",
      "Step: 7770, train/epoch: 1.8491194248199463\n",
      "Step: 7780, train/loss: 0.0\n",
      "Step: 7780, train/grad_norm: 6.622665438271724e-08\n",
      "Step: 7780, train/learning_rate: 1.914167842187453e-05\n",
      "Step: 7780, train/epoch: 1.851499319076538\n",
      "Step: 7790, train/loss: 0.0\n",
      "Step: 7790, train/grad_norm: 8.471334922433016e-07\n",
      "Step: 7790, train/learning_rate: 1.9102015357930213e-05\n",
      "Step: 7790, train/epoch: 1.8538790941238403\n",
      "Step: 7800, train/loss: 0.0015999999595806003\n",
      "Step: 7800, train/grad_norm: 0.0006181186181493104\n",
      "Step: 7800, train/learning_rate: 1.9062350474996492e-05\n",
      "Step: 7800, train/epoch: 1.8562588691711426\n",
      "Step: 7810, train/loss: 0.0\n",
      "Step: 7810, train/grad_norm: 3.862714220304042e-05\n",
      "Step: 7810, train/learning_rate: 1.9022687411052175e-05\n",
      "Step: 7810, train/epoch: 1.8586387634277344\n",
      "Step: 7820, train/loss: 0.0003000000142492354\n",
      "Step: 7820, train/grad_norm: 3.309239400550723e-05\n",
      "Step: 7820, train/learning_rate: 1.8983024347107857e-05\n",
      "Step: 7820, train/epoch: 1.8610185384750366\n",
      "Step: 7830, train/loss: 0.0\n",
      "Step: 7830, train/grad_norm: 4.507279811605258e-07\n",
      "Step: 7830, train/learning_rate: 1.8943359464174137e-05\n",
      "Step: 7830, train/epoch: 1.8633984327316284\n",
      "Step: 7840, train/loss: 0.0003000000142492354\n",
      "Step: 7840, train/grad_norm: 3.6648671652983467e-07\n",
      "Step: 7840, train/learning_rate: 1.890369640022982e-05\n",
      "Step: 7840, train/epoch: 1.8657782077789307\n",
      "Step: 7850, train/loss: 0.0\n",
      "Step: 7850, train/grad_norm: 1.6199265928662498e-07\n",
      "Step: 7850, train/learning_rate: 1.8864033336285502e-05\n",
      "Step: 7850, train/epoch: 1.868157982826233\n",
      "Step: 7860, train/loss: 0.0\n",
      "Step: 7860, train/grad_norm: 1.8039221671983796e-08\n",
      "Step: 7860, train/learning_rate: 1.882436845335178e-05\n",
      "Step: 7860, train/epoch: 1.8705378770828247\n",
      "Step: 7870, train/loss: 0.0\n",
      "Step: 7870, train/grad_norm: 1.0212490906269522e-06\n",
      "Step: 7870, train/learning_rate: 1.8784705389407463e-05\n",
      "Step: 7870, train/epoch: 1.872917652130127\n",
      "Step: 7880, train/loss: 0.0\n",
      "Step: 7880, train/grad_norm: 3.3159440704366716e-07\n",
      "Step: 7880, train/learning_rate: 1.8745042325463146e-05\n",
      "Step: 7880, train/epoch: 1.8752974271774292\n",
      "Step: 7890, train/loss: 0.0\n",
      "Step: 7890, train/grad_norm: 3.2861149357188424e-09\n",
      "Step: 7890, train/learning_rate: 1.870537926151883e-05\n",
      "Step: 7890, train/epoch: 1.877677321434021\n",
      "Step: 7900, train/loss: 0.0\n",
      "Step: 7900, train/grad_norm: 2.420183773210738e-05\n",
      "Step: 7900, train/learning_rate: 1.8665714378585108e-05\n",
      "Step: 7900, train/epoch: 1.8800570964813232\n",
      "Step: 7910, train/loss: 0.0\n",
      "Step: 7910, train/grad_norm: 3.834878192776614e-09\n",
      "Step: 7910, train/learning_rate: 1.862605131464079e-05\n",
      "Step: 7910, train/epoch: 1.882436990737915\n",
      "Step: 7920, train/loss: 0.0\n",
      "Step: 7920, train/grad_norm: 9.472969283996768e-12\n",
      "Step: 7920, train/learning_rate: 1.8586388250696473e-05\n",
      "Step: 7920, train/epoch: 1.8848167657852173\n",
      "Step: 7930, train/loss: 0.00839999970048666\n",
      "Step: 7930, train/grad_norm: 5.657043167239806e-10\n",
      "Step: 7930, train/learning_rate: 1.8546723367762752e-05\n",
      "Step: 7930, train/epoch: 1.8871965408325195\n",
      "Step: 7940, train/loss: 0.0\n",
      "Step: 7940, train/grad_norm: 9.103284355660435e-06\n",
      "Step: 7940, train/learning_rate: 1.8507060303818434e-05\n",
      "Step: 7940, train/epoch: 1.8895764350891113\n",
      "Step: 7950, train/loss: 0.0\n",
      "Step: 7950, train/grad_norm: 4.24576207525007e-10\n",
      "Step: 7950, train/learning_rate: 1.8467397239874117e-05\n",
      "Step: 7950, train/epoch: 1.8919562101364136\n",
      "Step: 7960, train/loss: 0.0\n",
      "Step: 7960, train/grad_norm: 1.0706136498173091e-12\n",
      "Step: 7960, train/learning_rate: 1.8427732356940396e-05\n",
      "Step: 7960, train/epoch: 1.8943359851837158\n",
      "Step: 7970, train/loss: 0.0\n",
      "Step: 7970, train/grad_norm: 8.363846941417119e-12\n",
      "Step: 7970, train/learning_rate: 1.838806929299608e-05\n",
      "Step: 7970, train/epoch: 1.8967158794403076\n",
      "Step: 7980, train/loss: 0.0\n",
      "Step: 7980, train/grad_norm: 1.5638685901819827e-09\n",
      "Step: 7980, train/learning_rate: 1.834840622905176e-05\n",
      "Step: 7980, train/epoch: 1.8990956544876099\n",
      "Step: 7990, train/loss: 0.0\n",
      "Step: 7990, train/grad_norm: 4.900010085184814e-12\n",
      "Step: 7990, train/learning_rate: 1.830874134611804e-05\n",
      "Step: 7990, train/epoch: 1.901475429534912\n",
      "Step: 8000, train/loss: 0.0\n",
      "Step: 8000, train/grad_norm: 6.045022745175196e-14\n",
      "Step: 8000, train/learning_rate: 1.8269078282173723e-05\n",
      "Step: 8000, train/epoch: 1.903855323791504\n",
      "Step: 8010, train/loss: 0.0\n",
      "Step: 8010, train/grad_norm: 3.049605584878011e-14\n",
      "Step: 8010, train/learning_rate: 1.8229415218229406e-05\n",
      "Step: 8010, train/epoch: 1.9062350988388062\n",
      "Step: 8020, train/loss: 0.0\n",
      "Step: 8020, train/grad_norm: 1.1283072566536934e-11\n",
      "Step: 8020, train/learning_rate: 1.8189750335295685e-05\n",
      "Step: 8020, train/epoch: 1.908614993095398\n",
      "Step: 8030, train/loss: 0.0\n",
      "Step: 8030, train/grad_norm: 4.49868392538022e-10\n",
      "Step: 8030, train/learning_rate: 1.8150087271351367e-05\n",
      "Step: 8030, train/epoch: 1.9109947681427002\n",
      "Step: 8040, train/loss: 0.0\n",
      "Step: 8040, train/grad_norm: 9.316692012362182e-05\n",
      "Step: 8040, train/learning_rate: 1.811042420740705e-05\n",
      "Step: 8040, train/epoch: 1.9133745431900024\n",
      "Step: 8050, train/loss: 0.0\n",
      "Step: 8050, train/grad_norm: 4.1427920649539374e-08\n",
      "Step: 8050, train/learning_rate: 1.807075932447333e-05\n",
      "Step: 8050, train/epoch: 1.9157544374465942\n",
      "Step: 8060, train/loss: 0.0\n",
      "Step: 8060, train/grad_norm: 2.0826879717356084e-12\n",
      "Step: 8060, train/learning_rate: 1.803109626052901e-05\n",
      "Step: 8060, train/epoch: 1.9181342124938965\n",
      "Step: 8070, train/loss: 0.0\n",
      "Step: 8070, train/grad_norm: 2.9786329491798824e-07\n",
      "Step: 8070, train/learning_rate: 1.7991433196584694e-05\n",
      "Step: 8070, train/epoch: 1.9205139875411987\n",
      "Step: 8080, train/loss: 0.0\n",
      "Step: 8080, train/grad_norm: 4.4858554371085546e-11\n",
      "Step: 8080, train/learning_rate: 1.7951768313650973e-05\n",
      "Step: 8080, train/epoch: 1.9228938817977905\n",
      "Step: 8090, train/loss: 0.0\n",
      "Step: 8090, train/grad_norm: 1.5170488154581108e-07\n",
      "Step: 8090, train/learning_rate: 1.7912105249706656e-05\n",
      "Step: 8090, train/epoch: 1.9252736568450928\n",
      "Step: 8100, train/loss: 0.0\n",
      "Step: 8100, train/grad_norm: 2.704554358246014e-10\n",
      "Step: 8100, train/learning_rate: 1.787244218576234e-05\n",
      "Step: 8100, train/epoch: 1.9276535511016846\n",
      "Step: 8110, train/loss: 0.0\n",
      "Step: 8110, train/grad_norm: 8.400754050730264e-12\n",
      "Step: 8110, train/learning_rate: 1.7832777302828617e-05\n",
      "Step: 8110, train/epoch: 1.9300333261489868\n",
      "Step: 8120, train/loss: 0.0\n",
      "Step: 8120, train/grad_norm: 1.0378261805499278e-07\n",
      "Step: 8120, train/learning_rate: 1.77931142388843e-05\n",
      "Step: 8120, train/epoch: 1.932413101196289\n",
      "Step: 8130, train/loss: 0.0\n",
      "Step: 8130, train/grad_norm: 4.384079904662386e-14\n",
      "Step: 8130, train/learning_rate: 1.7753451174939983e-05\n",
      "Step: 8130, train/epoch: 1.9347929954528809\n",
      "Step: 8140, train/loss: 0.0\n",
      "Step: 8140, train/grad_norm: 2.8802056226129302e-12\n",
      "Step: 8140, train/learning_rate: 1.771378629200626e-05\n",
      "Step: 8140, train/epoch: 1.937172770500183\n",
      "Step: 8150, train/loss: 0.0\n",
      "Step: 8150, train/grad_norm: 3.6544094417223505e-09\n",
      "Step: 8150, train/learning_rate: 1.7674123228061944e-05\n",
      "Step: 8150, train/epoch: 1.9395525455474854\n",
      "Step: 8160, train/loss: 0.0\n",
      "Step: 8160, train/grad_norm: 2.8862186809419654e-06\n",
      "Step: 8160, train/learning_rate: 1.7634460164117627e-05\n",
      "Step: 8160, train/epoch: 1.9419324398040771\n",
      "Step: 8170, train/loss: 0.0\n",
      "Step: 8170, train/grad_norm: 1.3113881508486358e-11\n",
      "Step: 8170, train/learning_rate: 1.7594795281183906e-05\n",
      "Step: 8170, train/epoch: 1.9443122148513794\n",
      "Step: 8180, train/loss: 0.0\n",
      "Step: 8180, train/grad_norm: 5.107398393100482e-10\n",
      "Step: 8180, train/learning_rate: 1.755513221723959e-05\n",
      "Step: 8180, train/epoch: 1.9466921091079712\n",
      "Step: 8190, train/loss: 0.0\n",
      "Step: 8190, train/grad_norm: 2.0705286714762378e-13\n",
      "Step: 8190, train/learning_rate: 1.751546915329527e-05\n",
      "Step: 8190, train/epoch: 1.9490718841552734\n",
      "Step: 8200, train/loss: 0.0\n",
      "Step: 8200, train/grad_norm: 1.494656655387505e-11\n",
      "Step: 8200, train/learning_rate: 1.747580427036155e-05\n",
      "Step: 8200, train/epoch: 1.9514516592025757\n",
      "Step: 8210, train/loss: 0.0\n",
      "Step: 8210, train/grad_norm: 7.070994831326338e-12\n",
      "Step: 8210, train/learning_rate: 1.7436141206417233e-05\n",
      "Step: 8210, train/epoch: 1.9538315534591675\n",
      "Step: 8220, train/loss: 0.0\n",
      "Step: 8220, train/grad_norm: 2.488473960227111e-08\n",
      "Step: 8220, train/learning_rate: 1.7396478142472915e-05\n",
      "Step: 8220, train/epoch: 1.9562113285064697\n",
      "Step: 8230, train/loss: 0.0\n",
      "Step: 8230, train/grad_norm: 1.2310633198606147e-12\n",
      "Step: 8230, train/learning_rate: 1.7356815078528598e-05\n",
      "Step: 8230, train/epoch: 1.958591103553772\n",
      "Step: 8240, train/loss: 0.0\n",
      "Step: 8240, train/grad_norm: 2.1676991798780243e-11\n",
      "Step: 8240, train/learning_rate: 1.7317150195594877e-05\n",
      "Step: 8240, train/epoch: 1.9609709978103638\n",
      "Step: 8250, train/loss: 0.0\n",
      "Step: 8250, train/grad_norm: 5.028155322661654e-11\n",
      "Step: 8250, train/learning_rate: 1.727748713165056e-05\n",
      "Step: 8250, train/epoch: 1.963350772857666\n",
      "Step: 8260, train/loss: 0.0\n",
      "Step: 8260, train/grad_norm: 2.4112893973193827e-11\n",
      "Step: 8260, train/learning_rate: 1.7237824067706242e-05\n",
      "Step: 8260, train/epoch: 1.9657305479049683\n",
      "Step: 8270, train/loss: 0.0\n",
      "Step: 8270, train/grad_norm: 4.7387893609140974e-09\n",
      "Step: 8270, train/learning_rate: 1.719815918477252e-05\n",
      "Step: 8270, train/epoch: 1.96811044216156\n",
      "Step: 8280, train/loss: 0.0\n",
      "Step: 8280, train/grad_norm: 0.014757520519196987\n",
      "Step: 8280, train/learning_rate: 1.7158496120828204e-05\n",
      "Step: 8280, train/epoch: 1.9704902172088623\n",
      "Step: 8290, train/loss: 0.0\n",
      "Step: 8290, train/grad_norm: 2.426900458374348e-09\n",
      "Step: 8290, train/learning_rate: 1.7118833056883886e-05\n",
      "Step: 8290, train/epoch: 1.972870111465454\n",
      "Step: 8300, train/loss: 0.0\n",
      "Step: 8300, train/grad_norm: 1.1894723758471698e-13\n",
      "Step: 8300, train/learning_rate: 1.7079168173950166e-05\n",
      "Step: 8300, train/epoch: 1.9752498865127563\n",
      "Step: 8310, train/loss: 0.0\n",
      "Step: 8310, train/grad_norm: 6.219658577229226e-14\n",
      "Step: 8310, train/learning_rate: 1.7039505110005848e-05\n",
      "Step: 8310, train/epoch: 1.9776296615600586\n",
      "Step: 8320, train/loss: 0.0\n",
      "Step: 8320, train/grad_norm: 4.104306824959103e-11\n",
      "Step: 8320, train/learning_rate: 1.699984204606153e-05\n",
      "Step: 8320, train/epoch: 1.9800095558166504\n",
      "Step: 8330, train/loss: 0.3093999922275543\n",
      "Step: 8330, train/grad_norm: 75.62713623046875\n",
      "Step: 8330, train/learning_rate: 1.696017716312781e-05\n",
      "Step: 8330, train/epoch: 1.9823893308639526\n",
      "Step: 8340, train/loss: 0.0\n",
      "Step: 8340, train/grad_norm: 0.002841630019247532\n",
      "Step: 8340, train/learning_rate: 1.6920514099183492e-05\n",
      "Step: 8340, train/epoch: 1.9847691059112549\n",
      "Step: 8350, train/loss: 0.0\n",
      "Step: 8350, train/grad_norm: 3.245941115892492e-08\n",
      "Step: 8350, train/learning_rate: 1.6880851035239175e-05\n",
      "Step: 8350, train/epoch: 1.9871490001678467\n",
      "Step: 8360, train/loss: 0.0\n",
      "Step: 8360, train/grad_norm: 0.007417414803057909\n",
      "Step: 8360, train/learning_rate: 1.6841186152305454e-05\n",
      "Step: 8360, train/epoch: 1.989528775215149\n",
      "Step: 8370, train/loss: 0.0\n",
      "Step: 8370, train/grad_norm: 2.292400644310355e-11\n",
      "Step: 8370, train/learning_rate: 1.6801523088361137e-05\n",
      "Step: 8370, train/epoch: 1.9919086694717407\n",
      "Step: 8380, train/loss: 0.0\n",
      "Step: 8380, train/grad_norm: 1.6879537270142464e-06\n",
      "Step: 8380, train/learning_rate: 1.676186002441682e-05\n",
      "Step: 8380, train/epoch: 1.994288444519043\n",
      "Step: 8390, train/loss: 0.00039999998989515007\n",
      "Step: 8390, train/grad_norm: 4.332424197173168e-08\n",
      "Step: 8390, train/learning_rate: 1.6722195141483098e-05\n",
      "Step: 8390, train/epoch: 1.9966682195663452\n",
      "Step: 8400, train/loss: 0.0\n",
      "Step: 8400, train/grad_norm: 2.5334395559184486e-07\n",
      "Step: 8400, train/learning_rate: 1.668253207753878e-05\n",
      "Step: 8400, train/epoch: 1.999048113822937\n",
      "Step: 8404, eval/loss: 0.013602747581899166\n",
      "Step: 8404, eval/accuracy: 0.9993058443069458\n",
      "Step: 8404, eval/f1: 0.9992668032646179\n",
      "Step: 8404, eval/runtime: 6244.89306640625\n",
      "Step: 8404, eval/samples_per_second: 1.152999997138977\n",
      "Step: 8404, eval/steps_per_second: 0.14399999380111694\n",
      "Step: 8404, train/epoch: 2.0\n",
      "Step: 8410, train/loss: 0.0\n",
      "Step: 8410, train/grad_norm: 4.541608788263124e-13\n",
      "Step: 8410, train/learning_rate: 1.6642869013594463e-05\n",
      "Step: 8410, train/epoch: 2.0014278888702393\n",
      "Step: 8420, train/loss: 0.0\n",
      "Step: 8420, train/grad_norm: 1.3432252712723431e-14\n",
      "Step: 8420, train/learning_rate: 1.6603204130660743e-05\n",
      "Step: 8420, train/epoch: 2.003807783126831\n",
      "Step: 8430, train/loss: 0.0\n",
      "Step: 8430, train/grad_norm: 1.6848313022510553e-10\n",
      "Step: 8430, train/learning_rate: 1.6563541066716425e-05\n",
      "Step: 8430, train/epoch: 2.0061874389648438\n",
      "Step: 8440, train/loss: 0.0\n",
      "Step: 8440, train/grad_norm: 0.00013039271289017051\n",
      "Step: 8440, train/learning_rate: 1.6523878002772108e-05\n",
      "Step: 8440, train/epoch: 2.0085673332214355\n",
      "Step: 8450, train/loss: 0.0\n",
      "Step: 8450, train/grad_norm: 3.212723953538443e-08\n",
      "Step: 8450, train/learning_rate: 1.6484213119838387e-05\n",
      "Step: 8450, train/epoch: 2.0109472274780273\n",
      "Step: 8460, train/loss: 0.0\n",
      "Step: 8460, train/grad_norm: 1.5631914651592638e-09\n",
      "Step: 8460, train/learning_rate: 1.644455005589407e-05\n",
      "Step: 8460, train/epoch: 2.01332688331604\n",
      "Step: 8470, train/loss: 0.0\n",
      "Step: 8470, train/grad_norm: 2.497455909612756e-13\n",
      "Step: 8470, train/learning_rate: 1.6404886991949752e-05\n",
      "Step: 8470, train/epoch: 2.015706777572632\n",
      "Step: 8480, train/loss: 0.0\n",
      "Step: 8480, train/grad_norm: 4.035484511659415e-13\n",
      "Step: 8480, train/learning_rate: 1.636522210901603e-05\n",
      "Step: 8480, train/epoch: 2.0180866718292236\n",
      "Step: 8490, train/loss: 0.0\n",
      "Step: 8490, train/grad_norm: 1.8105161814219173e-08\n",
      "Step: 8490, train/learning_rate: 1.6325559045071714e-05\n",
      "Step: 8490, train/epoch: 2.0204663276672363\n",
      "Step: 8500, train/loss: 0.0\n",
      "Step: 8500, train/grad_norm: 9.62647322966248e-12\n",
      "Step: 8500, train/learning_rate: 1.6285895981127396e-05\n",
      "Step: 8500, train/epoch: 2.022846221923828\n",
      "Step: 8510, train/loss: 0.0\n",
      "Step: 8510, train/grad_norm: 4.767028372043569e-07\n",
      "Step: 8510, train/learning_rate: 1.6246231098193675e-05\n",
      "Step: 8510, train/epoch: 2.02522611618042\n",
      "Step: 8520, train/loss: 0.0\n",
      "Step: 8520, train/grad_norm: 1.095764812220068e-09\n",
      "Step: 8520, train/learning_rate: 1.6206568034249358e-05\n",
      "Step: 8520, train/epoch: 2.0276060104370117\n",
      "Step: 8530, train/loss: 0.0\n",
      "Step: 8530, train/grad_norm: 4.332906922144275e-09\n",
      "Step: 8530, train/learning_rate: 1.616690497030504e-05\n",
      "Step: 8530, train/epoch: 2.0299856662750244\n",
      "Step: 8540, train/loss: 0.0\n",
      "Step: 8540, train/grad_norm: 1.9561976216664334e-07\n",
      "Step: 8540, train/learning_rate: 1.612724008737132e-05\n",
      "Step: 8540, train/epoch: 2.032365560531616\n",
      "Step: 8550, train/loss: 0.0\n",
      "Step: 8550, train/grad_norm: 7.646144459770576e-13\n",
      "Step: 8550, train/learning_rate: 1.6087577023427002e-05\n",
      "Step: 8550, train/epoch: 2.034745454788208\n",
      "Step: 8560, train/loss: 0.0\n",
      "Step: 8560, train/grad_norm: 8.525761074906768e-08\n",
      "Step: 8560, train/learning_rate: 1.6047913959482685e-05\n",
      "Step: 8560, train/epoch: 2.0371251106262207\n",
      "Step: 8570, train/loss: 0.0\n",
      "Step: 8570, train/grad_norm: 1.4192020822423324e-11\n",
      "Step: 8570, train/learning_rate: 1.6008250895538367e-05\n",
      "Step: 8570, train/epoch: 2.0395050048828125\n",
      "Step: 8580, train/loss: 0.0\n",
      "Step: 8580, train/grad_norm: 3.541236043552026e-14\n",
      "Step: 8580, train/learning_rate: 1.5968586012604646e-05\n",
      "Step: 8580, train/epoch: 2.0418848991394043\n",
      "Step: 8590, train/loss: 0.0\n",
      "Step: 8590, train/grad_norm: 1.0216819057440318e-10\n",
      "Step: 8590, train/learning_rate: 1.592892294866033e-05\n",
      "Step: 8590, train/epoch: 2.044264554977417\n",
      "Step: 8600, train/loss: 0.0\n",
      "Step: 8600, train/grad_norm: 2.6233057393021397e-11\n",
      "Step: 8600, train/learning_rate: 1.588925988471601e-05\n",
      "Step: 8600, train/epoch: 2.046644449234009\n",
      "Step: 8610, train/loss: 0.0\n",
      "Step: 8610, train/grad_norm: 4.917385400780849e-14\n",
      "Step: 8610, train/learning_rate: 1.584959500178229e-05\n",
      "Step: 8610, train/epoch: 2.0490243434906006\n",
      "Step: 8620, train/loss: 0.0\n",
      "Step: 8620, train/grad_norm: 1.2601324428374028e-09\n",
      "Step: 8620, train/learning_rate: 1.5809931937837973e-05\n",
      "Step: 8620, train/epoch: 2.0514039993286133\n",
      "Step: 8630, train/loss: 0.0\n",
      "Step: 8630, train/grad_norm: 3.866592462875573e-11\n",
      "Step: 8630, train/learning_rate: 1.5770268873893656e-05\n",
      "Step: 8630, train/epoch: 2.053783893585205\n",
      "Step: 8640, train/loss: 0.0729999989271164\n",
      "Step: 8640, train/grad_norm: 7.359245157712024e-11\n",
      "Step: 8640, train/learning_rate: 1.5730603990959935e-05\n",
      "Step: 8640, train/epoch: 2.056163787841797\n",
      "Step: 8650, train/loss: 0.0\n",
      "Step: 8650, train/grad_norm: 3.337744303166801e-09\n",
      "Step: 8650, train/learning_rate: 1.5690940927015617e-05\n",
      "Step: 8650, train/epoch: 2.0585434436798096\n",
      "Step: 8660, train/loss: 0.0\n",
      "Step: 8660, train/grad_norm: 1.0247151333031296e-14\n",
      "Step: 8660, train/learning_rate: 1.56512778630713e-05\n",
      "Step: 8660, train/epoch: 2.0609233379364014\n",
      "Step: 8670, train/loss: 0.0\n",
      "Step: 8670, train/grad_norm: 3.132550100391762e-13\n",
      "Step: 8670, train/learning_rate: 1.561161298013758e-05\n",
      "Step: 8670, train/epoch: 2.063303232192993\n",
      "Step: 8680, train/loss: 0.0\n",
      "Step: 8680, train/grad_norm: 2.908940449630748e-12\n",
      "Step: 8680, train/learning_rate: 1.5571949916193262e-05\n",
      "Step: 8680, train/epoch: 2.065683126449585\n",
      "Step: 8690, train/loss: 0.0\n",
      "Step: 8690, train/grad_norm: 4.324135721798378e-14\n",
      "Step: 8690, train/learning_rate: 1.5532286852248944e-05\n",
      "Step: 8690, train/epoch: 2.0680627822875977\n",
      "Step: 8700, train/loss: 0.0\n",
      "Step: 8700, train/grad_norm: 1.305423759541402e-12\n",
      "Step: 8700, train/learning_rate: 1.5492621969315223e-05\n",
      "Step: 8700, train/epoch: 2.0704426765441895\n",
      "Step: 8710, train/loss: 0.0\n",
      "Step: 8710, train/grad_norm: 1.4392823738385019e-11\n",
      "Step: 8710, train/learning_rate: 1.5452958905370906e-05\n",
      "Step: 8710, train/epoch: 2.0728225708007812\n",
      "Step: 8720, train/loss: 0.0\n",
      "Step: 8720, train/grad_norm: 3.264695111471205e-11\n",
      "Step: 8720, train/learning_rate: 1.541329584142659e-05\n",
      "Step: 8720, train/epoch: 2.075202226638794\n",
      "Step: 8730, train/loss: 0.0\n",
      "Step: 8730, train/grad_norm: 9.401678654347156e-13\n",
      "Step: 8730, train/learning_rate: 1.5373630958492868e-05\n",
      "Step: 8730, train/epoch: 2.0775821208953857\n",
      "Step: 8740, train/loss: 0.0\n",
      "Step: 8740, train/grad_norm: 1.900266510986015e-13\n",
      "Step: 8740, train/learning_rate: 1.533396789454855e-05\n",
      "Step: 8740, train/epoch: 2.0799620151519775\n",
      "Step: 8750, train/loss: 0.0\n",
      "Step: 8750, train/grad_norm: 2.892053350272983e-12\n",
      "Step: 8750, train/learning_rate: 1.5294304830604233e-05\n",
      "Step: 8750, train/epoch: 2.0823416709899902\n",
      "Step: 8760, train/loss: 0.0\n",
      "Step: 8760, train/grad_norm: 2.9655784772320004e-11\n",
      "Step: 8760, train/learning_rate: 1.5254640857165214e-05\n",
      "Step: 8760, train/epoch: 2.084721565246582\n",
      "Step: 8770, train/loss: 0.0\n",
      "Step: 8770, train/grad_norm: 4.4543596589008416e-10\n",
      "Step: 8770, train/learning_rate: 1.5214976883726195e-05\n",
      "Step: 8770, train/epoch: 2.087101459503174\n",
      "Step: 8780, train/loss: 0.0013000000035390258\n",
      "Step: 8780, train/grad_norm: 1.770932359090871e-11\n",
      "Step: 8780, train/learning_rate: 1.5175312910287175e-05\n",
      "Step: 8780, train/epoch: 2.0894811153411865\n",
      "Step: 8790, train/loss: 0.0\n",
      "Step: 8790, train/grad_norm: 3.3311691541865947e-17\n",
      "Step: 8790, train/learning_rate: 1.5135649846342858e-05\n",
      "Step: 8790, train/epoch: 2.0918610095977783\n",
      "Step: 8800, train/loss: 0.0\n",
      "Step: 8800, train/grad_norm: 2.665654824898409e-15\n",
      "Step: 8800, train/learning_rate: 1.5095985872903839e-05\n",
      "Step: 8800, train/epoch: 2.09424090385437\n",
      "Step: 8810, train/loss: 0.0\n",
      "Step: 8810, train/grad_norm: 2.6316158141526103e-09\n",
      "Step: 8810, train/learning_rate: 1.5056322808959521e-05\n",
      "Step: 8810, train/epoch: 2.096620559692383\n",
      "Step: 8820, train/loss: 0.0\n",
      "Step: 8820, train/grad_norm: 5.4176949385031925e-12\n",
      "Step: 8820, train/learning_rate: 1.5016658835520502e-05\n",
      "Step: 8820, train/epoch: 2.0990004539489746\n",
      "Step: 8830, train/loss: 0.0\n",
      "Step: 8830, train/grad_norm: 7.664066863782848e-13\n",
      "Step: 8830, train/learning_rate: 1.4976994862081483e-05\n",
      "Step: 8830, train/epoch: 2.1013803482055664\n",
      "Step: 8840, train/loss: 0.0\n",
      "Step: 8840, train/grad_norm: 3.7889792215209384e-13\n",
      "Step: 8840, train/learning_rate: 1.4937331798137166e-05\n",
      "Step: 8840, train/epoch: 2.103760004043579\n",
      "Step: 8850, train/loss: 0.0\n",
      "Step: 8850, train/grad_norm: 4.202576871573105e-13\n",
      "Step: 8850, train/learning_rate: 1.4897667824698146e-05\n",
      "Step: 8850, train/epoch: 2.106139898300171\n",
      "Step: 8860, train/loss: 0.0\n",
      "Step: 8860, train/grad_norm: 5.445353508548578e-06\n",
      "Step: 8860, train/learning_rate: 1.4858003851259127e-05\n",
      "Step: 8860, train/epoch: 2.1085197925567627\n",
      "Step: 8870, train/loss: 0.0\n",
      "Step: 8870, train/grad_norm: 4.544940812589715e-10\n",
      "Step: 8870, train/learning_rate: 1.481834078731481e-05\n",
      "Step: 8870, train/epoch: 2.1108996868133545\n",
      "Step: 8880, train/loss: 0.0\n",
      "Step: 8880, train/grad_norm: 3.7833940425002766e-09\n",
      "Step: 8880, train/learning_rate: 1.477867681387579e-05\n",
      "Step: 8880, train/epoch: 2.113279342651367\n",
      "Step: 8890, train/loss: 0.0\n",
      "Step: 8890, train/grad_norm: 8.15604788567681e-12\n",
      "Step: 8890, train/learning_rate: 1.4739012840436772e-05\n",
      "Step: 8890, train/epoch: 2.115659236907959\n",
      "Step: 8900, train/loss: 0.0\n",
      "Step: 8900, train/grad_norm: 6.638552440563217e-05\n",
      "Step: 8900, train/learning_rate: 1.4699349776492454e-05\n",
      "Step: 8900, train/epoch: 2.118039131164551\n",
      "Step: 8910, train/loss: 0.0\n",
      "Step: 8910, train/grad_norm: 9.562587700850944e-12\n",
      "Step: 8910, train/learning_rate: 1.4659685803053435e-05\n",
      "Step: 8910, train/epoch: 2.1204187870025635\n",
      "Step: 8920, train/loss: 0.0\n",
      "Step: 8920, train/grad_norm: 1.3850792020752515e-14\n",
      "Step: 8920, train/learning_rate: 1.4620021829614416e-05\n",
      "Step: 8920, train/epoch: 2.1227986812591553\n",
      "Step: 8930, train/loss: 0.0\n",
      "Step: 8930, train/grad_norm: 4.1088218439601365e-13\n",
      "Step: 8930, train/learning_rate: 1.4580358765670098e-05\n",
      "Step: 8930, train/epoch: 2.125178575515747\n",
      "Step: 8940, train/loss: 0.0\n",
      "Step: 8940, train/grad_norm: 6.003733751466789e-13\n",
      "Step: 8940, train/learning_rate: 1.454069479223108e-05\n",
      "Step: 8940, train/epoch: 2.1275582313537598\n",
      "Step: 8950, train/loss: 0.0\n",
      "Step: 8950, train/grad_norm: 2.1941497834632173e-05\n",
      "Step: 8950, train/learning_rate: 1.450103081879206e-05\n",
      "Step: 8950, train/epoch: 2.1299381256103516\n",
      "Step: 8960, train/loss: 0.0\n",
      "Step: 8960, train/grad_norm: 2.5738207121150936e-09\n",
      "Step: 8960, train/learning_rate: 1.4461367754847743e-05\n",
      "Step: 8960, train/epoch: 2.1323180198669434\n",
      "Step: 8970, train/loss: 0.0\n",
      "Step: 8970, train/grad_norm: 7.769636713519934e-15\n",
      "Step: 8970, train/learning_rate: 1.4421703781408723e-05\n",
      "Step: 8970, train/epoch: 2.134697675704956\n",
      "Step: 8980, train/loss: 0.0\n",
      "Step: 8980, train/grad_norm: 1.5111332185568926e-09\n",
      "Step: 8980, train/learning_rate: 1.4382040717464406e-05\n",
      "Step: 8980, train/epoch: 2.137077569961548\n",
      "Step: 8990, train/loss: 0.0\n",
      "Step: 8990, train/grad_norm: 9.557750424438183e-12\n",
      "Step: 8990, train/learning_rate: 1.4342376744025387e-05\n",
      "Step: 8990, train/epoch: 2.1394574642181396\n",
      "Step: 9000, train/loss: 0.0\n",
      "Step: 9000, train/grad_norm: 7.952983974504613e-13\n",
      "Step: 9000, train/learning_rate: 1.4302712770586368e-05\n",
      "Step: 9000, train/epoch: 2.1418371200561523\n",
      "Step: 9010, train/loss: 0.08049999922513962\n",
      "Step: 9010, train/grad_norm: 1.5592180798991266e-14\n",
      "Step: 9010, train/learning_rate: 1.426304970664205e-05\n",
      "Step: 9010, train/epoch: 2.144217014312744\n",
      "Step: 9020, train/loss: 0.0\n",
      "Step: 9020, train/grad_norm: 1.0104774835476742e-15\n",
      "Step: 9020, train/learning_rate: 1.4223385733203031e-05\n",
      "Step: 9020, train/epoch: 2.146596908569336\n",
      "Step: 9030, train/loss: 0.0\n",
      "Step: 9030, train/grad_norm: 7.501063572362687e-14\n",
      "Step: 9030, train/learning_rate: 1.4183721759764012e-05\n",
      "Step: 9030, train/epoch: 2.1489765644073486\n",
      "Step: 9040, train/loss: 0.0\n",
      "Step: 9040, train/grad_norm: 2.1052639341025137e-13\n",
      "Step: 9040, train/learning_rate: 1.4144058695819695e-05\n",
      "Step: 9040, train/epoch: 2.1513564586639404\n",
      "Step: 9050, train/loss: 0.0\n",
      "Step: 9050, train/grad_norm: 4.914850471049448e-11\n",
      "Step: 9050, train/learning_rate: 1.4104394722380675e-05\n",
      "Step: 9050, train/epoch: 2.1537363529205322\n",
      "Step: 9060, train/loss: 0.0\n",
      "Step: 9060, train/grad_norm: 3.656600304664803e-13\n",
      "Step: 9060, train/learning_rate: 1.4064730748941656e-05\n",
      "Step: 9060, train/epoch: 2.156116247177124\n",
      "Step: 9070, train/loss: 0.0\n",
      "Step: 9070, train/grad_norm: 7.937687626738714e-16\n",
      "Step: 9070, train/learning_rate: 1.4025067684997339e-05\n",
      "Step: 9070, train/epoch: 2.1584959030151367\n",
      "Step: 9080, train/loss: 0.0\n",
      "Step: 9080, train/grad_norm: 5.154402477718903e-16\n",
      "Step: 9080, train/learning_rate: 1.398540371155832e-05\n",
      "Step: 9080, train/epoch: 2.1608757972717285\n",
      "Step: 9090, train/loss: 0.0\n",
      "Step: 9090, train/grad_norm: 6.4487699034943e-12\n",
      "Step: 9090, train/learning_rate: 1.39457397381193e-05\n",
      "Step: 9090, train/epoch: 2.1632556915283203\n",
      "Step: 9100, train/loss: 0.0\n",
      "Step: 9100, train/grad_norm: 3.625675049079291e-08\n",
      "Step: 9100, train/learning_rate: 1.3906076674174983e-05\n",
      "Step: 9100, train/epoch: 2.165635347366333\n",
      "Step: 9110, train/loss: 0.0\n",
      "Step: 9110, train/grad_norm: 2.422966205927954e-14\n",
      "Step: 9110, train/learning_rate: 1.3866412700735964e-05\n",
      "Step: 9110, train/epoch: 2.168015241622925\n",
      "Step: 9120, train/loss: 0.0\n",
      "Step: 9120, train/grad_norm: 9.669981388706322e-15\n",
      "Step: 9120, train/learning_rate: 1.3826748727296945e-05\n",
      "Step: 9120, train/epoch: 2.1703951358795166\n",
      "Step: 9130, train/loss: 0.0\n",
      "Step: 9130, train/grad_norm: 4.8606891081548476e-12\n",
      "Step: 9130, train/learning_rate: 1.3787085663352627e-05\n",
      "Step: 9130, train/epoch: 2.1727747917175293\n",
      "Step: 9140, train/loss: 0.0\n",
      "Step: 9140, train/grad_norm: 5.025774718267484e-13\n",
      "Step: 9140, train/learning_rate: 1.3747421689913608e-05\n",
      "Step: 9140, train/epoch: 2.175154685974121\n",
      "Step: 9150, train/loss: 0.0\n",
      "Step: 9150, train/grad_norm: 8.03581133368425e-06\n",
      "Step: 9150, train/learning_rate: 1.370775862596929e-05\n",
      "Step: 9150, train/epoch: 2.177534580230713\n",
      "Step: 9160, train/loss: 0.0\n",
      "Step: 9160, train/grad_norm: 2.7440506464304626e-13\n",
      "Step: 9160, train/learning_rate: 1.3668094652530272e-05\n",
      "Step: 9160, train/epoch: 2.1799142360687256\n",
      "Step: 9170, train/loss: 0.0\n",
      "Step: 9170, train/grad_norm: 9.151050676977474e-16\n",
      "Step: 9170, train/learning_rate: 1.3628430679091252e-05\n",
      "Step: 9170, train/epoch: 2.1822941303253174\n",
      "Step: 9180, train/loss: 0.0\n",
      "Step: 9180, train/grad_norm: 8.633459073516259e-13\n",
      "Step: 9180, train/learning_rate: 1.3588767615146935e-05\n",
      "Step: 9180, train/epoch: 2.184674024581909\n",
      "Step: 9190, train/loss: 0.0\n",
      "Step: 9190, train/grad_norm: 1.5126884933460566e-13\n",
      "Step: 9190, train/learning_rate: 1.3549103641707916e-05\n",
      "Step: 9190, train/epoch: 2.187053680419922\n",
      "Step: 9200, train/loss: 0.0\n",
      "Step: 9200, train/grad_norm: 8.324006631546532e-13\n",
      "Step: 9200, train/learning_rate: 1.3509439668268897e-05\n",
      "Step: 9200, train/epoch: 2.1894335746765137\n",
      "Step: 9210, train/loss: 0.0\n",
      "Step: 9210, train/grad_norm: 5.9712426179309684e-15\n",
      "Step: 9210, train/learning_rate: 1.346977660432458e-05\n",
      "Step: 9210, train/epoch: 2.1918134689331055\n",
      "Step: 9220, train/loss: 0.0\n",
      "Step: 9220, train/grad_norm: 3.966824380086109e-08\n",
      "Step: 9220, train/learning_rate: 1.343011263088556e-05\n",
      "Step: 9220, train/epoch: 2.194193124771118\n",
      "Step: 9230, train/loss: 0.0\n",
      "Step: 9230, train/grad_norm: 8.124511907681153e-08\n",
      "Step: 9230, train/learning_rate: 1.3390448657446541e-05\n",
      "Step: 9230, train/epoch: 2.19657301902771\n",
      "Step: 9240, train/loss: 0.0\n",
      "Step: 9240, train/grad_norm: 1.0112117740379567e-11\n",
      "Step: 9240, train/learning_rate: 1.3350785593502223e-05\n",
      "Step: 9240, train/epoch: 2.1989529132843018\n",
      "Step: 9250, train/loss: 0.0\n",
      "Step: 9250, train/grad_norm: 6.67320270764972e-14\n",
      "Step: 9250, train/learning_rate: 1.3311121620063204e-05\n",
      "Step: 9250, train/epoch: 2.2013328075408936\n",
      "Step: 9260, train/loss: 0.0\n",
      "Step: 9260, train/grad_norm: 5.007415992963615e-08\n",
      "Step: 9260, train/learning_rate: 1.3271457646624185e-05\n",
      "Step: 9260, train/epoch: 2.2037124633789062\n",
      "Step: 9270, train/loss: 0.0\n",
      "Step: 9270, train/grad_norm: 2.5812089011340023e-12\n",
      "Step: 9270, train/learning_rate: 1.3231794582679868e-05\n",
      "Step: 9270, train/epoch: 2.206092357635498\n",
      "Step: 9280, train/loss: 0.0\n",
      "Step: 9280, train/grad_norm: 2.713471392024047e-10\n",
      "Step: 9280, train/learning_rate: 1.3192130609240849e-05\n",
      "Step: 9280, train/epoch: 2.20847225189209\n",
      "Step: 9290, train/loss: 0.0\n",
      "Step: 9290, train/grad_norm: 6.104218693014918e-13\n",
      "Step: 9290, train/learning_rate: 1.315246663580183e-05\n",
      "Step: 9290, train/epoch: 2.2108519077301025\n",
      "Step: 9300, train/loss: 0.0\n",
      "Step: 9300, train/grad_norm: 7.058142378595241e-16\n",
      "Step: 9300, train/learning_rate: 1.3112803571857512e-05\n",
      "Step: 9300, train/epoch: 2.2132318019866943\n",
      "Step: 9310, train/loss: 0.0\n",
      "Step: 9310, train/grad_norm: 1.8984136094732373e-14\n",
      "Step: 9310, train/learning_rate: 1.3073139598418493e-05\n",
      "Step: 9310, train/epoch: 2.215611696243286\n",
      "Step: 9320, train/loss: 0.0\n",
      "Step: 9320, train/grad_norm: 3.8011335344503155e-14\n",
      "Step: 9320, train/learning_rate: 1.3033476534474175e-05\n",
      "Step: 9320, train/epoch: 2.217991352081299\n",
      "Step: 9330, train/loss: 0.0\n",
      "Step: 9330, train/grad_norm: 2.2109232847289384e-12\n",
      "Step: 9330, train/learning_rate: 1.2993812561035156e-05\n",
      "Step: 9330, train/epoch: 2.2203712463378906\n",
      "Step: 9340, train/loss: 0.0\n",
      "Step: 9340, train/grad_norm: 3.60376420894773e-17\n",
      "Step: 9340, train/learning_rate: 1.2954148587596137e-05\n",
      "Step: 9340, train/epoch: 2.2227511405944824\n",
      "Step: 9350, train/loss: 0.0\n",
      "Step: 9350, train/grad_norm: 3.069708653935521e-10\n",
      "Step: 9350, train/learning_rate: 1.291448552365182e-05\n",
      "Step: 9350, train/epoch: 2.225130796432495\n",
      "Step: 9360, train/loss: 0.0\n",
      "Step: 9360, train/grad_norm: 4.3962128045988563e-14\n",
      "Step: 9360, train/learning_rate: 1.28748215502128e-05\n",
      "Step: 9360, train/epoch: 2.227510690689087\n",
      "Step: 9370, train/loss: 0.0\n",
      "Step: 9370, train/grad_norm: 3.8459083736685357e-14\n",
      "Step: 9370, train/learning_rate: 1.2835157576773781e-05\n",
      "Step: 9370, train/epoch: 2.2298905849456787\n",
      "Step: 9380, train/loss: 0.0\n",
      "Step: 9380, train/grad_norm: 8.764418634203339e-14\n",
      "Step: 9380, train/learning_rate: 1.2795494512829464e-05\n",
      "Step: 9380, train/epoch: 2.2322702407836914\n",
      "Step: 9390, train/loss: 0.0\n",
      "Step: 9390, train/grad_norm: 1.2382571530633601e-11\n",
      "Step: 9390, train/learning_rate: 1.2755830539390445e-05\n",
      "Step: 9390, train/epoch: 2.234650135040283\n",
      "Step: 9400, train/loss: 0.0\n",
      "Step: 9400, train/grad_norm: 1.8509139533551267e-12\n",
      "Step: 9400, train/learning_rate: 1.2716166565951426e-05\n",
      "Step: 9400, train/epoch: 2.237030029296875\n",
      "Step: 9410, train/loss: 0.0\n",
      "Step: 9410, train/grad_norm: 9.278330881379304e-10\n",
      "Step: 9410, train/learning_rate: 1.2676503502007108e-05\n",
      "Step: 9410, train/epoch: 2.239409923553467\n",
      "Step: 9420, train/loss: 0.0\n",
      "Step: 9420, train/grad_norm: 7.471409118285904e-15\n",
      "Step: 9420, train/learning_rate: 1.2636839528568089e-05\n",
      "Step: 9420, train/epoch: 2.2417895793914795\n",
      "Step: 9430, train/loss: 0.0\n",
      "Step: 9430, train/grad_norm: 1.7021554304941233e-11\n",
      "Step: 9430, train/learning_rate: 1.259717555512907e-05\n",
      "Step: 9430, train/epoch: 2.2441694736480713\n",
      "Step: 9440, train/loss: 0.0\n",
      "Step: 9440, train/grad_norm: 2.9779193000400994e-11\n",
      "Step: 9440, train/learning_rate: 1.2557512491184752e-05\n",
      "Step: 9440, train/epoch: 2.246549367904663\n",
      "Step: 9450, train/loss: 0.0\n",
      "Step: 9450, train/grad_norm: 3.2908897596944475e-17\n",
      "Step: 9450, train/learning_rate: 1.2517848517745733e-05\n",
      "Step: 9450, train/epoch: 2.248929023742676\n",
      "Step: 9460, train/loss: 0.0\n",
      "Step: 9460, train/grad_norm: 9.917232145659534e-18\n",
      "Step: 9460, train/learning_rate: 1.2478184544306714e-05\n",
      "Step: 9460, train/epoch: 2.2513089179992676\n",
      "Step: 9470, train/loss: 0.0\n",
      "Step: 9470, train/grad_norm: 4.975333079554334e-12\n",
      "Step: 9470, train/learning_rate: 1.2438521480362397e-05\n",
      "Step: 9470, train/epoch: 2.2536888122558594\n",
      "Step: 9480, train/loss: 0.0\n",
      "Step: 9480, train/grad_norm: 1.3036360185791906e-12\n",
      "Step: 9480, train/learning_rate: 1.2398857506923378e-05\n",
      "Step: 9480, train/epoch: 2.256068468093872\n",
      "Step: 9490, train/loss: 0.0\n",
      "Step: 9490, train/grad_norm: 7.324555406640343e-10\n",
      "Step: 9490, train/learning_rate: 1.235919444297906e-05\n",
      "Step: 9490, train/epoch: 2.258448362350464\n",
      "Step: 9500, train/loss: 0.0\n",
      "Step: 9500, train/grad_norm: 5.155822600452775e-13\n",
      "Step: 9500, train/learning_rate: 1.2319530469540041e-05\n",
      "Step: 9500, train/epoch: 2.2608282566070557\n",
      "Step: 9510, train/loss: 0.0\n",
      "Step: 9510, train/grad_norm: 9.035244774580065e-12\n",
      "Step: 9510, train/learning_rate: 1.2279866496101022e-05\n",
      "Step: 9510, train/epoch: 2.2632079124450684\n",
      "Step: 9520, train/loss: 0.0\n",
      "Step: 9520, train/grad_norm: 9.988326995819487e-17\n",
      "Step: 9520, train/learning_rate: 1.2240203432156704e-05\n",
      "Step: 9520, train/epoch: 2.26558780670166\n",
      "Step: 9530, train/loss: 0.0\n",
      "Step: 9530, train/grad_norm: 1.8701186377256818e-08\n",
      "Step: 9530, train/learning_rate: 1.2200539458717685e-05\n",
      "Step: 9530, train/epoch: 2.267967700958252\n",
      "Step: 9540, train/loss: 0.0\n",
      "Step: 9540, train/grad_norm: 3.5723510954932536e-11\n",
      "Step: 9540, train/learning_rate: 1.2160875485278666e-05\n",
      "Step: 9540, train/epoch: 2.2703473567962646\n",
      "Step: 9550, train/loss: 0.0\n",
      "Step: 9550, train/grad_norm: 8.560574116245334e-08\n",
      "Step: 9550, train/learning_rate: 1.2121212421334349e-05\n",
      "Step: 9550, train/epoch: 2.2727272510528564\n",
      "Step: 9560, train/loss: 0.0\n",
      "Step: 9560, train/grad_norm: 9.99324918368532e-16\n",
      "Step: 9560, train/learning_rate: 1.208154844789533e-05\n",
      "Step: 9560, train/epoch: 2.2751071453094482\n",
      "Step: 9570, train/loss: 0.0\n",
      "Step: 9570, train/grad_norm: 1.993124903402338e-13\n",
      "Step: 9570, train/learning_rate: 1.204188447445631e-05\n",
      "Step: 9570, train/epoch: 2.277486801147461\n",
      "Step: 9580, train/loss: 0.0\n",
      "Step: 9580, train/grad_norm: 1.6006433485032533e-15\n",
      "Step: 9580, train/learning_rate: 1.2002221410511993e-05\n",
      "Step: 9580, train/epoch: 2.2798666954040527\n",
      "Step: 9590, train/loss: 0.0\n",
      "Step: 9590, train/grad_norm: 6.076462250037551e-11\n",
      "Step: 9590, train/learning_rate: 1.1962557437072974e-05\n",
      "Step: 9590, train/epoch: 2.2822465896606445\n",
      "Step: 9600, train/loss: 0.0\n",
      "Step: 9600, train/grad_norm: 8.044185211545895e-17\n",
      "Step: 9600, train/learning_rate: 1.1922893463633955e-05\n",
      "Step: 9600, train/epoch: 2.2846264839172363\n",
      "Step: 9610, train/loss: 0.0\n",
      "Step: 9610, train/grad_norm: 5.110067577418498e-11\n",
      "Step: 9610, train/learning_rate: 1.1883230399689637e-05\n",
      "Step: 9610, train/epoch: 2.287006139755249\n",
      "Step: 9620, train/loss: 0.0\n",
      "Step: 9620, train/grad_norm: 1.6085276556901396e-12\n",
      "Step: 9620, train/learning_rate: 1.1843566426250618e-05\n",
      "Step: 9620, train/epoch: 2.289386034011841\n",
      "Step: 9630, train/loss: 0.0\n",
      "Step: 9630, train/grad_norm: 3.1362429840472896e-16\n",
      "Step: 9630, train/learning_rate: 1.1803902452811599e-05\n",
      "Step: 9630, train/epoch: 2.2917659282684326\n",
      "Step: 9640, train/loss: 0.0\n",
      "Step: 9640, train/grad_norm: 1.5124563927659818e-12\n",
      "Step: 9640, train/learning_rate: 1.1764239388867281e-05\n",
      "Step: 9640, train/epoch: 2.2941455841064453\n",
      "Step: 9650, train/loss: 0.0\n",
      "Step: 9650, train/grad_norm: 1.390448086530105e-11\n",
      "Step: 9650, train/learning_rate: 1.1724575415428262e-05\n",
      "Step: 9650, train/epoch: 2.296525478363037\n",
      "Step: 9660, train/loss: 0.0\n",
      "Step: 9660, train/grad_norm: 4.167608532584799e-12\n",
      "Step: 9660, train/learning_rate: 1.1684912351483945e-05\n",
      "Step: 9660, train/epoch: 2.298905372619629\n",
      "Step: 9670, train/loss: 0.09570000320672989\n",
      "Step: 9670, train/grad_norm: 5.937888527228485e-13\n",
      "Step: 9670, train/learning_rate: 1.1645248378044926e-05\n",
      "Step: 9670, train/epoch: 2.3012850284576416\n",
      "Step: 9680, train/loss: 0.0\n",
      "Step: 9680, train/grad_norm: 6.589478311960772e-13\n",
      "Step: 9680, train/learning_rate: 1.1605584404605906e-05\n",
      "Step: 9680, train/epoch: 2.3036649227142334\n",
      "Step: 9690, train/loss: 0.0\n",
      "Step: 9690, train/grad_norm: 9.479522722344313e-10\n",
      "Step: 9690, train/learning_rate: 1.1565921340661589e-05\n",
      "Step: 9690, train/epoch: 2.306044816970825\n",
      "Step: 9700, train/loss: 0.0\n",
      "Step: 9700, train/grad_norm: 5.908926126174663e-10\n",
      "Step: 9700, train/learning_rate: 1.152625736722257e-05\n",
      "Step: 9700, train/epoch: 2.308424472808838\n",
      "Step: 9710, train/loss: 0.0\n",
      "Step: 9710, train/grad_norm: 2.5244067361658573e-13\n",
      "Step: 9710, train/learning_rate: 1.148659339378355e-05\n",
      "Step: 9710, train/epoch: 2.3108043670654297\n",
      "Step: 9720, train/loss: 0.0\n",
      "Step: 9720, train/grad_norm: 3.8321261311771715e-15\n",
      "Step: 9720, train/learning_rate: 1.1446930329839233e-05\n",
      "Step: 9720, train/epoch: 2.3131842613220215\n",
      "Step: 9730, train/loss: 0.0\n",
      "Step: 9730, train/grad_norm: 1.069073052306635e-09\n",
      "Step: 9730, train/learning_rate: 1.1407266356400214e-05\n",
      "Step: 9730, train/epoch: 2.315563917160034\n",
      "Step: 9740, train/loss: 0.0\n",
      "Step: 9740, train/grad_norm: 7.368176624389378e-10\n",
      "Step: 9740, train/learning_rate: 1.1367602382961195e-05\n",
      "Step: 9740, train/epoch: 2.317943811416626\n",
      "Step: 9750, train/loss: 0.0\n",
      "Step: 9750, train/grad_norm: 1.0406462536138861e-07\n",
      "Step: 9750, train/learning_rate: 1.1327939319016878e-05\n",
      "Step: 9750, train/epoch: 2.3203237056732178\n",
      "Step: 9760, train/loss: 0.0\n",
      "Step: 9760, train/grad_norm: 1.3243403040874568e-09\n",
      "Step: 9760, train/learning_rate: 1.1288275345577858e-05\n",
      "Step: 9760, train/epoch: 2.3227033615112305\n",
      "Step: 9770, train/loss: 0.0\n",
      "Step: 9770, train/grad_norm: 3.816932405698026e-07\n",
      "Step: 9770, train/learning_rate: 1.124861137213884e-05\n",
      "Step: 9770, train/epoch: 2.3250832557678223\n",
      "Step: 9780, train/loss: 0.0\n",
      "Step: 9780, train/grad_norm: 9.232350439702941e-10\n",
      "Step: 9780, train/learning_rate: 1.1208948308194522e-05\n",
      "Step: 9780, train/epoch: 2.327463150024414\n",
      "Step: 9790, train/loss: 0.0\n",
      "Step: 9790, train/grad_norm: 1.1183489030675009e-11\n",
      "Step: 9790, train/learning_rate: 1.1169284334755503e-05\n",
      "Step: 9790, train/epoch: 2.329843044281006\n",
      "Step: 9800, train/loss: 0.0\n",
      "Step: 9800, train/grad_norm: 1.491725356572715e-06\n",
      "Step: 9800, train/learning_rate: 1.1129620361316483e-05\n",
      "Step: 9800, train/epoch: 2.3322227001190186\n",
      "Step: 9810, train/loss: 0.0\n",
      "Step: 9810, train/grad_norm: 5.252105544427869e-18\n",
      "Step: 9810, train/learning_rate: 1.1089957297372166e-05\n",
      "Step: 9810, train/epoch: 2.3346025943756104\n",
      "Step: 9820, train/loss: 0.0\n",
      "Step: 9820, train/grad_norm: 4.307215917265239e-08\n",
      "Step: 9820, train/learning_rate: 1.1050293323933147e-05\n",
      "Step: 9820, train/epoch: 2.336982488632202\n",
      "Step: 9830, train/loss: 0.0\n",
      "Step: 9830, train/grad_norm: 2.0915196685677984e-08\n",
      "Step: 9830, train/learning_rate: 1.101063025998883e-05\n",
      "Step: 9830, train/epoch: 2.339362144470215\n",
      "Step: 9840, train/loss: 0.0\n",
      "Step: 9840, train/grad_norm: 2.2476298437452275e-13\n",
      "Step: 9840, train/learning_rate: 1.097096628654981e-05\n",
      "Step: 9840, train/epoch: 2.3417420387268066\n",
      "Step: 9850, train/loss: 0.0\n",
      "Step: 9850, train/grad_norm: 1.5862122726417738e-10\n",
      "Step: 9850, train/learning_rate: 1.0931302313110791e-05\n",
      "Step: 9850, train/epoch: 2.3441219329833984\n",
      "Step: 9860, train/loss: 0.0\n",
      "Step: 9860, train/grad_norm: 1.1373492547609755e-16\n",
      "Step: 9860, train/learning_rate: 1.0891639249166474e-05\n",
      "Step: 9860, train/epoch: 2.346501588821411\n",
      "Step: 9870, train/loss: 0.0\n",
      "Step: 9870, train/grad_norm: 9.181005822256338e-08\n",
      "Step: 9870, train/learning_rate: 1.0851975275727455e-05\n",
      "Step: 9870, train/epoch: 2.348881483078003\n",
      "Step: 9880, train/loss: 0.0\n",
      "Step: 9880, train/grad_norm: 5.146319494997442e-07\n",
      "Step: 9880, train/learning_rate: 1.0812311302288435e-05\n",
      "Step: 9880, train/epoch: 2.3512613773345947\n",
      "Step: 9890, train/loss: 0.0\n",
      "Step: 9890, train/grad_norm: 1.1993567251812002e-15\n",
      "Step: 9890, train/learning_rate: 1.0772648238344118e-05\n",
      "Step: 9890, train/epoch: 2.3536410331726074\n",
      "Step: 9900, train/loss: 0.0\n",
      "Step: 9900, train/grad_norm: 2.4008286678145474e-11\n",
      "Step: 9900, train/learning_rate: 1.0732984264905099e-05\n",
      "Step: 9900, train/epoch: 2.356020927429199\n",
      "Step: 9910, train/loss: 0.0\n",
      "Step: 9910, train/grad_norm: 1.0997701593851161e-07\n",
      "Step: 9910, train/learning_rate: 1.069332029146608e-05\n",
      "Step: 9910, train/epoch: 2.358400821685791\n",
      "Step: 9920, train/loss: 0.0\n",
      "Step: 9920, train/grad_norm: 1.0772503548917012e-11\n",
      "Step: 9920, train/learning_rate: 1.0653657227521762e-05\n",
      "Step: 9920, train/epoch: 2.3607804775238037\n",
      "Step: 9930, train/loss: 0.0\n",
      "Step: 9930, train/grad_norm: 1.1864154103591318e-09\n",
      "Step: 9930, train/learning_rate: 1.0613993254082743e-05\n",
      "Step: 9930, train/epoch: 2.3631603717803955\n",
      "Step: 9940, train/loss: 0.0\n",
      "Step: 9940, train/grad_norm: 2.657853104270913e-12\n",
      "Step: 9940, train/learning_rate: 1.0574329280643724e-05\n",
      "Step: 9940, train/epoch: 2.3655402660369873\n",
      "Step: 9950, train/loss: 0.0\n",
      "Step: 9950, train/grad_norm: 0.00013888334797229618\n",
      "Step: 9950, train/learning_rate: 1.0534666216699407e-05\n",
      "Step: 9950, train/epoch: 2.367919921875\n",
      "Step: 9960, train/loss: 0.0\n",
      "Step: 9960, train/grad_norm: 1.4373569001691067e-06\n",
      "Step: 9960, train/learning_rate: 1.0495002243260387e-05\n",
      "Step: 9960, train/epoch: 2.370299816131592\n",
      "Step: 9970, train/loss: 0.0\n",
      "Step: 9970, train/grad_norm: 4.7952588351930087e-14\n",
      "Step: 9970, train/learning_rate: 1.045533917931607e-05\n",
      "Step: 9970, train/epoch: 2.3726797103881836\n",
      "Step: 9980, train/loss: 0.0\n",
      "Step: 9980, train/grad_norm: 2.6630757061357757e-12\n",
      "Step: 9980, train/learning_rate: 1.041567520587705e-05\n",
      "Step: 9980, train/epoch: 2.3750596046447754\n",
      "Step: 9990, train/loss: 0.0\n",
      "Step: 9990, train/grad_norm: 2.540925379435066e-06\n",
      "Step: 9990, train/learning_rate: 1.0376011232438032e-05\n",
      "Step: 9990, train/epoch: 2.377439260482788\n",
      "Step: 10000, train/loss: 0.0\n",
      "Step: 10000, train/grad_norm: 9.33637889222183e-12\n",
      "Step: 10000, train/learning_rate: 1.0336348168493714e-05\n",
      "Step: 10000, train/epoch: 2.37981915473938\n",
      "Step: 10010, train/loss: 0.0\n",
      "Step: 10010, train/grad_norm: 5.431469940475608e-09\n",
      "Step: 10010, train/learning_rate: 1.0296684195054695e-05\n",
      "Step: 10010, train/epoch: 2.3821990489959717\n",
      "Step: 10020, train/loss: 0.0\n",
      "Step: 10020, train/grad_norm: 8.316556887599802e-10\n",
      "Step: 10020, train/learning_rate: 1.0257020221615676e-05\n",
      "Step: 10020, train/epoch: 2.3845787048339844\n",
      "Step: 10030, train/loss: 0.0\n",
      "Step: 10030, train/grad_norm: 1.314920069529768e-12\n",
      "Step: 10030, train/learning_rate: 1.0217357157671358e-05\n",
      "Step: 10030, train/epoch: 2.386958599090576\n",
      "Step: 10040, train/loss: 0.0\n",
      "Step: 10040, train/grad_norm: 0.0014719647588208318\n",
      "Step: 10040, train/learning_rate: 1.017769318423234e-05\n",
      "Step: 10040, train/epoch: 2.389338493347168\n",
      "Step: 10050, train/loss: 0.0\n",
      "Step: 10050, train/grad_norm: 1.3653625756046561e-12\n",
      "Step: 10050, train/learning_rate: 1.013802921079332e-05\n",
      "Step: 10050, train/epoch: 2.3917181491851807\n",
      "Step: 10060, train/loss: 0.0\n",
      "Step: 10060, train/grad_norm: 7.1123928968575e-09\n",
      "Step: 10060, train/learning_rate: 1.0098366146849003e-05\n",
      "Step: 10060, train/epoch: 2.3940980434417725\n",
      "Step: 10070, train/loss: 0.0\n",
      "Step: 10070, train/grad_norm: 4.9854236294777365e-09\n",
      "Step: 10070, train/learning_rate: 1.0058702173409984e-05\n",
      "Step: 10070, train/epoch: 2.3964779376983643\n",
      "Step: 10080, train/loss: 0.0\n",
      "Step: 10080, train/grad_norm: 6.310826972111272e-09\n",
      "Step: 10080, train/learning_rate: 1.0019038199970964e-05\n",
      "Step: 10080, train/epoch: 2.398857593536377\n",
      "Step: 10090, train/loss: 0.0\n",
      "Step: 10090, train/grad_norm: 5.889065679554051e-09\n",
      "Step: 10090, train/learning_rate: 9.979375136026647e-06\n",
      "Step: 10090, train/epoch: 2.4012374877929688\n",
      "Step: 10100, train/loss: 0.0\n",
      "Step: 10100, train/grad_norm: 1.1134070011450947e-12\n",
      "Step: 10100, train/learning_rate: 9.939711162587628e-06\n",
      "Step: 10100, train/epoch: 2.4036173820495605\n",
      "Step: 10110, train/loss: 0.0\n",
      "Step: 10110, train/grad_norm: 4.2519058342542146e-14\n",
      "Step: 10110, train/learning_rate: 9.900047189148609e-06\n",
      "Step: 10110, train/epoch: 2.4059970378875732\n",
      "Step: 10120, train/loss: 0.0\n",
      "Step: 10120, train/grad_norm: 3.851963019385618e-12\n",
      "Step: 10120, train/learning_rate: 9.860384125204291e-06\n",
      "Step: 10120, train/epoch: 2.408376932144165\n",
      "Step: 10130, train/loss: 0.0\n",
      "Step: 10130, train/grad_norm: 9.749495655242413e-10\n",
      "Step: 10130, train/learning_rate: 9.820720151765272e-06\n",
      "Step: 10130, train/epoch: 2.410756826400757\n",
      "Step: 10140, train/loss: 0.0\n",
      "Step: 10140, train/grad_norm: 9.648421384761408e-13\n",
      "Step: 10140, train/learning_rate: 9.781057087820955e-06\n",
      "Step: 10140, train/epoch: 2.4131367206573486\n",
      "Step: 10150, train/loss: 0.0\n",
      "Step: 10150, train/grad_norm: 8.094898085175828e-09\n",
      "Step: 10150, train/learning_rate: 9.741393114381935e-06\n",
      "Step: 10150, train/epoch: 2.4155163764953613\n",
      "Step: 10160, train/loss: 0.0\n",
      "Step: 10160, train/grad_norm: 9.277849954969497e-08\n",
      "Step: 10160, train/learning_rate: 9.701729140942916e-06\n",
      "Step: 10160, train/epoch: 2.417896270751953\n",
      "Step: 10170, train/loss: 0.0\n",
      "Step: 10170, train/grad_norm: 1.3557097068225832e-12\n",
      "Step: 10170, train/learning_rate: 9.662066076998599e-06\n",
      "Step: 10170, train/epoch: 2.420276165008545\n",
      "Step: 10180, train/loss: 0.0\n",
      "Step: 10180, train/grad_norm: 1.0024204383229929e-10\n",
      "Step: 10180, train/learning_rate: 9.62240210355958e-06\n",
      "Step: 10180, train/epoch: 2.4226558208465576\n",
      "Step: 10190, train/loss: 0.0\n",
      "Step: 10190, train/grad_norm: 1.5257326652085013e-14\n",
      "Step: 10190, train/learning_rate: 9.58273813012056e-06\n",
      "Step: 10190, train/epoch: 2.4250357151031494\n",
      "Step: 10200, train/loss: 0.0\n",
      "Step: 10200, train/grad_norm: 8.096463952701532e-16\n",
      "Step: 10200, train/learning_rate: 9.543075066176243e-06\n",
      "Step: 10200, train/epoch: 2.427415609359741\n",
      "Step: 10210, train/loss: 0.0\n",
      "Step: 10210, train/grad_norm: 2.823636506901761e-13\n",
      "Step: 10210, train/learning_rate: 9.503411092737224e-06\n",
      "Step: 10210, train/epoch: 2.429795265197754\n",
      "Step: 10220, train/loss: 0.0\n",
      "Step: 10220, train/grad_norm: 1.1432239688247137e-07\n",
      "Step: 10220, train/learning_rate: 9.463747119298205e-06\n",
      "Step: 10220, train/epoch: 2.4321751594543457\n",
      "Step: 10230, train/loss: 0.0\n",
      "Step: 10230, train/grad_norm: 1.1717236070296622e-08\n",
      "Step: 10230, train/learning_rate: 9.424084055353887e-06\n",
      "Step: 10230, train/epoch: 2.4345550537109375\n",
      "Step: 10240, train/loss: 0.0\n",
      "Step: 10240, train/grad_norm: 1.439971253347494e-08\n",
      "Step: 10240, train/learning_rate: 9.384420081914868e-06\n",
      "Step: 10240, train/epoch: 2.43693470954895\n",
      "Step: 10250, train/loss: 0.0\n",
      "Step: 10250, train/grad_norm: 7.430903115057674e-11\n",
      "Step: 10250, train/learning_rate: 9.344756108475849e-06\n",
      "Step: 10250, train/epoch: 2.439314603805542\n",
      "Step: 10260, train/loss: 0.0\n",
      "Step: 10260, train/grad_norm: 1.8835033870345796e-06\n",
      "Step: 10260, train/learning_rate: 9.305093044531532e-06\n",
      "Step: 10260, train/epoch: 2.441694498062134\n",
      "Step: 10270, train/loss: 0.0\n",
      "Step: 10270, train/grad_norm: 8.812508944799902e-09\n",
      "Step: 10270, train/learning_rate: 9.265429071092512e-06\n",
      "Step: 10270, train/epoch: 2.4440741539001465\n",
      "Step: 10280, train/loss: 0.0\n",
      "Step: 10280, train/grad_norm: 2.0356553775968678e-08\n",
      "Step: 10280, train/learning_rate: 9.225765097653493e-06\n",
      "Step: 10280, train/epoch: 2.4464540481567383\n",
      "Step: 10290, train/loss: 0.0\n",
      "Step: 10290, train/grad_norm: 1.882948311160426e-11\n",
      "Step: 10290, train/learning_rate: 9.186102033709176e-06\n",
      "Step: 10290, train/epoch: 2.44883394241333\n",
      "Step: 10300, train/loss: 0.0\n",
      "Step: 10300, train/grad_norm: 3.6778150240990987e-14\n",
      "Step: 10300, train/learning_rate: 9.146438060270157e-06\n",
      "Step: 10300, train/epoch: 2.4512135982513428\n",
      "Step: 10310, train/loss: 0.0\n",
      "Step: 10310, train/grad_norm: 2.0938148992399874e-08\n",
      "Step: 10310, train/learning_rate: 9.10677499632584e-06\n",
      "Step: 10310, train/epoch: 2.4535934925079346\n",
      "Step: 10320, train/loss: 0.0\n",
      "Step: 10320, train/grad_norm: 1.490131108783377e-11\n",
      "Step: 10320, train/learning_rate: 9.06711102288682e-06\n",
      "Step: 10320, train/epoch: 2.4559733867645264\n",
      "Step: 10330, train/loss: 0.0\n",
      "Step: 10330, train/grad_norm: 6.330469568688324e-12\n",
      "Step: 10330, train/learning_rate: 9.027447049447801e-06\n",
      "Step: 10330, train/epoch: 2.458353281021118\n",
      "Step: 10340, train/loss: 0.0\n",
      "Step: 10340, train/grad_norm: 1.891015420424011e-13\n",
      "Step: 10340, train/learning_rate: 8.987783985503484e-06\n",
      "Step: 10340, train/epoch: 2.460732936859131\n",
      "Step: 10350, train/loss: 0.0\n",
      "Step: 10350, train/grad_norm: 2.441065309483352e-16\n",
      "Step: 10350, train/learning_rate: 8.948120012064464e-06\n",
      "Step: 10350, train/epoch: 2.4631128311157227\n",
      "Step: 10360, train/loss: 0.0\n",
      "Step: 10360, train/grad_norm: 3.2359505297563373e-08\n",
      "Step: 10360, train/learning_rate: 8.908456038625445e-06\n",
      "Step: 10360, train/epoch: 2.4654927253723145\n",
      "Step: 10370, train/loss: 0.0\n",
      "Step: 10370, train/grad_norm: 6.303524108935976e-12\n",
      "Step: 10370, train/learning_rate: 8.868792974681128e-06\n",
      "Step: 10370, train/epoch: 2.467872381210327\n",
      "Step: 10380, train/loss: 0.0\n",
      "Step: 10380, train/grad_norm: 3.6995855390248116e-16\n",
      "Step: 10380, train/learning_rate: 8.829129001242109e-06\n",
      "Step: 10380, train/epoch: 2.470252275466919\n",
      "Step: 10390, train/loss: 0.0\n",
      "Step: 10390, train/grad_norm: 3.145941352474674e-13\n",
      "Step: 10390, train/learning_rate: 8.78946502780309e-06\n",
      "Step: 10390, train/epoch: 2.4726321697235107\n",
      "Step: 10400, train/loss: 0.0\n",
      "Step: 10400, train/grad_norm: 1.0021896201806157e-08\n",
      "Step: 10400, train/learning_rate: 8.749801963858772e-06\n",
      "Step: 10400, train/epoch: 2.4750118255615234\n",
      "Step: 10410, train/loss: 0.0\n",
      "Step: 10410, train/grad_norm: 8.289245483593388e-13\n",
      "Step: 10410, train/learning_rate: 8.710137990419753e-06\n",
      "Step: 10410, train/epoch: 2.4773917198181152\n",
      "Step: 10420, train/loss: 0.0\n",
      "Step: 10420, train/grad_norm: 1.56402153598037e-13\n",
      "Step: 10420, train/learning_rate: 8.670474016980734e-06\n",
      "Step: 10420, train/epoch: 2.479771614074707\n",
      "Step: 10430, train/loss: 0.0\n",
      "Step: 10430, train/grad_norm: 6.730261313536712e-12\n",
      "Step: 10430, train/learning_rate: 8.630810953036416e-06\n",
      "Step: 10430, train/epoch: 2.4821512699127197\n",
      "Step: 10440, train/loss: 0.0\n",
      "Step: 10440, train/grad_norm: 4.870469479112405e-12\n",
      "Step: 10440, train/learning_rate: 8.591146979597397e-06\n",
      "Step: 10440, train/epoch: 2.4845311641693115\n",
      "Step: 10450, train/loss: 0.0\n",
      "Step: 10450, train/grad_norm: 3.755519628612092e-07\n",
      "Step: 10450, train/learning_rate: 8.551483006158378e-06\n",
      "Step: 10450, train/epoch: 2.4869110584259033\n",
      "Step: 10460, train/loss: 0.0\n",
      "Step: 10460, train/grad_norm: 2.922139641370336e-09\n",
      "Step: 10460, train/learning_rate: 8.51181994221406e-06\n",
      "Step: 10460, train/epoch: 2.489290714263916\n",
      "Step: 10470, train/loss: 0.0\n",
      "Step: 10470, train/grad_norm: 1.7306338653821918e-13\n",
      "Step: 10470, train/learning_rate: 8.472155968775041e-06\n",
      "Step: 10470, train/epoch: 2.491670608520508\n",
      "Step: 10480, train/loss: 0.0\n",
      "Step: 10480, train/grad_norm: 2.2955760556331306e-11\n",
      "Step: 10480, train/learning_rate: 8.432492904830724e-06\n",
      "Step: 10480, train/epoch: 2.4940505027770996\n",
      "Step: 10490, train/loss: 0.0\n",
      "Step: 10490, train/grad_norm: 2.9727689754288633e-10\n",
      "Step: 10490, train/learning_rate: 8.392828931391705e-06\n",
      "Step: 10490, train/epoch: 2.4964301586151123\n",
      "Step: 10500, train/loss: 0.0\n",
      "Step: 10500, train/grad_norm: 2.1166535435695273e-16\n",
      "Step: 10500, train/learning_rate: 8.353164957952686e-06\n",
      "Step: 10500, train/epoch: 2.498810052871704\n",
      "Step: 10510, train/loss: 0.0\n",
      "Step: 10510, train/grad_norm: 7.302769500228123e-11\n",
      "Step: 10510, train/learning_rate: 8.313501894008368e-06\n",
      "Step: 10510, train/epoch: 2.501189947128296\n",
      "Step: 10520, train/loss: 0.0\n",
      "Step: 10520, train/grad_norm: 1.8190180028909708e-08\n",
      "Step: 10520, train/learning_rate: 8.273837920569349e-06\n",
      "Step: 10520, train/epoch: 2.5035698413848877\n",
      "Step: 10530, train/loss: 0.0\n",
      "Step: 10530, train/grad_norm: 1.0892709477461349e-11\n",
      "Step: 10530, train/learning_rate: 8.23417394713033e-06\n",
      "Step: 10530, train/epoch: 2.5059494972229004\n",
      "Step: 10540, train/loss: 0.0\n",
      "Step: 10540, train/grad_norm: 5.508185053904047e-13\n",
      "Step: 10540, train/learning_rate: 8.194510883186013e-06\n",
      "Step: 10540, train/epoch: 2.508329391479492\n",
      "Step: 10550, train/loss: 0.0\n",
      "Step: 10550, train/grad_norm: 3.993341977093223e-07\n",
      "Step: 10550, train/learning_rate: 8.154846909746993e-06\n",
      "Step: 10550, train/epoch: 2.510709285736084\n",
      "Step: 10560, train/loss: 0.0\n",
      "Step: 10560, train/grad_norm: 1.4495784128598643e-08\n",
      "Step: 10560, train/learning_rate: 8.115182936307974e-06\n",
      "Step: 10560, train/epoch: 2.5130889415740967\n",
      "Step: 10570, train/loss: 0.0\n",
      "Step: 10570, train/grad_norm: 1.5091348309903552e-12\n",
      "Step: 10570, train/learning_rate: 8.075519872363657e-06\n",
      "Step: 10570, train/epoch: 2.5154688358306885\n",
      "Step: 10580, train/loss: 0.0\n",
      "Step: 10580, train/grad_norm: 7.769931471557356e-07\n",
      "Step: 10580, train/learning_rate: 8.035855898924638e-06\n",
      "Step: 10580, train/epoch: 2.5178487300872803\n",
      "Step: 10590, train/loss: 0.0\n",
      "Step: 10590, train/grad_norm: 1.196048468599109e-11\n",
      "Step: 10590, train/learning_rate: 7.996191925485618e-06\n",
      "Step: 10590, train/epoch: 2.520228385925293\n",
      "Step: 10600, train/loss: 0.0\n",
      "Step: 10600, train/grad_norm: 6.592161927443385e-09\n",
      "Step: 10600, train/learning_rate: 7.956528861541301e-06\n",
      "Step: 10600, train/epoch: 2.5226082801818848\n",
      "Step: 10610, train/loss: 0.0\n",
      "Step: 10610, train/grad_norm: 1.8077022517770658e-11\n",
      "Step: 10610, train/learning_rate: 7.916864888102282e-06\n",
      "Step: 10610, train/epoch: 2.5249881744384766\n",
      "Step: 10620, train/loss: 0.0\n",
      "Step: 10620, train/grad_norm: 8.387438015233784e-07\n",
      "Step: 10620, train/learning_rate: 7.877200914663263e-06\n",
      "Step: 10620, train/epoch: 2.5273678302764893\n",
      "Step: 10630, train/loss: 0.0\n",
      "Step: 10630, train/grad_norm: 5.88470570016264e-13\n",
      "Step: 10630, train/learning_rate: 7.837537850718945e-06\n",
      "Step: 10630, train/epoch: 2.529747724533081\n",
      "Step: 10640, train/loss: 0.0\n",
      "Step: 10640, train/grad_norm: 1.1669110989986908e-11\n",
      "Step: 10640, train/learning_rate: 7.797873877279926e-06\n",
      "Step: 10640, train/epoch: 2.532127618789673\n",
      "Step: 10650, train/loss: 0.0\n",
      "Step: 10650, train/grad_norm: 0.003859208896756172\n",
      "Step: 10650, train/learning_rate: 7.758210813335609e-06\n",
      "Step: 10650, train/epoch: 2.5345072746276855\n",
      "Step: 10660, train/loss: 0.0\n",
      "Step: 10660, train/grad_norm: 2.9160512979992115e-12\n",
      "Step: 10660, train/learning_rate: 7.71854683989659e-06\n",
      "Step: 10660, train/epoch: 2.5368871688842773\n",
      "Step: 10670, train/loss: 0.0\n",
      "Step: 10670, train/grad_norm: 2.463417247966504e-09\n",
      "Step: 10670, train/learning_rate: 7.67888286645757e-06\n",
      "Step: 10670, train/epoch: 2.539267063140869\n",
      "Step: 10680, train/loss: 0.0\n",
      "Step: 10680, train/grad_norm: 2.8789012840313433e-10\n",
      "Step: 10680, train/learning_rate: 7.639219802513253e-06\n",
      "Step: 10680, train/epoch: 2.541646718978882\n",
      "Step: 10690, train/loss: 0.0\n",
      "Step: 10690, train/grad_norm: 1.883574893279949e-11\n",
      "Step: 10690, train/learning_rate: 7.599555829074234e-06\n",
      "Step: 10690, train/epoch: 2.5440266132354736\n",
      "Step: 10700, train/loss: 0.0\n",
      "Step: 10700, train/grad_norm: 1.0826570542854519e-11\n",
      "Step: 10700, train/learning_rate: 7.5598923103825655e-06\n",
      "Step: 10700, train/epoch: 2.5464065074920654\n",
      "Step: 10710, train/loss: 0.0\n",
      "Step: 10710, train/grad_norm: 2.656152631175246e-09\n",
      "Step: 10710, train/learning_rate: 7.520228336943546e-06\n",
      "Step: 10710, train/epoch: 2.5487864017486572\n",
      "Step: 10720, train/loss: 0.0\n",
      "Step: 10720, train/grad_norm: 5.710798847702314e-11\n",
      "Step: 10720, train/learning_rate: 7.480564818251878e-06\n",
      "Step: 10720, train/epoch: 2.55116605758667\n",
      "Step: 10730, train/loss: 0.0\n",
      "Step: 10730, train/grad_norm: 1.9770417941655793e-11\n",
      "Step: 10730, train/learning_rate: 7.44090129956021e-06\n",
      "Step: 10730, train/epoch: 2.5535459518432617\n",
      "Step: 10740, train/loss: 0.0\n",
      "Step: 10740, train/grad_norm: 5.6895970573789256e-11\n",
      "Step: 10740, train/learning_rate: 7.4012373261211906e-06\n",
      "Step: 10740, train/epoch: 2.5559258460998535\n",
      "Step: 10750, train/loss: 0.0\n",
      "Step: 10750, train/grad_norm: 4.0262174150207386e-10\n",
      "Step: 10750, train/learning_rate: 7.361573807429522e-06\n",
      "Step: 10750, train/epoch: 2.558305501937866\n",
      "Step: 10760, train/loss: 0.0\n",
      "Step: 10760, train/grad_norm: 6.29612403879598e-11\n",
      "Step: 10760, train/learning_rate: 7.321910288737854e-06\n",
      "Step: 10760, train/epoch: 2.560685396194458\n",
      "Step: 10770, train/loss: 0.0044999998062849045\n",
      "Step: 10770, train/grad_norm: 8.95503848852286e-09\n",
      "Step: 10770, train/learning_rate: 7.282246770046186e-06\n",
      "Step: 10770, train/epoch: 2.56306529045105\n",
      "Step: 10780, train/loss: 0.0\n",
      "Step: 10780, train/grad_norm: 2.790528981222451e-07\n",
      "Step: 10780, train/learning_rate: 7.2425827966071665e-06\n",
      "Step: 10780, train/epoch: 2.5654449462890625\n",
      "Step: 10790, train/loss: 0.0\n",
      "Step: 10790, train/grad_norm: 6.680606068343309e-11\n",
      "Step: 10790, train/learning_rate: 7.202919277915498e-06\n",
      "Step: 10790, train/epoch: 2.5678248405456543\n",
      "Step: 10800, train/loss: 0.0\n",
      "Step: 10800, train/grad_norm: 1.454792375454872e-08\n",
      "Step: 10800, train/learning_rate: 7.16325575922383e-06\n",
      "Step: 10800, train/epoch: 2.570204734802246\n",
      "Step: 10810, train/loss: 0.0\n",
      "Step: 10810, train/grad_norm: 5.3649071296568707e-11\n",
      "Step: 10810, train/learning_rate: 7.123591785784811e-06\n",
      "Step: 10810, train/epoch: 2.572584390640259\n",
      "Step: 10820, train/loss: 0.0\n",
      "Step: 10820, train/grad_norm: 2.3702376866774344e-12\n",
      "Step: 10820, train/learning_rate: 7.0839282670931425e-06\n",
      "Step: 10820, train/epoch: 2.5749642848968506\n",
      "Step: 10830, train/loss: 0.0\n",
      "Step: 10830, train/grad_norm: 4.267020628978199e-14\n",
      "Step: 10830, train/learning_rate: 7.044264748401474e-06\n",
      "Step: 10830, train/epoch: 2.5773441791534424\n",
      "Step: 10840, train/loss: 0.0\n",
      "Step: 10840, train/grad_norm: 7.71462782722665e-06\n",
      "Step: 10840, train/learning_rate: 7.004600774962455e-06\n",
      "Step: 10840, train/epoch: 2.579723834991455\n",
      "Step: 10850, train/loss: 0.0\n",
      "Step: 10850, train/grad_norm: 3.8499106125300386e-08\n",
      "Step: 10850, train/learning_rate: 6.964937256270787e-06\n",
      "Step: 10850, train/epoch: 2.582103729248047\n",
      "Step: 10860, train/loss: 0.0\n",
      "Step: 10860, train/grad_norm: 7.730554898444097e-06\n",
      "Step: 10860, train/learning_rate: 6.9252737375791185e-06\n",
      "Step: 10860, train/epoch: 2.5844836235046387\n",
      "Step: 10870, train/loss: 0.0\n",
      "Step: 10870, train/grad_norm: 2.6690602505929917e-10\n",
      "Step: 10870, train/learning_rate: 6.88561021888745e-06\n",
      "Step: 10870, train/epoch: 2.5868632793426514\n",
      "Step: 10880, train/loss: 0.0\n",
      "Step: 10880, train/grad_norm: 1.6939523395098632e-10\n",
      "Step: 10880, train/learning_rate: 6.845946245448431e-06\n",
      "Step: 10880, train/epoch: 2.589243173599243\n",
      "Step: 10890, train/loss: 0.0\n",
      "Step: 10890, train/grad_norm: 2.0077663490103426e-12\n",
      "Step: 10890, train/learning_rate: 6.806282726756763e-06\n",
      "Step: 10890, train/epoch: 2.591623067855835\n",
      "Step: 10900, train/loss: 0.0\n",
      "Step: 10900, train/grad_norm: 2.077421186186257e-06\n",
      "Step: 10900, train/learning_rate: 6.7666192080650944e-06\n",
      "Step: 10900, train/epoch: 2.5940029621124268\n",
      "Step: 10910, train/loss: 0.0\n",
      "Step: 10910, train/grad_norm: 5.225810468800773e-07\n",
      "Step: 10910, train/learning_rate: 6.726955234626075e-06\n",
      "Step: 10910, train/epoch: 2.5963826179504395\n",
      "Step: 10920, train/loss: 0.0\n",
      "Step: 10920, train/grad_norm: 1.6240728380190195e-12\n",
      "Step: 10920, train/learning_rate: 6.687291715934407e-06\n",
      "Step: 10920, train/epoch: 2.5987625122070312\n",
      "Step: 10930, train/loss: 0.0\n",
      "Step: 10930, train/grad_norm: 5.242287492990272e-09\n",
      "Step: 10930, train/learning_rate: 6.647628197242739e-06\n",
      "Step: 10930, train/epoch: 2.601142406463623\n",
      "Step: 10940, train/loss: 0.0\n",
      "Step: 10940, train/grad_norm: 1.802835547515258e-09\n",
      "Step: 10940, train/learning_rate: 6.60796467855107e-06\n",
      "Step: 10940, train/epoch: 2.6035220623016357\n",
      "Step: 10950, train/loss: 0.0\n",
      "Step: 10950, train/grad_norm: 6.507356875573578e-10\n",
      "Step: 10950, train/learning_rate: 6.568300705112051e-06\n",
      "Step: 10950, train/epoch: 2.6059019565582275\n",
      "Step: 10960, train/loss: 0.0\n",
      "Step: 10960, train/grad_norm: 1.459547704918407e-09\n",
      "Step: 10960, train/learning_rate: 6.528637186420383e-06\n",
      "Step: 10960, train/epoch: 2.6082818508148193\n",
      "Step: 10970, train/loss: 0.0\n",
      "Step: 10970, train/grad_norm: 1.713587938212413e-09\n",
      "Step: 10970, train/learning_rate: 6.488973667728715e-06\n",
      "Step: 10970, train/epoch: 2.610661506652832\n",
      "Step: 10980, train/loss: 0.0\n",
      "Step: 10980, train/grad_norm: 9.378847698471304e-10\n",
      "Step: 10980, train/learning_rate: 6.4493096942896955e-06\n",
      "Step: 10980, train/epoch: 2.613041400909424\n",
      "Step: 10990, train/loss: 0.0\n",
      "Step: 10990, train/grad_norm: 7.887468456146962e-08\n",
      "Step: 10990, train/learning_rate: 6.409646175598027e-06\n",
      "Step: 10990, train/epoch: 2.6154212951660156\n",
      "Step: 11000, train/loss: 0.0\n",
      "Step: 11000, train/grad_norm: 8.608311286018022e-10\n",
      "Step: 11000, train/learning_rate: 6.369982656906359e-06\n",
      "Step: 11000, train/epoch: 2.6178009510040283\n",
      "Step: 11010, train/loss: 0.0\n",
      "Step: 11010, train/grad_norm: 6.754399350938911e-08\n",
      "Step: 11010, train/learning_rate: 6.33031868346734e-06\n",
      "Step: 11010, train/epoch: 2.62018084526062\n",
      "Step: 11020, train/loss: 0.0\n",
      "Step: 11020, train/grad_norm: 4.7033154260134324e-08\n",
      "Step: 11020, train/learning_rate: 6.2906551647756714e-06\n",
      "Step: 11020, train/epoch: 2.622560739517212\n",
      "Step: 11030, train/loss: 0.0\n",
      "Step: 11030, train/grad_norm: 3.473173970647281e-10\n",
      "Step: 11030, train/learning_rate: 6.250991646084003e-06\n",
      "Step: 11030, train/epoch: 2.6249403953552246\n",
      "Step: 11040, train/loss: 0.0\n",
      "Step: 11040, train/grad_norm: 1.9915800952841067e-10\n",
      "Step: 11040, train/learning_rate: 6.211328127392335e-06\n",
      "Step: 11040, train/epoch: 2.6273202896118164\n",
      "Step: 11050, train/loss: 0.0\n",
      "Step: 11050, train/grad_norm: 1.1449671072361056e-10\n",
      "Step: 11050, train/learning_rate: 6.171664153953316e-06\n",
      "Step: 11050, train/epoch: 2.629700183868408\n",
      "Step: 11060, train/loss: 0.0\n",
      "Step: 11060, train/grad_norm: 2.603448592708446e-05\n",
      "Step: 11060, train/learning_rate: 6.132000635261647e-06\n",
      "Step: 11060, train/epoch: 2.632080078125\n",
      "Step: 11070, train/loss: 0.0\n",
      "Step: 11070, train/grad_norm: 2.33153016893084e-08\n",
      "Step: 11070, train/learning_rate: 6.092337116569979e-06\n",
      "Step: 11070, train/epoch: 2.6344597339630127\n",
      "Step: 11080, train/loss: 0.0\n",
      "Step: 11080, train/grad_norm: 6.044393785487046e-08\n",
      "Step: 11080, train/learning_rate: 6.05267314313096e-06\n",
      "Step: 11080, train/epoch: 2.6368396282196045\n",
      "Step: 11090, train/loss: 0.0\n",
      "Step: 11090, train/grad_norm: 1.9529346900526434e-05\n",
      "Step: 11090, train/learning_rate: 6.013009624439292e-06\n",
      "Step: 11090, train/epoch: 2.6392195224761963\n",
      "Step: 11100, train/loss: 0.0\n",
      "Step: 11100, train/grad_norm: 3.1979516279534437e-07\n",
      "Step: 11100, train/learning_rate: 5.973346105747623e-06\n",
      "Step: 11100, train/epoch: 2.641599178314209\n",
      "Step: 11110, train/loss: 0.0\n",
      "Step: 11110, train/grad_norm: 2.3736688170572506e-09\n",
      "Step: 11110, train/learning_rate: 5.933682587055955e-06\n",
      "Step: 11110, train/epoch: 2.643979072570801\n",
      "Step: 11120, train/loss: 0.0\n",
      "Step: 11120, train/grad_norm: 1.6472760655528162e-13\n",
      "Step: 11120, train/learning_rate: 5.894018613616936e-06\n",
      "Step: 11120, train/epoch: 2.6463589668273926\n",
      "Step: 11130, train/loss: 0.0\n",
      "Step: 11130, train/grad_norm: 5.265078595328987e-09\n",
      "Step: 11130, train/learning_rate: 5.854355094925268e-06\n",
      "Step: 11130, train/epoch: 2.6487386226654053\n",
      "Step: 11140, train/loss: 0.0\n",
      "Step: 11140, train/grad_norm: 1.9562013120477673e-10\n",
      "Step: 11140, train/learning_rate: 5.814691576233599e-06\n",
      "Step: 11140, train/epoch: 2.651118516921997\n",
      "Step: 11150, train/loss: 0.0\n",
      "Step: 11150, train/grad_norm: 2.679417718809418e-07\n",
      "Step: 11150, train/learning_rate: 5.77502760279458e-06\n",
      "Step: 11150, train/epoch: 2.653498411178589\n",
      "Step: 11160, train/loss: 0.0\n",
      "Step: 11160, train/grad_norm: 3.7640386582615554e-15\n",
      "Step: 11160, train/learning_rate: 5.735364084102912e-06\n",
      "Step: 11160, train/epoch: 2.6558780670166016\n",
      "Step: 11170, train/loss: 0.0\n",
      "Step: 11170, train/grad_norm: 6.170466360089222e-10\n",
      "Step: 11170, train/learning_rate: 5.695700565411244e-06\n",
      "Step: 11170, train/epoch: 2.6582579612731934\n",
      "Step: 11180, train/loss: 0.0\n",
      "Step: 11180, train/grad_norm: 3.519458550726995e-05\n",
      "Step: 11180, train/learning_rate: 5.656036591972224e-06\n",
      "Step: 11180, train/epoch: 2.660637855529785\n",
      "Step: 11190, train/loss: 0.0\n",
      "Step: 11190, train/grad_norm: 5.471854969130163e-09\n",
      "Step: 11190, train/learning_rate: 5.616373073280556e-06\n",
      "Step: 11190, train/epoch: 2.663017511367798\n",
      "Step: 11200, train/loss: 0.0\n",
      "Step: 11200, train/grad_norm: 5.146180703796688e-10\n",
      "Step: 11200, train/learning_rate: 5.576709554588888e-06\n",
      "Step: 11200, train/epoch: 2.6653974056243896\n",
      "Step: 11210, train/loss: 0.0\n",
      "Step: 11210, train/grad_norm: 6.950593572918606e-11\n",
      "Step: 11210, train/learning_rate: 5.5370460358972196e-06\n",
      "Step: 11210, train/epoch: 2.6677772998809814\n",
      "Step: 11220, train/loss: 0.0\n",
      "Step: 11220, train/grad_norm: 1.6132453595218976e-07\n",
      "Step: 11220, train/learning_rate: 5.4973820624582e-06\n",
      "Step: 11220, train/epoch: 2.670156955718994\n",
      "Step: 11230, train/loss: 0.0\n",
      "Step: 11230, train/grad_norm: 2.0983427495568918e-10\n",
      "Step: 11230, train/learning_rate: 5.457718543766532e-06\n",
      "Step: 11230, train/epoch: 2.672536849975586\n",
      "Step: 11240, train/loss: 0.0\n",
      "Step: 11240, train/grad_norm: 5.746683129359553e-10\n",
      "Step: 11240, train/learning_rate: 5.418055025074864e-06\n",
      "Step: 11240, train/epoch: 2.6749167442321777\n",
      "Step: 11250, train/loss: 0.0\n",
      "Step: 11250, train/grad_norm: 8.939616964198649e-05\n",
      "Step: 11250, train/learning_rate: 5.378391051635845e-06\n",
      "Step: 11250, train/epoch: 2.6772966384887695\n",
      "Step: 11260, train/loss: 0.0\n",
      "Step: 11260, train/grad_norm: 1.098525492793101e-09\n",
      "Step: 11260, train/learning_rate: 5.338727532944176e-06\n",
      "Step: 11260, train/epoch: 2.6796762943267822\n",
      "Step: 11270, train/loss: 0.0\n",
      "Step: 11270, train/grad_norm: 1.4596412256651092e-07\n",
      "Step: 11270, train/learning_rate: 5.299064014252508e-06\n",
      "Step: 11270, train/epoch: 2.682056188583374\n",
      "Step: 11280, train/loss: 0.00019999999494757503\n",
      "Step: 11280, train/grad_norm: 7.889443764952375e-09\n",
      "Step: 11280, train/learning_rate: 5.25940049556084e-06\n",
      "Step: 11280, train/epoch: 2.684436082839966\n",
      "Step: 11290, train/loss: 0.0\n",
      "Step: 11290, train/grad_norm: 6.729757018320015e-08\n",
      "Step: 11290, train/learning_rate: 5.219736522121821e-06\n",
      "Step: 11290, train/epoch: 2.6868157386779785\n",
      "Step: 11300, train/loss: 0.0\n",
      "Step: 11300, train/grad_norm: 2.51985125032661e-06\n",
      "Step: 11300, train/learning_rate: 5.180073003430152e-06\n",
      "Step: 11300, train/epoch: 2.6891956329345703\n",
      "Step: 11310, train/loss: 0.0\n",
      "Step: 11310, train/grad_norm: 8.080297675405745e-08\n",
      "Step: 11310, train/learning_rate: 5.140409484738484e-06\n",
      "Step: 11310, train/epoch: 2.691575527191162\n",
      "Step: 11320, train/loss: 0.0\n",
      "Step: 11320, train/grad_norm: 2.934285193295638e-12\n",
      "Step: 11320, train/learning_rate: 5.100745511299465e-06\n",
      "Step: 11320, train/epoch: 2.693955183029175\n",
      "Step: 11330, train/loss: 0.0\n",
      "Step: 11330, train/grad_norm: 1.4882409540839525e-14\n",
      "Step: 11330, train/learning_rate: 5.0610819926077966e-06\n",
      "Step: 11330, train/epoch: 2.6963350772857666\n",
      "Step: 11340, train/loss: 0.0\n",
      "Step: 11340, train/grad_norm: 2.5641813294896565e-07\n",
      "Step: 11340, train/learning_rate: 5.021418473916128e-06\n",
      "Step: 11340, train/epoch: 2.6987149715423584\n",
      "Step: 11350, train/loss: 0.0\n",
      "Step: 11350, train/grad_norm: 2.670574604014299e-15\n",
      "Step: 11350, train/learning_rate: 4.981754500477109e-06\n",
      "Step: 11350, train/epoch: 2.701094627380371\n",
      "Step: 11360, train/loss: 0.0\n",
      "Step: 11360, train/grad_norm: 1.9941022166847233e-09\n",
      "Step: 11360, train/learning_rate: 4.942090981785441e-06\n",
      "Step: 11360, train/epoch: 2.703474521636963\n",
      "Step: 11370, train/loss: 0.0\n",
      "Step: 11370, train/grad_norm: 2.2934948384900622e-10\n",
      "Step: 11370, train/learning_rate: 4.9024274630937725e-06\n",
      "Step: 11370, train/epoch: 2.7058544158935547\n",
      "Step: 11380, train/loss: 0.0\n",
      "Step: 11380, train/grad_norm: 4.665036001938461e-08\n",
      "Step: 11380, train/learning_rate: 4.862763944402104e-06\n",
      "Step: 11380, train/epoch: 2.7082340717315674\n",
      "Step: 11390, train/loss: 0.0\n",
      "Step: 11390, train/grad_norm: 8.854980637629239e-10\n",
      "Step: 11390, train/learning_rate: 4.823099970963085e-06\n",
      "Step: 11390, train/epoch: 2.710613965988159\n",
      "Step: 11400, train/loss: 0.000699999975040555\n",
      "Step: 11400, train/grad_norm: 1.1187445284402409e-12\n",
      "Step: 11400, train/learning_rate: 4.783436452271417e-06\n",
      "Step: 11400, train/epoch: 2.712993860244751\n",
      "Step: 11410, train/loss: 0.0\n",
      "Step: 11410, train/grad_norm: 9.377490103879005e-13\n",
      "Step: 11410, train/learning_rate: 4.7437729335797485e-06\n",
      "Step: 11410, train/epoch: 2.7153735160827637\n",
      "Step: 11420, train/loss: 0.0\n",
      "Step: 11420, train/grad_norm: 4.026756428299194e-10\n",
      "Step: 11420, train/learning_rate: 4.704108960140729e-06\n",
      "Step: 11420, train/epoch: 2.7177534103393555\n",
      "Step: 11430, train/loss: 0.0\n",
      "Step: 11430, train/grad_norm: 1.7489544035242943e-08\n",
      "Step: 11430, train/learning_rate: 4.664445441449061e-06\n",
      "Step: 11430, train/epoch: 2.7201333045959473\n",
      "Step: 11440, train/loss: 0.0\n",
      "Step: 11440, train/grad_norm: 6.332200822717349e-12\n",
      "Step: 11440, train/learning_rate: 4.624781922757393e-06\n",
      "Step: 11440, train/epoch: 2.722513198852539\n",
      "Step: 11450, train/loss: 0.0\n",
      "Step: 11450, train/grad_norm: 5.349360884809862e-11\n",
      "Step: 11450, train/learning_rate: 4.5851184040657245e-06\n",
      "Step: 11450, train/epoch: 2.7248928546905518\n",
      "Step: 11460, train/loss: 0.0\n",
      "Step: 11460, train/grad_norm: 9.079240831377788e-12\n",
      "Step: 11460, train/learning_rate: 4.545454430626705e-06\n",
      "Step: 11460, train/epoch: 2.7272727489471436\n",
      "Step: 11470, train/loss: 0.0\n",
      "Step: 11470, train/grad_norm: 6.724032138374914e-13\n",
      "Step: 11470, train/learning_rate: 4.505790911935037e-06\n",
      "Step: 11470, train/epoch: 2.7296526432037354\n",
      "Step: 11480, train/loss: 0.0\n",
      "Step: 11480, train/grad_norm: 7.74809175152491e-11\n",
      "Step: 11480, train/learning_rate: 4.466127393243369e-06\n",
      "Step: 11480, train/epoch: 2.732032299041748\n",
      "Step: 11490, train/loss: 0.0\n",
      "Step: 11490, train/grad_norm: 1.4136837588139134e-14\n",
      "Step: 11490, train/learning_rate: 4.4264634198043495e-06\n",
      "Step: 11490, train/epoch: 2.73441219329834\n",
      "Step: 11500, train/loss: 0.0\n",
      "Step: 11500, train/grad_norm: 4.050475232997286e-13\n",
      "Step: 11500, train/learning_rate: 4.386799901112681e-06\n",
      "Step: 11500, train/epoch: 2.7367920875549316\n",
      "Step: 11510, train/loss: 0.0\n",
      "Step: 11510, train/grad_norm: 1.4091597888565843e-09\n",
      "Step: 11510, train/learning_rate: 4.347136382421013e-06\n",
      "Step: 11510, train/epoch: 2.7391717433929443\n",
      "Step: 11520, train/loss: 0.0\n",
      "Step: 11520, train/grad_norm: 8.625901659620183e-10\n",
      "Step: 11520, train/learning_rate: 4.307472408981994e-06\n",
      "Step: 11520, train/epoch: 2.741551637649536\n",
      "Step: 11530, train/loss: 0.0\n",
      "Step: 11530, train/grad_norm: 1.0252165782986822e-10\n",
      "Step: 11530, train/learning_rate: 4.2678088902903255e-06\n",
      "Step: 11530, train/epoch: 2.743931531906128\n",
      "Step: 11540, train/loss: 0.0\n",
      "Step: 11540, train/grad_norm: 3.035454665401005e-10\n",
      "Step: 11540, train/learning_rate: 4.228145371598657e-06\n",
      "Step: 11540, train/epoch: 2.7463111877441406\n",
      "Step: 11550, train/loss: 0.0\n",
      "Step: 11550, train/grad_norm: 1.020648635052801e-10\n",
      "Step: 11550, train/learning_rate: 4.188481852906989e-06\n",
      "Step: 11550, train/epoch: 2.7486910820007324\n",
      "Step: 11560, train/loss: 0.0\n",
      "Step: 11560, train/grad_norm: 1.950137534095786e-12\n",
      "Step: 11560, train/learning_rate: 4.14881787946797e-06\n",
      "Step: 11560, train/epoch: 2.751070976257324\n",
      "Step: 11570, train/loss: 0.0\n",
      "Step: 11570, train/grad_norm: 2.408168803258448e-11\n",
      "Step: 11570, train/learning_rate: 4.1091543607763015e-06\n",
      "Step: 11570, train/epoch: 2.753450632095337\n",
      "Step: 11580, train/loss: 0.0\n",
      "Step: 11580, train/grad_norm: 4.697425115551823e-09\n",
      "Step: 11580, train/learning_rate: 4.069490842084633e-06\n",
      "Step: 11580, train/epoch: 2.7558305263519287\n",
      "Step: 11590, train/loss: 0.0\n",
      "Step: 11590, train/grad_norm: 3.875986198664805e-10\n",
      "Step: 11590, train/learning_rate: 4.029826868645614e-06\n",
      "Step: 11590, train/epoch: 2.7582104206085205\n",
      "Step: 11600, train/loss: 0.0\n",
      "Step: 11600, train/grad_norm: 8.504809634324317e-10\n",
      "Step: 11600, train/learning_rate: 3.990163349953946e-06\n",
      "Step: 11600, train/epoch: 2.760590076446533\n",
      "Step: 11610, train/loss: 0.0\n",
      "Step: 11610, train/grad_norm: 1.9629232267392993e-11\n",
      "Step: 11610, train/learning_rate: 3.9504998312622774e-06\n",
      "Step: 11610, train/epoch: 2.762969970703125\n",
      "Step: 11620, train/loss: 0.0\n",
      "Step: 11620, train/grad_norm: 5.158847216080176e-08\n",
      "Step: 11620, train/learning_rate: 3.910836312570609e-06\n",
      "Step: 11620, train/epoch: 2.765349864959717\n",
      "Step: 11630, train/loss: 0.0\n",
      "Step: 11630, train/grad_norm: 2.929299598025681e-13\n",
      "Step: 11630, train/learning_rate: 3.87117233913159e-06\n",
      "Step: 11630, train/epoch: 2.7677297592163086\n",
      "Step: 11640, train/loss: 0.0\n",
      "Step: 11640, train/grad_norm: 4.575117618088598e-09\n",
      "Step: 11640, train/learning_rate: 3.831508820439922e-06\n",
      "Step: 11640, train/epoch: 2.7701094150543213\n",
      "Step: 11650, train/loss: 0.0\n",
      "Step: 11650, train/grad_norm: 1.3118372707565662e-11\n",
      "Step: 11650, train/learning_rate: 3.791845074374578e-06\n",
      "Step: 11650, train/epoch: 2.772489309310913\n",
      "Step: 11660, train/loss: 0.0\n",
      "Step: 11660, train/grad_norm: 1.7900201321197073e-09\n",
      "Step: 11660, train/learning_rate: 3.7521815556829097e-06\n",
      "Step: 11660, train/epoch: 2.774869203567505\n",
      "Step: 11670, train/loss: 0.0\n",
      "Step: 11670, train/grad_norm: 3.0234645342908095e-10\n",
      "Step: 11670, train/learning_rate: 3.712517809617566e-06\n",
      "Step: 11670, train/epoch: 2.7772488594055176\n",
      "Step: 11680, train/loss: 0.0\n",
      "Step: 11680, train/grad_norm: 6.220959858360686e-11\n",
      "Step: 11680, train/learning_rate: 3.6728542909258977e-06\n",
      "Step: 11680, train/epoch: 2.7796287536621094\n",
      "Step: 11690, train/loss: 0.0\n",
      "Step: 11690, train/grad_norm: 5.272033032355239e-09\n",
      "Step: 11690, train/learning_rate: 3.633190544860554e-06\n",
      "Step: 11690, train/epoch: 2.782008647918701\n",
      "Step: 11700, train/loss: 0.0\n",
      "Step: 11700, train/grad_norm: 3.538214372097309e-13\n",
      "Step: 11700, train/learning_rate: 3.59352679879521e-06\n",
      "Step: 11700, train/epoch: 2.784388303756714\n",
      "Step: 11710, train/loss: 0.0\n",
      "Step: 11710, train/grad_norm: 9.990866640940954e-12\n",
      "Step: 11710, train/learning_rate: 3.553863280103542e-06\n",
      "Step: 11710, train/epoch: 2.7867681980133057\n",
      "Step: 11720, train/loss: 0.0\n",
      "Step: 11720, train/grad_norm: 5.322215912428874e-08\n",
      "Step: 11720, train/learning_rate: 3.514199534038198e-06\n",
      "Step: 11720, train/epoch: 2.7891480922698975\n",
      "Step: 11730, train/loss: 0.0\n",
      "Step: 11730, train/grad_norm: 6.905935513116385e-14\n",
      "Step: 11730, train/learning_rate: 3.47453601534653e-06\n",
      "Step: 11730, train/epoch: 2.79152774810791\n",
      "Step: 11740, train/loss: 0.0\n",
      "Step: 11740, train/grad_norm: 1.3367823612497887e-06\n",
      "Step: 11740, train/learning_rate: 3.434872269281186e-06\n",
      "Step: 11740, train/epoch: 2.793907642364502\n",
      "Step: 11750, train/loss: 0.0\n",
      "Step: 11750, train/grad_norm: 7.75393367135474e-13\n",
      "Step: 11750, train/learning_rate: 3.3952085232158424e-06\n",
      "Step: 11750, train/epoch: 2.7962875366210938\n",
      "Step: 11760, train/loss: 0.0\n",
      "Step: 11760, train/grad_norm: 3.981173724021403e-10\n",
      "Step: 11760, train/learning_rate: 3.355545004524174e-06\n",
      "Step: 11760, train/epoch: 2.7986671924591064\n",
      "Step: 11770, train/loss: 0.0\n",
      "Step: 11770, train/grad_norm: 2.0314332438431393e-09\n",
      "Step: 11770, train/learning_rate: 3.3158812584588304e-06\n",
      "Step: 11770, train/epoch: 2.8010470867156982\n",
      "Step: 11780, train/loss: 0.0\n",
      "Step: 11780, train/grad_norm: 1.099881145362469e-12\n",
      "Step: 11780, train/learning_rate: 3.276217739767162e-06\n",
      "Step: 11780, train/epoch: 2.80342698097229\n",
      "Step: 11790, train/loss: 0.0\n",
      "Step: 11790, train/grad_norm: 1.0287442425704896e-10\n",
      "Step: 11790, train/learning_rate: 3.2365539937018184e-06\n",
      "Step: 11790, train/epoch: 2.805806875228882\n",
      "Step: 11800, train/loss: 0.0\n",
      "Step: 11800, train/grad_norm: 3.1080013562778674e-10\n",
      "Step: 11800, train/learning_rate: 3.19689047501015e-06\n",
      "Step: 11800, train/epoch: 2.8081865310668945\n",
      "Step: 11810, train/loss: 0.0\n",
      "Step: 11810, train/grad_norm: 9.014532231788053e-09\n",
      "Step: 11810, train/learning_rate: 3.1572267289448064e-06\n",
      "Step: 11810, train/epoch: 2.8105664253234863\n",
      "Step: 11820, train/loss: 0.0\n",
      "Step: 11820, train/grad_norm: 1.2672463989629734e-12\n",
      "Step: 11820, train/learning_rate: 3.1175629828794627e-06\n",
      "Step: 11820, train/epoch: 2.812946319580078\n",
      "Step: 11830, train/loss: 0.0\n",
      "Step: 11830, train/grad_norm: 1.0858455425477587e-05\n",
      "Step: 11830, train/learning_rate: 3.0778994641877944e-06\n",
      "Step: 11830, train/epoch: 2.815325975418091\n",
      "Step: 11840, train/loss: 0.0\n",
      "Step: 11840, train/grad_norm: 1.336846011668058e-08\n",
      "Step: 11840, train/learning_rate: 3.0382357181224506e-06\n",
      "Step: 11840, train/epoch: 2.8177058696746826\n",
      "Step: 11850, train/loss: 0.0\n",
      "Step: 11850, train/grad_norm: 3.211550563264609e-07\n",
      "Step: 11850, train/learning_rate: 2.9985721994307823e-06\n",
      "Step: 11850, train/epoch: 2.8200857639312744\n",
      "Step: 11860, train/loss: 0.0\n",
      "Step: 11860, train/grad_norm: 1.6905343791506766e-09\n",
      "Step: 11860, train/learning_rate: 2.9589084533654386e-06\n",
      "Step: 11860, train/epoch: 2.822465419769287\n",
      "Step: 11870, train/loss: 0.0\n",
      "Step: 11870, train/grad_norm: 1.8502998244684932e-08\n",
      "Step: 11870, train/learning_rate: 2.919244707300095e-06\n",
      "Step: 11870, train/epoch: 2.824845314025879\n",
      "Step: 11880, train/loss: 0.0\n",
      "Step: 11880, train/grad_norm: 1.4925352065375819e-09\n",
      "Step: 11880, train/learning_rate: 2.8795811886084266e-06\n",
      "Step: 11880, train/epoch: 2.8272252082824707\n",
      "Step: 11890, train/loss: 0.0\n",
      "Step: 11890, train/grad_norm: 2.1101316249882984e-08\n",
      "Step: 11890, train/learning_rate: 2.839917442543083e-06\n",
      "Step: 11890, train/epoch: 2.8296048641204834\n",
      "Step: 11900, train/loss: 0.0\n",
      "Step: 11900, train/grad_norm: 9.40271645266666e-12\n",
      "Step: 11900, train/learning_rate: 2.8002539238514146e-06\n",
      "Step: 11900, train/epoch: 2.831984758377075\n",
      "Step: 11910, train/loss: 0.0\n",
      "Step: 11910, train/grad_norm: 1.1215373248163463e-11\n",
      "Step: 11910, train/learning_rate: 2.760590177786071e-06\n",
      "Step: 11910, train/epoch: 2.834364652633667\n",
      "Step: 11920, train/loss: 0.0\n",
      "Step: 11920, train/grad_norm: 2.370051637584636e-12\n",
      "Step: 11920, train/learning_rate: 2.720926431720727e-06\n",
      "Step: 11920, train/epoch: 2.8367443084716797\n",
      "Step: 11930, train/loss: 0.0\n",
      "Step: 11930, train/grad_norm: 1.0194690008629922e-13\n",
      "Step: 11930, train/learning_rate: 2.681262913029059e-06\n",
      "Step: 11930, train/epoch: 2.8391242027282715\n",
      "Step: 11940, train/loss: 0.0\n",
      "Step: 11940, train/grad_norm: 1.0108786030471606e-09\n",
      "Step: 11940, train/learning_rate: 2.641599166963715e-06\n",
      "Step: 11940, train/epoch: 2.8415040969848633\n",
      "Step: 11950, train/loss: 0.0\n",
      "Step: 11950, train/grad_norm: 2.3895356179859384e-11\n",
      "Step: 11950, train/learning_rate: 2.601935648272047e-06\n",
      "Step: 11950, train/epoch: 2.843883752822876\n",
      "Step: 11960, train/loss: 0.0\n",
      "Step: 11960, train/grad_norm: 2.5035942069484562e-11\n",
      "Step: 11960, train/learning_rate: 2.562271902206703e-06\n",
      "Step: 11960, train/epoch: 2.8462636470794678\n",
      "Step: 11970, train/loss: 0.0\n",
      "Step: 11970, train/grad_norm: 2.395160621487119e-13\n",
      "Step: 11970, train/learning_rate: 2.522608383515035e-06\n",
      "Step: 11970, train/epoch: 2.8486435413360596\n",
      "Step: 11980, train/loss: 0.0\n",
      "Step: 11980, train/grad_norm: 7.437496798989862e-13\n",
      "Step: 11980, train/learning_rate: 2.482944637449691e-06\n",
      "Step: 11980, train/epoch: 2.8510234355926514\n",
      "Step: 11990, train/loss: 0.0\n",
      "Step: 11990, train/grad_norm: 5.128005225313315e-13\n",
      "Step: 11990, train/learning_rate: 2.4432808913843473e-06\n",
      "Step: 11990, train/epoch: 2.853403091430664\n",
      "Step: 12000, train/loss: 0.0\n",
      "Step: 12000, train/grad_norm: 3.286041874162038e-07\n",
      "Step: 12000, train/learning_rate: 2.403617372692679e-06\n",
      "Step: 12000, train/epoch: 2.855782985687256\n",
      "Step: 12010, train/loss: 0.0\n",
      "Step: 12010, train/grad_norm: 9.816326489728647e-14\n",
      "Step: 12010, train/learning_rate: 2.3639536266273353e-06\n",
      "Step: 12010, train/epoch: 2.8581628799438477\n",
      "Step: 12020, train/loss: 0.0\n",
      "Step: 12020, train/grad_norm: 2.178777449256264e-13\n",
      "Step: 12020, train/learning_rate: 2.324290107935667e-06\n",
      "Step: 12020, train/epoch: 2.8605425357818604\n",
      "Step: 12030, train/loss: 0.0\n",
      "Step: 12030, train/grad_norm: 3.033123737210673e-17\n",
      "Step: 12030, train/learning_rate: 2.2846263618703233e-06\n",
      "Step: 12030, train/epoch: 2.862922430038452\n",
      "Step: 12040, train/loss: 0.0\n",
      "Step: 12040, train/grad_norm: 3.160612301934297e-14\n",
      "Step: 12040, train/learning_rate: 2.2449626158049796e-06\n",
      "Step: 12040, train/epoch: 2.865302324295044\n",
      "Step: 12050, train/loss: 0.0\n",
      "Step: 12050, train/grad_norm: 1.8221757436265307e-10\n",
      "Step: 12050, train/learning_rate: 2.2052990971133113e-06\n",
      "Step: 12050, train/epoch: 2.8676819801330566\n",
      "Step: 12060, train/loss: 0.0\n",
      "Step: 12060, train/grad_norm: 6.0852386497833866e-12\n",
      "Step: 12060, train/learning_rate: 2.1656353510479676e-06\n",
      "Step: 12060, train/epoch: 2.8700618743896484\n",
      "Step: 12070, train/loss: 0.0\n",
      "Step: 12070, train/grad_norm: 6.609359104459145e-08\n",
      "Step: 12070, train/learning_rate: 2.1259718323562993e-06\n",
      "Step: 12070, train/epoch: 2.8724417686462402\n",
      "Step: 12080, train/loss: 0.0\n",
      "Step: 12080, train/grad_norm: 1.7249651707018643e-10\n",
      "Step: 12080, train/learning_rate: 2.0863080862909555e-06\n",
      "Step: 12080, train/epoch: 2.874821424484253\n",
      "Step: 12090, train/loss: 0.0\n",
      "Step: 12090, train/grad_norm: 3.5992649832783385e-11\n",
      "Step: 12090, train/learning_rate: 2.0466445675992873e-06\n",
      "Step: 12090, train/epoch: 2.8772013187408447\n",
      "Step: 12100, train/loss: 0.0\n",
      "Step: 12100, train/grad_norm: 1.8508239645748104e-12\n",
      "Step: 12100, train/learning_rate: 2.0069808215339435e-06\n",
      "Step: 12100, train/epoch: 2.8795812129974365\n",
      "Step: 12110, train/loss: 0.0\n",
      "Step: 12110, train/grad_norm: 5.965470895041847e-10\n",
      "Step: 12110, train/learning_rate: 1.9673170754686e-06\n",
      "Step: 12110, train/epoch: 2.881960868835449\n",
      "Step: 12120, train/loss: 0.0\n",
      "Step: 12120, train/grad_norm: 1.6138500041548709e-12\n",
      "Step: 12120, train/learning_rate: 1.9276535567769315e-06\n",
      "Step: 12120, train/epoch: 2.884340763092041\n",
      "Step: 12130, train/loss: 0.0\n",
      "Step: 12130, train/grad_norm: 6.1926632710651575e-09\n",
      "Step: 12130, train/learning_rate: 1.8879898107115878e-06\n",
      "Step: 12130, train/epoch: 2.886720657348633\n",
      "Step: 12140, train/loss: 0.0\n",
      "Step: 12140, train/grad_norm: 2.690113243275505e-09\n",
      "Step: 12140, train/learning_rate: 1.8483261783330818e-06\n",
      "Step: 12140, train/epoch: 2.8891003131866455\n",
      "Step: 12150, train/loss: 0.0\n",
      "Step: 12150, train/grad_norm: 3.833047594603567e-12\n",
      "Step: 12150, train/learning_rate: 1.8086625459545758e-06\n",
      "Step: 12150, train/epoch: 2.8914802074432373\n",
      "Step: 12160, train/loss: 0.0\n",
      "Step: 12160, train/grad_norm: 5.888867139151183e-14\n",
      "Step: 12160, train/learning_rate: 1.7689989135760698e-06\n",
      "Step: 12160, train/epoch: 2.893860101699829\n",
      "Step: 12170, train/loss: 0.0\n",
      "Step: 12170, train/grad_norm: 3.63810176429169e-10\n",
      "Step: 12170, train/learning_rate: 1.7293352811975637e-06\n",
      "Step: 12170, train/epoch: 2.896239995956421\n",
      "Step: 12180, train/loss: 0.0\n",
      "Step: 12180, train/grad_norm: 2.3855296535889303e-13\n",
      "Step: 12180, train/learning_rate: 1.68967153513222e-06\n",
      "Step: 12180, train/epoch: 2.8986196517944336\n",
      "Step: 12190, train/loss: 0.0\n",
      "Step: 12190, train/grad_norm: 1.0977201370110379e-08\n",
      "Step: 12190, train/learning_rate: 1.650007902753714e-06\n",
      "Step: 12190, train/epoch: 2.9009995460510254\n",
      "Step: 12200, train/loss: 0.0\n",
      "Step: 12200, train/grad_norm: 1.581713072692538e-14\n",
      "Step: 12200, train/learning_rate: 1.610344270375208e-06\n",
      "Step: 12200, train/epoch: 2.903379440307617\n",
      "Step: 12210, train/loss: 0.0\n",
      "Step: 12210, train/grad_norm: 6.895029400766362e-07\n",
      "Step: 12210, train/learning_rate: 1.570680637996702e-06\n",
      "Step: 12210, train/epoch: 2.90575909614563\n",
      "Step: 12220, train/loss: 0.0\n",
      "Step: 12220, train/grad_norm: 7.482741182229802e-08\n",
      "Step: 12220, train/learning_rate: 1.531017005618196e-06\n",
      "Step: 12220, train/epoch: 2.9081389904022217\n",
      "Step: 12230, train/loss: 0.0\n",
      "Step: 12230, train/grad_norm: 0.0028646427672356367\n",
      "Step: 12230, train/learning_rate: 1.49135337323969e-06\n",
      "Step: 12230, train/epoch: 2.9105188846588135\n",
      "Step: 12240, train/loss: 0.0\n",
      "Step: 12240, train/grad_norm: 3.3282212164205405e-13\n",
      "Step: 12240, train/learning_rate: 1.4516896271743462e-06\n",
      "Step: 12240, train/epoch: 2.912898540496826\n",
      "Step: 12250, train/loss: 0.0\n",
      "Step: 12250, train/grad_norm: 3.162265588274593e-11\n",
      "Step: 12250, train/learning_rate: 1.4120259947958402e-06\n",
      "Step: 12250, train/epoch: 2.915278434753418\n",
      "Step: 12260, train/loss: 0.0\n",
      "Step: 12260, train/grad_norm: 5.021066940003038e-08\n",
      "Step: 12260, train/learning_rate: 1.3723623624173342e-06\n",
      "Step: 12260, train/epoch: 2.9176583290100098\n",
      "Step: 12270, train/loss: 0.0\n",
      "Step: 12270, train/grad_norm: 3.6613831411180797e-10\n",
      "Step: 12270, train/learning_rate: 1.3326987300388282e-06\n",
      "Step: 12270, train/epoch: 2.9200379848480225\n",
      "Step: 12280, train/loss: 0.0\n",
      "Step: 12280, train/grad_norm: 2.6162013527786243e-11\n",
      "Step: 12280, train/learning_rate: 1.2930350976603222e-06\n",
      "Step: 12280, train/epoch: 2.9224178791046143\n",
      "Step: 12290, train/loss: 0.0\n",
      "Step: 12290, train/grad_norm: 1.9073551332099425e-12\n",
      "Step: 12290, train/learning_rate: 1.2533714652818162e-06\n",
      "Step: 12290, train/epoch: 2.924797773361206\n",
      "Step: 12300, train/loss: 0.0\n",
      "Step: 12300, train/grad_norm: 1.551825107514064e-16\n",
      "Step: 12300, train/learning_rate: 1.2137077192164725e-06\n",
      "Step: 12300, train/epoch: 2.9271774291992188\n",
      "Step: 12310, train/loss: 0.0\n",
      "Step: 12310, train/grad_norm: 6.359648051235123e-12\n",
      "Step: 12310, train/learning_rate: 1.1740440868379665e-06\n",
      "Step: 12310, train/epoch: 2.9295573234558105\n",
      "Step: 12320, train/loss: 0.0\n",
      "Step: 12320, train/grad_norm: 5.109166423267197e-10\n",
      "Step: 12320, train/learning_rate: 1.1343804544594605e-06\n",
      "Step: 12320, train/epoch: 2.9319372177124023\n",
      "Step: 12330, train/loss: 0.0\n",
      "Step: 12330, train/grad_norm: 1.5685176681795587e-12\n",
      "Step: 12330, train/learning_rate: 1.0947168220809544e-06\n",
      "Step: 12330, train/epoch: 2.934316873550415\n",
      "Step: 12340, train/loss: 0.0\n",
      "Step: 12340, train/grad_norm: 3.1618899498653263e-08\n",
      "Step: 12340, train/learning_rate: 1.0550531897024484e-06\n",
      "Step: 12340, train/epoch: 2.936696767807007\n",
      "Step: 12350, train/loss: 0.0\n",
      "Step: 12350, train/grad_norm: 1.1845822989187127e-08\n",
      "Step: 12350, train/learning_rate: 1.0153894436371047e-06\n",
      "Step: 12350, train/epoch: 2.9390766620635986\n",
      "Step: 12360, train/loss: 0.0\n",
      "Step: 12360, train/grad_norm: 4.2295344310216976e-11\n",
      "Step: 12360, train/learning_rate: 9.757258112585987e-07\n",
      "Step: 12360, train/epoch: 2.9414565563201904\n",
      "Step: 12370, train/loss: 0.0\n",
      "Step: 12370, train/grad_norm: 1.0667241440387443e-05\n",
      "Step: 12370, train/learning_rate: 9.360621788800927e-07\n",
      "Step: 12370, train/epoch: 2.943836212158203\n",
      "Step: 12380, train/loss: 0.0\n",
      "Step: 12380, train/grad_norm: 5.119573859356345e-15\n",
      "Step: 12380, train/learning_rate: 8.963985465015867e-07\n",
      "Step: 12380, train/epoch: 2.946216106414795\n",
      "Step: 12390, train/loss: 0.0\n",
      "Step: 12390, train/grad_norm: 7.279248137592731e-08\n",
      "Step: 12390, train/learning_rate: 8.567349141230807e-07\n",
      "Step: 12390, train/epoch: 2.9485960006713867\n",
      "Step: 12400, train/loss: 0.0\n",
      "Step: 12400, train/grad_norm: 2.51528442171689e-12\n",
      "Step: 12400, train/learning_rate: 8.170712249011558e-07\n",
      "Step: 12400, train/epoch: 2.9509756565093994\n",
      "Step: 12410, train/loss: 0.0\n",
      "Step: 12410, train/grad_norm: 3.0187550376092886e-11\n",
      "Step: 12410, train/learning_rate: 7.774075925226498e-07\n",
      "Step: 12410, train/epoch: 2.953355550765991\n",
      "Step: 12420, train/loss: 0.0\n",
      "Step: 12420, train/grad_norm: 9.362302222371e-08\n",
      "Step: 12420, train/learning_rate: 7.377439033007249e-07\n",
      "Step: 12420, train/epoch: 2.955735445022583\n",
      "Step: 12430, train/loss: 0.0\n",
      "Step: 12430, train/grad_norm: 1.6013414949611615e-07\n",
      "Step: 12430, train/learning_rate: 6.980802709222189e-07\n",
      "Step: 12430, train/epoch: 2.9581151008605957\n",
      "Step: 12440, train/loss: 0.0\n",
      "Step: 12440, train/grad_norm: 7.636518709595343e-15\n",
      "Step: 12440, train/learning_rate: 6.584166385437129e-07\n",
      "Step: 12440, train/epoch: 2.9604949951171875\n",
      "Step: 12450, train/loss: 0.0\n",
      "Step: 12450, train/grad_norm: 6.011290909491152e-10\n",
      "Step: 12450, train/learning_rate: 6.18752949321788e-07\n",
      "Step: 12450, train/epoch: 2.9628748893737793\n",
      "Step: 12460, train/loss: 0.0\n",
      "Step: 12460, train/grad_norm: 7.854088835301809e-06\n",
      "Step: 12460, train/learning_rate: 5.79089316943282e-07\n",
      "Step: 12460, train/epoch: 2.965254545211792\n",
      "Step: 12470, train/loss: 0.0\n",
      "Step: 12470, train/grad_norm: 1.174057961961239e-09\n",
      "Step: 12470, train/learning_rate: 5.39425684564776e-07\n",
      "Step: 12470, train/epoch: 2.967634439468384\n",
      "Step: 12480, train/loss: 0.0\n",
      "Step: 12480, train/grad_norm: 8.935822188530645e-14\n",
      "Step: 12480, train/learning_rate: 4.997619953428512e-07\n",
      "Step: 12480, train/epoch: 2.9700143337249756\n",
      "Step: 12490, train/loss: 0.0\n",
      "Step: 12490, train/grad_norm: 1.4883729556984526e-13\n",
      "Step: 12490, train/learning_rate: 4.6009836296434514e-07\n",
      "Step: 12490, train/epoch: 2.9723939895629883\n",
      "Step: 12500, train/loss: 0.0\n",
      "Step: 12500, train/grad_norm: 1.75094966303746e-10\n",
      "Step: 12500, train/learning_rate: 4.204347021641297e-07\n",
      "Step: 12500, train/epoch: 2.97477388381958\n",
      "Step: 12510, train/loss: 0.0\n",
      "Step: 12510, train/grad_norm: 1.0152487574591196e-12\n",
      "Step: 12510, train/learning_rate: 3.807710697856237e-07\n",
      "Step: 12510, train/epoch: 2.977153778076172\n",
      "Step: 12520, train/loss: 0.0\n",
      "Step: 12520, train/grad_norm: 6.185673067510278e-12\n",
      "Step: 12520, train/learning_rate: 3.4110740898540826e-07\n",
      "Step: 12520, train/epoch: 2.9795336723327637\n",
      "Step: 12530, train/loss: 0.0\n",
      "Step: 12530, train/grad_norm: 2.724277159416033e-08\n",
      "Step: 12530, train/learning_rate: 3.014437481851928e-07\n",
      "Step: 12530, train/epoch: 2.9819133281707764\n",
      "Step: 12540, train/loss: 0.0\n",
      "Step: 12540, train/grad_norm: 1.1691428269955395e-08\n",
      "Step: 12540, train/learning_rate: 2.617801158066868e-07\n",
      "Step: 12540, train/epoch: 2.984293222427368\n",
      "Step: 12550, train/loss: 0.0\n",
      "Step: 12550, train/grad_norm: 2.7310957764575536e-17\n",
      "Step: 12550, train/learning_rate: 2.2211645500647137e-07\n",
      "Step: 12550, train/epoch: 2.98667311668396\n",
      "Step: 12560, train/loss: 0.0\n",
      "Step: 12560, train/grad_norm: 4.1412540952023846e-08\n",
      "Step: 12560, train/learning_rate: 1.8245279420625593e-07\n",
      "Step: 12560, train/epoch: 2.9890527725219727\n",
      "Step: 12570, train/loss: 0.0\n",
      "Step: 12570, train/grad_norm: 4.17725541657088e-11\n",
      "Step: 12570, train/learning_rate: 1.427891476168952e-07\n",
      "Step: 12570, train/epoch: 2.9914326667785645\n",
      "Step: 12580, train/loss: 0.0\n",
      "Step: 12580, train/grad_norm: 2.6840068524918576e-14\n",
      "Step: 12580, train/learning_rate: 1.0312549392210713e-07\n",
      "Step: 12580, train/epoch: 2.9938125610351562\n",
      "Step: 12590, train/loss: 0.0\n",
      "Step: 12590, train/grad_norm: 9.64504192658977e-13\n",
      "Step: 12590, train/learning_rate: 6.346184022731904e-08\n",
      "Step: 12590, train/epoch: 2.996192216873169\n",
      "Step: 12600, train/loss: 0.0\n",
      "Step: 12600, train/grad_norm: 4.110644810650932e-11\n",
      "Step: 12600, train/learning_rate: 2.379819186160148e-08\n",
      "Step: 12600, train/epoch: 2.9985721111297607\n",
      "Step: 12606, eval/loss: 0.017047898843884468\n",
      "Step: 12606, eval/accuracy: 0.9993058443069458\n",
      "Step: 12606, eval/f1: 0.9992671012878418\n",
      "Step: 12606, eval/runtime: 6242.546875\n",
      "Step: 12606, eval/samples_per_second: 1.1540000438690186\n",
      "Step: 12606, eval/steps_per_second: 0.14399999380111694\n",
      "Step: 12606, train/epoch: 3.0\n",
      "Step: 12606, train/train_runtime: 280508.78125\n",
      "Step: 12606, train/train_samples_per_second: 0.35899999737739563\n",
      "Step: 12606, train/train_steps_per_second: 0.04500000178813934\n",
      "Step: 12606, train/total_flos: 4.344210903370878e+19\n",
      "Step: 12606, train/train_loss: 0.021112242713570595\n",
      "Step: 12606, train/epoch: 3.0\n",
      "Reading events from file: ./praxis-Mixtral-8x22B-v0.1-small-finetune/logs/events.out.tfevents.1717616716.hephaestus.5848.0\n",
      "Step: 10, train/loss: 2.1201999187469482\n",
      "Step: 10, train/grad_norm: 81.73965454101562\n",
      "Step: 10, train/learning_rate: 4.9960337491938844e-05\n",
      "Step: 10, train/epoch: 0.0023798190522938967\n",
      "Step: 20, train/loss: 1.8639999628067017\n",
      "Step: 20, train/grad_norm: 87.46385192871094\n",
      "Step: 20, train/learning_rate: 4.992067260900512e-05\n",
      "Step: 20, train/epoch: 0.004759638104587793\n",
      "Step: 30, train/loss: 1.1872999668121338\n",
      "Step: 30, train/grad_norm: 72.0192642211914\n",
      "Step: 30, train/learning_rate: 4.98810077260714e-05\n",
      "Step: 30, train/epoch: 0.007139457389712334\n",
      "Step: 40, train/loss: 0.9312999844551086\n",
      "Step: 40, train/grad_norm: 107.25865936279297\n",
      "Step: 40, train/learning_rate: 4.984134648111649e-05\n",
      "Step: 40, train/epoch: 0.009519276209175587\n",
      "Step: 50, train/loss: 0.1080000028014183\n",
      "Step: 50, train/grad_norm: 0.01720575988292694\n",
      "Step: 50, train/learning_rate: 4.980168159818277e-05\n",
      "Step: 50, train/epoch: 0.011899095959961414\n",
      "Step: 60, train/loss: 0.023000000044703484\n",
      "Step: 60, train/grad_norm: 0.04401233047246933\n",
      "Step: 60, train/learning_rate: 4.976201671524905e-05\n",
      "Step: 60, train/epoch: 0.014278914779424667\n",
      "Step: 70, train/loss: 9.999999747378752e-05\n",
      "Step: 70, train/grad_norm: 0.01658078283071518\n",
      "Step: 70, train/learning_rate: 4.972235547029413e-05\n",
      "Step: 70, train/epoch: 0.016658734530210495\n",
      "Step: 80, train/loss: 0.013299999758601189\n",
      "Step: 80, train/grad_norm: 4.0640887164045125e-05\n",
      "Step: 80, train/learning_rate: 4.968269058736041e-05\n",
      "Step: 80, train/epoch: 0.019038552418351173\n",
      "Step: 90, train/loss: 0.025800000876188278\n",
      "Step: 90, train/grad_norm: 1.0881669965101537e-07\n",
      "Step: 90, train/learning_rate: 4.964302570442669e-05\n",
      "Step: 90, train/epoch: 0.021418372169137\n",
      "Step: 100, train/loss: 0.0\n",
      "Step: 100, train/grad_norm: 7.60898774387897e-06\n",
      "Step: 100, train/learning_rate: 4.960336445947178e-05\n",
      "Step: 100, train/epoch: 0.02379819191992283\n",
      "Step: 110, train/loss: 0.0\n",
      "Step: 110, train/grad_norm: 7.703050869167782e-06\n",
      "Step: 110, train/learning_rate: 4.9563699576538056e-05\n",
      "Step: 110, train/epoch: 0.026178009808063507\n",
      "Step: 120, train/loss: 0.3928000032901764\n",
      "Step: 120, train/grad_norm: 0.008612355217337608\n",
      "Step: 120, train/learning_rate: 4.9524034693604335e-05\n",
      "Step: 120, train/epoch: 0.028557829558849335\n",
      "Step: 130, train/loss: 0.0\n",
      "Step: 130, train/grad_norm: 0.01085876114666462\n",
      "Step: 130, train/learning_rate: 4.948437344864942e-05\n",
      "Step: 130, train/epoch: 0.030937649309635162\n",
      "Step: 140, train/loss: 0.1906999945640564\n",
      "Step: 140, train/grad_norm: 74.01075744628906\n",
      "Step: 140, train/learning_rate: 4.94447085657157e-05\n",
      "Step: 140, train/epoch: 0.03331746906042099\n",
      "Step: 150, train/loss: 0.00019999999494757503\n",
      "Step: 150, train/grad_norm: 0.0013046774547547102\n",
      "Step: 150, train/learning_rate: 4.940504368278198e-05\n",
      "Step: 150, train/epoch: 0.03569728881120682\n",
      "Step: 160, train/loss: 0.32100000977516174\n",
      "Step: 160, train/grad_norm: 97.29717254638672\n",
      "Step: 160, train/learning_rate: 4.9365382437827066e-05\n",
      "Step: 160, train/epoch: 0.03807710483670235\n",
      "Step: 170, train/loss: 0.36640000343322754\n",
      "Step: 170, train/grad_norm: 5.2378336334868436e-08\n",
      "Step: 170, train/learning_rate: 4.9325717554893345e-05\n",
      "Step: 170, train/epoch: 0.040456924587488174\n",
      "Step: 180, train/loss: 0.0\n",
      "Step: 180, train/grad_norm: 3.6848618037765846e-05\n",
      "Step: 180, train/learning_rate: 4.9286052671959624e-05\n",
      "Step: 180, train/epoch: 0.042836744338274\n",
      "Step: 190, train/loss: 0.21369999647140503\n",
      "Step: 190, train/grad_norm: 0.006074149161577225\n",
      "Step: 190, train/learning_rate: 4.924639142700471e-05\n",
      "Step: 190, train/epoch: 0.04521656408905983\n",
      "Step: 200, train/loss: 0.0\n",
      "Step: 200, train/grad_norm: 8.028808906601625e-08\n",
      "Step: 200, train/learning_rate: 4.920672654407099e-05\n",
      "Step: 200, train/epoch: 0.04759638383984566\n",
      "Step: 210, train/loss: 0.0\n",
      "Step: 210, train/grad_norm: 1.808070192055311e-05\n",
      "Step: 210, train/learning_rate: 4.916706166113727e-05\n",
      "Step: 210, train/epoch: 0.049976203590631485\n",
      "Step: 220, train/loss: 0.0\n",
      "Step: 220, train/grad_norm: 0.023240460082888603\n",
      "Step: 220, train/learning_rate: 4.9127400416182354e-05\n",
      "Step: 220, train/epoch: 0.052356019616127014\n",
      "Step: 230, train/loss: 0.0\n",
      "Step: 230, train/grad_norm: 2.6428151613799855e-06\n",
      "Step: 230, train/learning_rate: 4.908773553324863e-05\n",
      "Step: 230, train/epoch: 0.05473583936691284\n",
      "Step: 240, train/loss: 0.1251000016927719\n",
      "Step: 240, train/grad_norm: 0.00019548108684830368\n",
      "Step: 240, train/learning_rate: 4.904807065031491e-05\n",
      "Step: 240, train/epoch: 0.05711565911769867\n",
      "Step: 250, train/loss: 0.06840000301599503\n",
      "Step: 250, train/grad_norm: 2.5601084416848607e-06\n",
      "Step: 250, train/learning_rate: 4.900840940536e-05\n",
      "Step: 250, train/epoch: 0.0594954788684845\n",
      "Step: 260, train/loss: 0.0\n",
      "Step: 260, train/grad_norm: 0.010858699679374695\n",
      "Step: 260, train/learning_rate: 4.896874452242628e-05\n",
      "Step: 260, train/epoch: 0.061875298619270325\n",
      "Step: 270, train/loss: 0.0\n",
      "Step: 270, train/grad_norm: 2.926712205919557e-10\n",
      "Step: 270, train/learning_rate: 4.8929079639492556e-05\n",
      "Step: 270, train/epoch: 0.06425511837005615\n",
      "Step: 280, train/loss: 0.0\n",
      "Step: 280, train/grad_norm: 6.050034073723509e-08\n",
      "Step: 280, train/learning_rate: 4.888941839453764e-05\n",
      "Step: 280, train/epoch: 0.06663493812084198\n",
      "Step: 290, train/loss: 0.0\n",
      "Step: 290, train/grad_norm: 5.244079042537875e-12\n",
      "Step: 290, train/learning_rate: 4.884975351160392e-05\n",
      "Step: 290, train/epoch: 0.06901475787162781\n",
      "Step: 300, train/loss: 0.0\n",
      "Step: 300, train/grad_norm: 1.3836494616725759e-08\n",
      "Step: 300, train/learning_rate: 4.88100886286702e-05\n",
      "Step: 300, train/epoch: 0.07139457762241364\n",
      "Step: 310, train/loss: 0.06629999727010727\n",
      "Step: 310, train/grad_norm: 5.159794813636154e-10\n",
      "Step: 310, train/learning_rate: 4.877042738371529e-05\n",
      "Step: 310, train/epoch: 0.07377438992261887\n",
      "Step: 320, train/loss: 0.0\n",
      "Step: 320, train/grad_norm: 2.676288661171422e-12\n",
      "Step: 320, train/learning_rate: 4.8730762500781566e-05\n",
      "Step: 320, train/epoch: 0.0761542096734047\n",
      "Step: 330, train/loss: 0.0\n",
      "Step: 330, train/grad_norm: 7.746205760161828e-11\n",
      "Step: 330, train/learning_rate: 4.869110125582665e-05\n",
      "Step: 330, train/epoch: 0.07853402942419052\n",
      "Step: 340, train/loss: 0.20000000298023224\n",
      "Step: 340, train/grad_norm: 2.390392571383071e-11\n",
      "Step: 340, train/learning_rate: 4.865143637289293e-05\n",
      "Step: 340, train/epoch: 0.08091384917497635\n",
      "Step: 350, train/loss: 0.003000000026077032\n",
      "Step: 350, train/grad_norm: 7.4030609130859375\n",
      "Step: 350, train/learning_rate: 4.861177148995921e-05\n",
      "Step: 350, train/epoch: 0.08329366892576218\n",
      "Step: 360, train/loss: 0.0\n",
      "Step: 360, train/grad_norm: 2.037413651123643e-07\n",
      "Step: 360, train/learning_rate: 4.8572110245004296e-05\n",
      "Step: 360, train/epoch: 0.085673488676548\n",
      "Step: 370, train/loss: 0.0\n",
      "Step: 370, train/grad_norm: 3.2856679599291283e-09\n",
      "Step: 370, train/learning_rate: 4.8532445362070575e-05\n",
      "Step: 370, train/epoch: 0.08805330842733383\n",
      "Step: 380, train/loss: 0.23909999430179596\n",
      "Step: 380, train/grad_norm: 2.055492664965186e-08\n",
      "Step: 380, train/learning_rate: 4.8492780479136854e-05\n",
      "Step: 380, train/epoch: 0.09043312817811966\n",
      "Step: 390, train/loss: 0.7250000238418579\n",
      "Step: 390, train/grad_norm: 79.19331359863281\n",
      "Step: 390, train/learning_rate: 4.845311923418194e-05\n",
      "Step: 390, train/epoch: 0.09281294792890549\n",
      "Step: 400, train/loss: 0.5910000205039978\n",
      "Step: 400, train/grad_norm: 4.8691159463487566e-05\n",
      "Step: 400, train/learning_rate: 4.841345435124822e-05\n",
      "Step: 400, train/epoch: 0.09519276767969131\n",
      "Step: 410, train/loss: 0.028599999845027924\n",
      "Step: 410, train/grad_norm: 4.80417329526972e-05\n",
      "Step: 410, train/learning_rate: 4.83737894683145e-05\n",
      "Step: 410, train/epoch: 0.09757258743047714\n",
      "Step: 420, train/loss: 0.0\n",
      "Step: 420, train/grad_norm: 7.2812178508741e-09\n",
      "Step: 420, train/learning_rate: 4.8334128223359585e-05\n",
      "Step: 420, train/epoch: 0.09995240718126297\n",
      "Step: 430, train/loss: 0.0\n",
      "Step: 430, train/grad_norm: 1.104252279893192e-11\n",
      "Step: 430, train/learning_rate: 4.8294463340425864e-05\n",
      "Step: 430, train/epoch: 0.1023322194814682\n",
      "Step: 440, train/loss: 0.0\n",
      "Step: 440, train/grad_norm: 0.0007564510451629758\n",
      "Step: 440, train/learning_rate: 4.825479845749214e-05\n",
      "Step: 440, train/epoch: 0.10471203923225403\n",
      "Step: 450, train/loss: 0.0\n",
      "Step: 450, train/grad_norm: 0.0003233639872632921\n",
      "Step: 450, train/learning_rate: 4.821513721253723e-05\n",
      "Step: 450, train/epoch: 0.10709185898303986\n",
      "Step: 460, train/loss: 0.0\n",
      "Step: 460, train/grad_norm: 2.548412592062732e-09\n",
      "Step: 460, train/learning_rate: 4.817547232960351e-05\n",
      "Step: 460, train/epoch: 0.10947167873382568\n",
      "Step: 470, train/loss: 0.2874999940395355\n",
      "Step: 470, train/grad_norm: 2.6310646994431863e-09\n",
      "Step: 470, train/learning_rate: 4.813580744666979e-05\n",
      "Step: 470, train/epoch: 0.11185149848461151\n",
      "Step: 480, train/loss: 0.0\n",
      "Step: 480, train/grad_norm: 1.1724273463187274e-06\n",
      "Step: 480, train/learning_rate: 4.809614620171487e-05\n",
      "Step: 480, train/epoch: 0.11423131823539734\n",
      "Step: 490, train/loss: 0.0\n",
      "Step: 490, train/grad_norm: 1.8102195298297374e-09\n",
      "Step: 490, train/learning_rate: 4.805648131878115e-05\n",
      "Step: 490, train/epoch: 0.11661113798618317\n",
      "Step: 500, train/loss: 0.0\n",
      "Step: 500, train/grad_norm: 3.958259053860047e-09\n",
      "Step: 500, train/learning_rate: 4.801681643584743e-05\n",
      "Step: 500, train/epoch: 0.118990957736969\n",
      "Step: 510, train/loss: 0.003800000064074993\n",
      "Step: 510, train/grad_norm: 1.4381409951802482e-10\n",
      "Step: 510, train/learning_rate: 4.797715519089252e-05\n",
      "Step: 510, train/epoch: 0.12137077748775482\n",
      "Step: 520, train/loss: 0.0\n",
      "Step: 520, train/grad_norm: 1.0191614592258702e-06\n",
      "Step: 520, train/learning_rate: 4.79374903079588e-05\n",
      "Step: 520, train/epoch: 0.12375059723854065\n",
      "Step: 530, train/loss: 0.2921999990940094\n",
      "Step: 530, train/grad_norm: 1.4601407194447802e-08\n",
      "Step: 530, train/learning_rate: 4.7897825425025076e-05\n",
      "Step: 530, train/epoch: 0.12613041698932648\n",
      "Step: 540, train/loss: 0.0\n",
      "Step: 540, train/grad_norm: 4.7105687406290286e-11\n",
      "Step: 540, train/learning_rate: 4.785816418007016e-05\n",
      "Step: 540, train/epoch: 0.1285102367401123\n",
      "Step: 550, train/loss: 0.0\n",
      "Step: 550, train/grad_norm: 0.00024524173932150006\n",
      "Step: 550, train/learning_rate: 4.781849929713644e-05\n",
      "Step: 550, train/epoch: 0.13089005649089813\n",
      "Step: 560, train/loss: 0.1648000031709671\n",
      "Step: 560, train/grad_norm: 1.2639259239222156e-07\n",
      "Step: 560, train/learning_rate: 4.777883441420272e-05\n",
      "Step: 560, train/epoch: 0.13326987624168396\n",
      "Step: 570, train/loss: 0.00019999999494757503\n",
      "Step: 570, train/grad_norm: 0.010737513191998005\n",
      "Step: 570, train/learning_rate: 4.7739173169247806e-05\n",
      "Step: 570, train/epoch: 0.1356496959924698\n",
      "Step: 580, train/loss: 0.0\n",
      "Step: 580, train/grad_norm: 1.0251699933405689e-07\n",
      "Step: 580, train/learning_rate: 4.7699508286314085e-05\n",
      "Step: 580, train/epoch: 0.13802951574325562\n",
      "Step: 590, train/loss: 0.2485000044107437\n",
      "Step: 590, train/grad_norm: 1.0339681466575712e-05\n",
      "Step: 590, train/learning_rate: 4.7659843403380364e-05\n",
      "Step: 590, train/epoch: 0.14040933549404144\n",
      "Step: 600, train/loss: 0.0\n",
      "Step: 600, train/grad_norm: 0.088399738073349\n",
      "Step: 600, train/learning_rate: 4.762018215842545e-05\n",
      "Step: 600, train/epoch: 0.14278915524482727\n",
      "Step: 610, train/loss: 0.0\n",
      "Step: 610, train/grad_norm: 4.282224495000264e-07\n",
      "Step: 610, train/learning_rate: 4.758051727549173e-05\n",
      "Step: 610, train/epoch: 0.1451689600944519\n",
      "Step: 620, train/loss: 0.0\n",
      "Step: 620, train/grad_norm: 1.8454936423495383e-07\n",
      "Step: 620, train/learning_rate: 4.754085239255801e-05\n",
      "Step: 620, train/epoch: 0.14754877984523773\n",
      "Step: 630, train/loss: 0.0\n",
      "Step: 630, train/grad_norm: 0.0001581971300765872\n",
      "Step: 630, train/learning_rate: 4.7501191147603095e-05\n",
      "Step: 630, train/epoch: 0.14992859959602356\n",
      "Step: 640, train/loss: 0.0\n",
      "Step: 640, train/grad_norm: 1.1094050478277495e-07\n",
      "Step: 640, train/learning_rate: 4.7461526264669374e-05\n",
      "Step: 640, train/epoch: 0.1523084193468094\n",
      "Step: 650, train/loss: 0.0\n",
      "Step: 650, train/grad_norm: 1.0144433417735854e-06\n",
      "Step: 650, train/learning_rate: 4.742186138173565e-05\n",
      "Step: 650, train/epoch: 0.15468823909759521\n",
      "Step: 660, train/loss: 0.0\n",
      "Step: 660, train/grad_norm: 0.014472667127847672\n",
      "Step: 660, train/learning_rate: 4.738220013678074e-05\n",
      "Step: 660, train/epoch: 0.15706805884838104\n",
      "Step: 670, train/loss: 0.0\n",
      "Step: 670, train/grad_norm: 4.011871737596984e-11\n",
      "Step: 670, train/learning_rate: 4.734253525384702e-05\n",
      "Step: 670, train/epoch: 0.15944787859916687\n",
      "Step: 680, train/loss: 0.04699999839067459\n",
      "Step: 680, train/grad_norm: 0.9688159823417664\n",
      "Step: 680, train/learning_rate: 4.73028703709133e-05\n",
      "Step: 680, train/epoch: 0.1618276983499527\n",
      "Step: 690, train/loss: 0.4187999963760376\n",
      "Step: 690, train/grad_norm: 1.4526973473039106e-06\n",
      "Step: 690, train/learning_rate: 4.726320912595838e-05\n",
      "Step: 690, train/epoch: 0.16420751810073853\n",
      "Step: 700, train/loss: 0.1454000025987625\n",
      "Step: 700, train/grad_norm: 4.544951438903809\n",
      "Step: 700, train/learning_rate: 4.722354424302466e-05\n",
      "Step: 700, train/epoch: 0.16658733785152435\n",
      "Step: 710, train/loss: 0.3628000020980835\n",
      "Step: 710, train/grad_norm: 9.590764420863707e-06\n",
      "Step: 710, train/learning_rate: 4.718387936009094e-05\n",
      "Step: 710, train/epoch: 0.16896715760231018\n",
      "Step: 720, train/loss: 0.0012000000569969416\n",
      "Step: 720, train/grad_norm: 0.0034838549327105284\n",
      "Step: 720, train/learning_rate: 4.714421811513603e-05\n",
      "Step: 720, train/epoch: 0.171346977353096\n",
      "Step: 730, train/loss: 0.03849999979138374\n",
      "Step: 730, train/grad_norm: 2.752032060016063e-07\n",
      "Step: 730, train/learning_rate: 4.7104553232202306e-05\n",
      "Step: 730, train/epoch: 0.17372679710388184\n",
      "Step: 740, train/loss: 0.002199999988079071\n",
      "Step: 740, train/grad_norm: 9.276271839553374e-08\n",
      "Step: 740, train/learning_rate: 4.7064888349268585e-05\n",
      "Step: 740, train/epoch: 0.17610661685466766\n",
      "Step: 750, train/loss: 0.00019999999494757503\n",
      "Step: 750, train/grad_norm: 1.855431037256494e-05\n",
      "Step: 750, train/learning_rate: 4.702522710431367e-05\n",
      "Step: 750, train/epoch: 0.1784864366054535\n",
      "Step: 760, train/loss: 0.0\n",
      "Step: 760, train/grad_norm: 0.0006355285877361894\n",
      "Step: 760, train/learning_rate: 4.698556222137995e-05\n",
      "Step: 760, train/epoch: 0.18086625635623932\n",
      "Step: 770, train/loss: 0.00039999998989515007\n",
      "Step: 770, train/grad_norm: 7.00010932632722e-05\n",
      "Step: 770, train/learning_rate: 4.694589733844623e-05\n",
      "Step: 770, train/epoch: 0.18324607610702515\n",
      "Step: 780, train/loss: 0.0\n",
      "Step: 780, train/grad_norm: 5.510936418318124e-08\n",
      "Step: 780, train/learning_rate: 4.6906236093491316e-05\n",
      "Step: 780, train/epoch: 0.18562589585781097\n",
      "Step: 790, train/loss: 9.999999747378752e-05\n",
      "Step: 790, train/grad_norm: 0.5673570036888123\n",
      "Step: 790, train/learning_rate: 4.6866571210557595e-05\n",
      "Step: 790, train/epoch: 0.1880057156085968\n",
      "Step: 800, train/loss: 0.0\n",
      "Step: 800, train/grad_norm: 0.00036635552532970905\n",
      "Step: 800, train/learning_rate: 4.6826906327623874e-05\n",
      "Step: 800, train/epoch: 0.19038553535938263\n",
      "Step: 810, train/loss: 0.3352000117301941\n",
      "Step: 810, train/grad_norm: 4.290363264658481e-08\n",
      "Step: 810, train/learning_rate: 4.678724508266896e-05\n",
      "Step: 810, train/epoch: 0.19276535511016846\n",
      "Step: 820, train/loss: 0.0\n",
      "Step: 820, train/grad_norm: 2.1905410818590099e-10\n",
      "Step: 820, train/learning_rate: 4.674758019973524e-05\n",
      "Step: 820, train/epoch: 0.19514517486095428\n",
      "Step: 830, train/loss: 0.07190000265836716\n",
      "Step: 830, train/grad_norm: 293.7374267578125\n",
      "Step: 830, train/learning_rate: 4.670791531680152e-05\n",
      "Step: 830, train/epoch: 0.1975249946117401\n",
      "Step: 840, train/loss: 0.19380000233650208\n",
      "Step: 840, train/grad_norm: 9.779119864106178e-06\n",
      "Step: 840, train/learning_rate: 4.6668254071846604e-05\n",
      "Step: 840, train/epoch: 0.19990481436252594\n",
      "Step: 850, train/loss: 0.0\n",
      "Step: 850, train/grad_norm: 2.281357956235297e-05\n",
      "Step: 850, train/learning_rate: 4.6628589188912883e-05\n",
      "Step: 850, train/epoch: 0.20228461921215057\n",
      "Step: 860, train/loss: 0.0\n",
      "Step: 860, train/grad_norm: 7.653431384824216e-05\n",
      "Step: 860, train/learning_rate: 4.658892430597916e-05\n",
      "Step: 860, train/epoch: 0.2046644389629364\n",
      "Step: 870, train/loss: 0.00019999999494757503\n",
      "Step: 870, train/grad_norm: 0.00011805989925051108\n",
      "Step: 870, train/learning_rate: 4.654926306102425e-05\n",
      "Step: 870, train/epoch: 0.20704425871372223\n",
      "Step: 880, train/loss: 0.0020000000949949026\n",
      "Step: 880, train/grad_norm: 6.401017937918141e-09\n",
      "Step: 880, train/learning_rate: 4.650959817809053e-05\n",
      "Step: 880, train/epoch: 0.20942407846450806\n",
      "Step: 890, train/loss: 0.0\n",
      "Step: 890, train/grad_norm: 2.932013964951352e-09\n",
      "Step: 890, train/learning_rate: 4.646993329515681e-05\n",
      "Step: 890, train/epoch: 0.21180389821529388\n",
      "Step: 900, train/loss: 0.0\n",
      "Step: 900, train/grad_norm: 1.7988362515097833e-06\n",
      "Step: 900, train/learning_rate: 4.643027205020189e-05\n",
      "Step: 900, train/epoch: 0.2141837179660797\n",
      "Step: 910, train/loss: 0.0\n",
      "Step: 910, train/grad_norm: 5.440488615171546e-10\n",
      "Step: 910, train/learning_rate: 4.639060716726817e-05\n",
      "Step: 910, train/epoch: 0.21656353771686554\n",
      "Step: 920, train/loss: 0.0\n",
      "Step: 920, train/grad_norm: 0.0001691088400548324\n",
      "Step: 920, train/learning_rate: 4.635094228433445e-05\n",
      "Step: 920, train/epoch: 0.21894335746765137\n",
      "Step: 930, train/loss: 0.0\n",
      "Step: 930, train/grad_norm: 1.1184434489663886e-09\n",
      "Step: 930, train/learning_rate: 4.631128103937954e-05\n",
      "Step: 930, train/epoch: 0.2213231772184372\n",
      "Step: 940, train/loss: 0.0\n",
      "Step: 940, train/grad_norm: 1.5617986264260253e-06\n",
      "Step: 940, train/learning_rate: 4.6271616156445816e-05\n",
      "Step: 940, train/epoch: 0.22370299696922302\n",
      "Step: 950, train/loss: 0.0\n",
      "Step: 950, train/grad_norm: 1.9445591803446405e-08\n",
      "Step: 950, train/learning_rate: 4.6231951273512095e-05\n",
      "Step: 950, train/epoch: 0.22608281672000885\n",
      "Step: 960, train/loss: 0.0\n",
      "Step: 960, train/grad_norm: 4.3612149447369575e-09\n",
      "Step: 960, train/learning_rate: 4.619229002855718e-05\n",
      "Step: 960, train/epoch: 0.22846263647079468\n",
      "Step: 970, train/loss: 0.0\n",
      "Step: 970, train/grad_norm: 6.97460933452021e-08\n",
      "Step: 970, train/learning_rate: 4.615262514562346e-05\n",
      "Step: 970, train/epoch: 0.2308424562215805\n",
      "Step: 980, train/loss: 0.2687999904155731\n",
      "Step: 980, train/grad_norm: 0.025798747316002846\n",
      "Step: 980, train/learning_rate: 4.611296026268974e-05\n",
      "Step: 980, train/epoch: 0.23322227597236633\n",
      "Step: 990, train/loss: 0.0\n",
      "Step: 990, train/grad_norm: 0.08484301716089249\n",
      "Step: 990, train/learning_rate: 4.6073299017734826e-05\n",
      "Step: 990, train/epoch: 0.23560209572315216\n",
      "Step: 1000, train/loss: 0.0\n",
      "Step: 1000, train/grad_norm: 1.0114144970430061e-05\n",
      "Step: 1000, train/learning_rate: 4.6033634134801105e-05\n",
      "Step: 1000, train/epoch: 0.237981915473938\n",
      "Step: 1010, train/loss: 0.2969000041484833\n",
      "Step: 1010, train/grad_norm: 8.086733032541815e-06\n",
      "Step: 1010, train/learning_rate: 4.599397288984619e-05\n",
      "Step: 1010, train/epoch: 0.24036173522472382\n",
      "Step: 1020, train/loss: 0.04540000110864639\n",
      "Step: 1020, train/grad_norm: 1.914010863401927e-05\n",
      "Step: 1020, train/learning_rate: 4.595430800691247e-05\n",
      "Step: 1020, train/epoch: 0.24274155497550964\n",
      "Step: 1030, train/loss: 0.8970000147819519\n",
      "Step: 1030, train/grad_norm: 1.8656716171960852e-09\n",
      "Step: 1030, train/learning_rate: 4.591464312397875e-05\n",
      "Step: 1030, train/epoch: 0.24512137472629547\n",
      "Step: 1040, train/loss: 0.003700000001117587\n",
      "Step: 1040, train/grad_norm: 1.018511674999445e-08\n",
      "Step: 1040, train/learning_rate: 4.5874981879023835e-05\n",
      "Step: 1040, train/epoch: 0.2475011944770813\n",
      "Step: 1050, train/loss: 0.0\n",
      "Step: 1050, train/grad_norm: 6.265522074500041e-07\n",
      "Step: 1050, train/learning_rate: 4.5835316996090114e-05\n",
      "Step: 1050, train/epoch: 0.24988101422786713\n",
      "Step: 1060, train/loss: 0.004999999888241291\n",
      "Step: 1060, train/grad_norm: 3.050010022320748e-08\n",
      "Step: 1060, train/learning_rate: 4.579565211315639e-05\n",
      "Step: 1060, train/epoch: 0.25226083397865295\n",
      "Step: 1070, train/loss: 0.0\n",
      "Step: 1070, train/grad_norm: 2.9782489718854777e-07\n",
      "Step: 1070, train/learning_rate: 4.575599086820148e-05\n",
      "Step: 1070, train/epoch: 0.2546406388282776\n",
      "Step: 1080, train/loss: 0.0\n",
      "Step: 1080, train/grad_norm: 3.190428898847131e-08\n",
      "Step: 1080, train/learning_rate: 4.571632598526776e-05\n",
      "Step: 1080, train/epoch: 0.2570204734802246\n",
      "Step: 1090, train/loss: 0.0\n",
      "Step: 1090, train/grad_norm: 1.010787098465471e-08\n",
      "Step: 1090, train/learning_rate: 4.567666110233404e-05\n",
      "Step: 1090, train/epoch: 0.25940027832984924\n",
      "Step: 1100, train/loss: 0.14920000731945038\n",
      "Step: 1100, train/grad_norm: 8.084679037345666e-10\n",
      "Step: 1100, train/learning_rate: 4.5636999857379124e-05\n",
      "Step: 1100, train/epoch: 0.26178011298179626\n",
      "Step: 1110, train/loss: 0.0\n",
      "Step: 1110, train/grad_norm: 2.821714133460773e-06\n",
      "Step: 1110, train/learning_rate: 4.55973349744454e-05\n",
      "Step: 1110, train/epoch: 0.2641599178314209\n",
      "Step: 1120, train/loss: 0.0\n",
      "Step: 1120, train/grad_norm: 2.560565093290279e-09\n",
      "Step: 1120, train/learning_rate: 4.555767009151168e-05\n",
      "Step: 1120, train/epoch: 0.2665397524833679\n",
      "Step: 1130, train/loss: 0.0\n",
      "Step: 1130, train/grad_norm: 1.2383831329512316e-15\n",
      "Step: 1130, train/learning_rate: 4.551800884655677e-05\n",
      "Step: 1130, train/epoch: 0.26891955733299255\n",
      "Step: 1140, train/loss: 0.0\n",
      "Step: 1140, train/grad_norm: 1.2732503940071638e-09\n",
      "Step: 1140, train/learning_rate: 4.547834396362305e-05\n",
      "Step: 1140, train/epoch: 0.2712993919849396\n",
      "Step: 1150, train/loss: 0.0\n",
      "Step: 1150, train/grad_norm: 7.485618169766894e-08\n",
      "Step: 1150, train/learning_rate: 4.5438679080689326e-05\n",
      "Step: 1150, train/epoch: 0.2736791968345642\n",
      "Step: 1160, train/loss: 0.0\n",
      "Step: 1160, train/grad_norm: 4.409019330187114e-11\n",
      "Step: 1160, train/learning_rate: 4.539901783573441e-05\n",
      "Step: 1160, train/epoch: 0.27605903148651123\n",
      "Step: 1170, train/loss: 0.0\n",
      "Step: 1170, train/grad_norm: 3.4084408184753556e-08\n",
      "Step: 1170, train/learning_rate: 4.535935295280069e-05\n",
      "Step: 1170, train/epoch: 0.27843883633613586\n",
      "Step: 1180, train/loss: 0.0\n",
      "Step: 1180, train/grad_norm: 5.675857006615104e-10\n",
      "Step: 1180, train/learning_rate: 4.531968806986697e-05\n",
      "Step: 1180, train/epoch: 0.2808186709880829\n",
      "Step: 1190, train/loss: 0.0\n",
      "Step: 1190, train/grad_norm: 1.718937348016425e-08\n",
      "Step: 1190, train/learning_rate: 4.5280026824912056e-05\n",
      "Step: 1190, train/epoch: 0.2831984758377075\n",
      "Step: 1200, train/loss: 0.014499999582767487\n",
      "Step: 1200, train/grad_norm: 1.1470563549664803e-05\n",
      "Step: 1200, train/learning_rate: 4.5240361941978335e-05\n",
      "Step: 1200, train/epoch: 0.28557831048965454\n",
      "Step: 1210, train/loss: 0.0\n",
      "Step: 1210, train/grad_norm: 1.6167074345929677e-09\n",
      "Step: 1210, train/learning_rate: 4.5200697059044614e-05\n",
      "Step: 1210, train/epoch: 0.2879581153392792\n",
      "Step: 1220, train/loss: 0.0\n",
      "Step: 1220, train/grad_norm: 9.137311351992139e-09\n",
      "Step: 1220, train/learning_rate: 4.51610358140897e-05\n",
      "Step: 1220, train/epoch: 0.2903379201889038\n",
      "Step: 1230, train/loss: 0.0\n",
      "Step: 1230, train/grad_norm: 2.164067813836823e-09\n",
      "Step: 1230, train/learning_rate: 4.512137093115598e-05\n",
      "Step: 1230, train/epoch: 0.29271775484085083\n",
      "Step: 1240, train/loss: 0.0\n",
      "Step: 1240, train/grad_norm: 4.283123473669548e-07\n",
      "Step: 1240, train/learning_rate: 4.508170604822226e-05\n",
      "Step: 1240, train/epoch: 0.29509755969047546\n",
      "Step: 1250, train/loss: 0.38359999656677246\n",
      "Step: 1250, train/grad_norm: 1.1133330701795785e-07\n",
      "Step: 1250, train/learning_rate: 4.5042044803267345e-05\n",
      "Step: 1250, train/epoch: 0.2974773943424225\n",
      "Step: 1260, train/loss: 0.09380000084638596\n",
      "Step: 1260, train/grad_norm: 102.41167449951172\n",
      "Step: 1260, train/learning_rate: 4.5002379920333624e-05\n",
      "Step: 1260, train/epoch: 0.2998571991920471\n",
      "Step: 1270, train/loss: 0.0\n",
      "Step: 1270, train/grad_norm: 7.548539997515036e-06\n",
      "Step: 1270, train/learning_rate: 4.49627150373999e-05\n",
      "Step: 1270, train/epoch: 0.30223703384399414\n",
      "Step: 1280, train/loss: 0.0\n",
      "Step: 1280, train/grad_norm: 2.7440520966592885e-08\n",
      "Step: 1280, train/learning_rate: 4.492305379244499e-05\n",
      "Step: 1280, train/epoch: 0.3046168386936188\n",
      "Step: 1290, train/loss: 0.0\n",
      "Step: 1290, train/grad_norm: 4.2903411379136003e-10\n",
      "Step: 1290, train/learning_rate: 4.488338890951127e-05\n",
      "Step: 1290, train/epoch: 0.3069966733455658\n",
      "Step: 1300, train/loss: 0.0\n",
      "Step: 1300, train/grad_norm: 1.0705351769502158e-06\n",
      "Step: 1300, train/learning_rate: 4.484372402657755e-05\n",
      "Step: 1300, train/epoch: 0.30937647819519043\n",
      "Step: 1310, train/loss: 0.0\n",
      "Step: 1310, train/grad_norm: 2.9824448574800044e-05\n",
      "Step: 1310, train/learning_rate: 4.480406278162263e-05\n",
      "Step: 1310, train/epoch: 0.31175631284713745\n",
      "Step: 1320, train/loss: 0.0\n",
      "Step: 1320, train/grad_norm: 7.430408288655599e-09\n",
      "Step: 1320, train/learning_rate: 4.476439789868891e-05\n",
      "Step: 1320, train/epoch: 0.3141361176967621\n",
      "Step: 1330, train/loss: 0.0\n",
      "Step: 1330, train/grad_norm: 4.047105983673305e-10\n",
      "Step: 1330, train/learning_rate: 4.472473301575519e-05\n",
      "Step: 1330, train/epoch: 0.3165159523487091\n",
      "Step: 1340, train/loss: 0.0\n",
      "Step: 1340, train/grad_norm: 2.0707128456365353e-08\n",
      "Step: 1340, train/learning_rate: 4.468507177080028e-05\n",
      "Step: 1340, train/epoch: 0.31889575719833374\n",
      "Step: 1350, train/loss: 0.3172000050544739\n",
      "Step: 1350, train/grad_norm: 0.0011215104022994637\n",
      "Step: 1350, train/learning_rate: 4.464540688786656e-05\n",
      "Step: 1350, train/epoch: 0.32127559185028076\n",
      "Step: 1360, train/loss: 0.0\n",
      "Step: 1360, train/grad_norm: 1.219194274426627e-07\n",
      "Step: 1360, train/learning_rate: 4.4605742004932836e-05\n",
      "Step: 1360, train/epoch: 0.3236553966999054\n",
      "Step: 1370, train/loss: 0.0\n",
      "Step: 1370, train/grad_norm: 0.002928240690380335\n",
      "Step: 1370, train/learning_rate: 4.456608075997792e-05\n",
      "Step: 1370, train/epoch: 0.3260352313518524\n",
      "Step: 1380, train/loss: 0.0\n",
      "Step: 1380, train/grad_norm: 2.6739438908407465e-05\n",
      "Step: 1380, train/learning_rate: 4.45264158770442e-05\n",
      "Step: 1380, train/epoch: 0.32841503620147705\n",
      "Step: 1390, train/loss: 0.0\n",
      "Step: 1390, train/grad_norm: 1.402841280651046e-05\n",
      "Step: 1390, train/learning_rate: 4.448675099411048e-05\n",
      "Step: 1390, train/epoch: 0.3307948708534241\n",
      "Step: 1400, train/loss: 0.0\n",
      "Step: 1400, train/grad_norm: 2.8894880870211637e-06\n",
      "Step: 1400, train/learning_rate: 4.4447089749155566e-05\n",
      "Step: 1400, train/epoch: 0.3331746757030487\n",
      "Step: 1410, train/loss: 0.0\n",
      "Step: 1410, train/grad_norm: 2.2191395032677974e-07\n",
      "Step: 1410, train/learning_rate: 4.4407424866221845e-05\n",
      "Step: 1410, train/epoch: 0.3355545103549957\n",
      "Step: 1420, train/loss: 0.0\n",
      "Step: 1420, train/grad_norm: 9.838081496127415e-06\n",
      "Step: 1420, train/learning_rate: 4.4367759983288124e-05\n",
      "Step: 1420, train/epoch: 0.33793431520462036\n",
      "Step: 1430, train/loss: 0.2312999963760376\n",
      "Step: 1430, train/grad_norm: 1.6641408819850767e-06\n",
      "Step: 1430, train/learning_rate: 4.432809873833321e-05\n",
      "Step: 1430, train/epoch: 0.3403141498565674\n",
      "Step: 1440, train/loss: 0.0012000000569969416\n",
      "Step: 1440, train/grad_norm: 2.8296174292563592e-08\n",
      "Step: 1440, train/learning_rate: 4.428843385539949e-05\n",
      "Step: 1440, train/epoch: 0.342693954706192\n",
      "Step: 1450, train/loss: 0.007400000002235174\n",
      "Step: 1450, train/grad_norm: 1.1815775735612988e-07\n",
      "Step: 1450, train/learning_rate: 4.424876897246577e-05\n",
      "Step: 1450, train/epoch: 0.34507375955581665\n",
      "Step: 1460, train/loss: 0.00019999999494757503\n",
      "Step: 1460, train/grad_norm: 0.3471050560474396\n",
      "Step: 1460, train/learning_rate: 4.4209107727510855e-05\n",
      "Step: 1460, train/epoch: 0.34745359420776367\n",
      "Step: 1470, train/loss: 0.0\n",
      "Step: 1470, train/grad_norm: 1.1496247509512614e-08\n",
      "Step: 1470, train/learning_rate: 4.4169442844577134e-05\n",
      "Step: 1470, train/epoch: 0.3498333990573883\n",
      "Step: 1480, train/loss: 0.0006000000284984708\n",
      "Step: 1480, train/grad_norm: 1.213438238778508e-09\n",
      "Step: 1480, train/learning_rate: 4.412977796164341e-05\n",
      "Step: 1480, train/epoch: 0.3522132337093353\n",
      "Step: 1490, train/loss: 0.03519999980926514\n",
      "Step: 1490, train/grad_norm: 2.0078124052247404e-08\n",
      "Step: 1490, train/learning_rate: 4.40901167166885e-05\n",
      "Step: 1490, train/epoch: 0.35459303855895996\n",
      "Step: 1500, train/loss: 0.0\n",
      "Step: 1500, train/grad_norm: 3.0245324023070452e-09\n",
      "Step: 1500, train/learning_rate: 4.405045183375478e-05\n",
      "Step: 1500, train/epoch: 0.356972873210907\n",
      "Step: 1510, train/loss: 0.0\n",
      "Step: 1510, train/grad_norm: 1.3541787780013692e-07\n",
      "Step: 1510, train/learning_rate: 4.401078695082106e-05\n",
      "Step: 1510, train/epoch: 0.3593526780605316\n",
      "Step: 1520, train/loss: 0.0\n",
      "Step: 1520, train/grad_norm: 0.0481974259018898\n",
      "Step: 1520, train/learning_rate: 4.397112570586614e-05\n",
      "Step: 1520, train/epoch: 0.36173251271247864\n",
      "Step: 1530, train/loss: 0.0\n",
      "Step: 1530, train/grad_norm: 1.5689718679823272e-07\n",
      "Step: 1530, train/learning_rate: 4.393146082293242e-05\n",
      "Step: 1530, train/epoch: 0.36411231756210327\n",
      "Step: 1540, train/loss: 0.0\n",
      "Step: 1540, train/grad_norm: 2.035477770492497e-13\n",
      "Step: 1540, train/learning_rate: 4.38917959399987e-05\n",
      "Step: 1540, train/epoch: 0.3664921522140503\n",
      "Step: 1550, train/loss: 0.0\n",
      "Step: 1550, train/grad_norm: 2.7069188490713714e-07\n",
      "Step: 1550, train/learning_rate: 4.385213469504379e-05\n",
      "Step: 1550, train/epoch: 0.3688719570636749\n",
      "Step: 1560, train/loss: 0.0\n",
      "Step: 1560, train/grad_norm: 2.0338070783054718e-07\n",
      "Step: 1560, train/learning_rate: 4.3812469812110066e-05\n",
      "Step: 1560, train/epoch: 0.37125179171562195\n",
      "Step: 1570, train/loss: 0.0\n",
      "Step: 1570, train/grad_norm: 6.266754892791937e-10\n",
      "Step: 1570, train/learning_rate: 4.3772804929176345e-05\n",
      "Step: 1570, train/epoch: 0.3736315965652466\n",
      "Step: 1580, train/loss: 0.0\n",
      "Step: 1580, train/grad_norm: 1.9543263988452964e-05\n",
      "Step: 1580, train/learning_rate: 4.373314368422143e-05\n",
      "Step: 1580, train/epoch: 0.3760114312171936\n",
      "Step: 1590, train/loss: 0.0\n",
      "Step: 1590, train/grad_norm: 5.542276539927116e-07\n",
      "Step: 1590, train/learning_rate: 4.369347880128771e-05\n",
      "Step: 1590, train/epoch: 0.37839123606681824\n",
      "Step: 1600, train/loss: 0.0\n",
      "Step: 1600, train/grad_norm: 8.098259058897384e-06\n",
      "Step: 1600, train/learning_rate: 4.365381391835399e-05\n",
      "Step: 1600, train/epoch: 0.38077107071876526\n",
      "Step: 1610, train/loss: 0.0\n",
      "Step: 1610, train/grad_norm: 1.769231374737501e-07\n",
      "Step: 1610, train/learning_rate: 4.3614152673399076e-05\n",
      "Step: 1610, train/epoch: 0.3831508755683899\n",
      "Step: 1620, train/loss: 0.0\n",
      "Step: 1620, train/grad_norm: 6.851384881656486e-08\n",
      "Step: 1620, train/learning_rate: 4.3574487790465355e-05\n",
      "Step: 1620, train/epoch: 0.3855307102203369\n",
      "Step: 1630, train/loss: 0.0\n",
      "Step: 1630, train/grad_norm: 0.0033404265996068716\n",
      "Step: 1630, train/learning_rate: 4.3534822907531634e-05\n",
      "Step: 1630, train/epoch: 0.38791051506996155\n",
      "Step: 1640, train/loss: 0.0\n",
      "Step: 1640, train/grad_norm: 7.865955353736354e-08\n",
      "Step: 1640, train/learning_rate: 4.349516166257672e-05\n",
      "Step: 1640, train/epoch: 0.39029034972190857\n",
      "Step: 1650, train/loss: 9.999999747378752e-05\n",
      "Step: 1650, train/grad_norm: 3.981060370250589e-09\n",
      "Step: 1650, train/learning_rate: 4.3455496779643e-05\n",
      "Step: 1650, train/epoch: 0.3926701545715332\n",
      "Step: 1660, train/loss: 0.0003000000142492354\n",
      "Step: 1660, train/grad_norm: 2.1795018767534202e-07\n",
      "Step: 1660, train/learning_rate: 4.3415835534688085e-05\n",
      "Step: 1660, train/epoch: 0.3950499892234802\n",
      "Step: 1670, train/loss: 0.023900000378489494\n",
      "Step: 1670, train/grad_norm: 2.579990799993215e-13\n",
      "Step: 1670, train/learning_rate: 4.3376170651754364e-05\n",
      "Step: 1670, train/epoch: 0.39742979407310486\n",
      "Step: 1680, train/loss: 0.0\n",
      "Step: 1680, train/grad_norm: 1.780484266966642e-11\n",
      "Step: 1680, train/learning_rate: 4.3336505768820643e-05\n",
      "Step: 1680, train/epoch: 0.3998096287250519\n",
      "Step: 1690, train/loss: 0.30630001425743103\n",
      "Step: 1690, train/grad_norm: 3.133034454094741e-07\n",
      "Step: 1690, train/learning_rate: 4.329684452386573e-05\n",
      "Step: 1690, train/epoch: 0.4021894335746765\n",
      "Step: 1700, train/loss: 0.0\n",
      "Step: 1700, train/grad_norm: 2.0946673284782946e-09\n",
      "Step: 1700, train/learning_rate: 4.325717964093201e-05\n",
      "Step: 1700, train/epoch: 0.40456923842430115\n",
      "Step: 1710, train/loss: 0.004800000227987766\n",
      "Step: 1710, train/grad_norm: 6.80001666086838e-10\n",
      "Step: 1710, train/learning_rate: 4.321751475799829e-05\n",
      "Step: 1710, train/epoch: 0.40694907307624817\n",
      "Step: 1720, train/loss: 0.0\n",
      "Step: 1720, train/grad_norm: 2.372028689023864e-07\n",
      "Step: 1720, train/learning_rate: 4.3177853513043374e-05\n",
      "Step: 1720, train/epoch: 0.4093288779258728\n",
      "Step: 1730, train/loss: 0.0\n",
      "Step: 1730, train/grad_norm: 9.679516910221508e-11\n",
      "Step: 1730, train/learning_rate: 4.313818863010965e-05\n",
      "Step: 1730, train/epoch: 0.4117087125778198\n",
      "Step: 1740, train/loss: 0.0\n",
      "Step: 1740, train/grad_norm: 5.950012109678937e-07\n",
      "Step: 1740, train/learning_rate: 4.309852374717593e-05\n",
      "Step: 1740, train/epoch: 0.41408851742744446\n",
      "Step: 1750, train/loss: 0.0\n",
      "Step: 1750, train/grad_norm: 9.180485148127526e-13\n",
      "Step: 1750, train/learning_rate: 4.305886250222102e-05\n",
      "Step: 1750, train/epoch: 0.4164683520793915\n",
      "Step: 1760, train/loss: 0.0\n",
      "Step: 1760, train/grad_norm: 5.034073602416811e-09\n",
      "Step: 1760, train/learning_rate: 4.30191976192873e-05\n",
      "Step: 1760, train/epoch: 0.4188481569290161\n",
      "Step: 1770, train/loss: 0.0\n",
      "Step: 1770, train/grad_norm: 4.439390868804516e-11\n",
      "Step: 1770, train/learning_rate: 4.2979532736353576e-05\n",
      "Step: 1770, train/epoch: 0.42122799158096313\n",
      "Step: 1780, train/loss: 0.0\n",
      "Step: 1780, train/grad_norm: 1.780727529876458e-06\n",
      "Step: 1780, train/learning_rate: 4.293987149139866e-05\n",
      "Step: 1780, train/epoch: 0.42360779643058777\n",
      "Step: 1790, train/loss: 0.0\n",
      "Step: 1790, train/grad_norm: 2.3054208281997335e-12\n",
      "Step: 1790, train/learning_rate: 4.290020660846494e-05\n",
      "Step: 1790, train/epoch: 0.4259876310825348\n",
      "Step: 1800, train/loss: 0.0\n",
      "Step: 1800, train/grad_norm: 0.004130410961806774\n",
      "Step: 1800, train/learning_rate: 4.286054172553122e-05\n",
      "Step: 1800, train/epoch: 0.4283674359321594\n",
      "Step: 1810, train/loss: 0.0\n",
      "Step: 1810, train/grad_norm: 5.983178582707202e-15\n",
      "Step: 1810, train/learning_rate: 4.2820880480576307e-05\n",
      "Step: 1810, train/epoch: 0.43074727058410645\n",
      "Step: 1820, train/loss: 0.0\n",
      "Step: 1820, train/grad_norm: 8.321258064825088e-06\n",
      "Step: 1820, train/learning_rate: 4.2781215597642586e-05\n",
      "Step: 1820, train/epoch: 0.4331270754337311\n",
      "Step: 1830, train/loss: 0.0\n",
      "Step: 1830, train/grad_norm: 1.8894472304964438e-05\n",
      "Step: 1830, train/learning_rate: 4.2741550714708865e-05\n",
      "Step: 1830, train/epoch: 0.4355069100856781\n",
      "Step: 1840, train/loss: 0.0\n",
      "Step: 1840, train/grad_norm: 5.9085265569081e-09\n",
      "Step: 1840, train/learning_rate: 4.270188946975395e-05\n",
      "Step: 1840, train/epoch: 0.43788671493530273\n",
      "Step: 1850, train/loss: 0.14920000731945038\n",
      "Step: 1850, train/grad_norm: 4.2299055425833176e-13\n",
      "Step: 1850, train/learning_rate: 4.266222458682023e-05\n",
      "Step: 1850, train/epoch: 0.44026654958724976\n",
      "Step: 1860, train/loss: 0.0\n",
      "Step: 1860, train/grad_norm: 6.771581906317437e-11\n",
      "Step: 1860, train/learning_rate: 4.262255970388651e-05\n",
      "Step: 1860, train/epoch: 0.4426463544368744\n",
      "Step: 1870, train/loss: 0.0\n",
      "Step: 1870, train/grad_norm: 2.1537712058261604e-08\n",
      "Step: 1870, train/learning_rate: 4.2582898458931595e-05\n",
      "Step: 1870, train/epoch: 0.4450261890888214\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ffd0164-0f41-4bbe-b905-7dda371dfd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Step Train Loss Eval Loss  Accuracy        F1\n",
      "0   4202   0.000000  0.008017  0.999306  0.999267\n",
      "1   8404   0.000000  0.013603  0.999306  0.999267\n",
      "2  12606   0.000000  0.017048  0.999306  0.999267\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"./images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37cc5f92-d47f-4356-8fad-d9b64b6f5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-4202\n",
      "Step              4202\n",
      "Train Loss    0.000000\n",
      "Eval Loss     0.008017\n",
      "Accuracy      0.999306\n",
      "F1            0.999267\n",
      "Rank Sum           3.0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3c3999-8c32-4a25-aaca-2ec9036b69ed",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f36022-dc73-492f-998c-43e5ed1a6f46",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "444ae65b-241c-47ef-bbc2-81f3a2ce50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8be3672-68e0-44f8-b8b1-31290665658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb3b5ed2-5320-49c0-9e61-90d6f0942f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5111194-5eeb-4104-8df0-f977a6b716e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5b29f7b2414dfc9d7b97345972ccb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MixtralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mixtral-8x22B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, BitsAndBytesConfig\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    num_labels=2,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    checkpoint,\n",
    "    token=access_token,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "test_tokenizer.pad_token = test_tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(news):\n",
    "    return test_tokenizer(news['article'], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "530fa55f-c8c4-485f-a093-09bf2175d586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "ft_model = PeftModel.from_pretrained(base_model, project_name+'/'+test_checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27354c4f-95cc-4d1f-ac64-c5e6b4a842ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#     ft_model = torch.nn.DataParallel(ft_model)\n",
    "\n",
    "# ft_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eee9ab6f-a7d5-4dd5-9436-2f6f6e2bffaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7c23807a6348029810818fd2d2f10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "processed_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for record in tqdm(tokenized_test_ds):\n",
    "\n",
    "        eval_prompt = record['article']\n",
    "        model_input = tokenize_fn({'article': eval_prompt})\n",
    "\n",
    "        # model_input = {k: v.to('cuda') for k, v in model_input.items()}\n",
    "        \n",
    "        outputs = ft_model(**model_input)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        prediction = logits[0].argmax(-1).item()  # Use .item() to get a Python number\n",
    "        processed_predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "### Accuracy and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e891bc5-55e0-4c43-a035-0edc37dcb6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "true_articles = tokenized_test_ds['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bf33824-d7cc-4d22-af4d-7d44e569c2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9991670137442732\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(true_articles, processed_predictions)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b37dc24-3bc0-48a0-ace9-a360bae91169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29ebf68-632f-49d1-b903-407ff05b5569",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(processed_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.999121529800828\n"
     ]
    }
   ],
   "source": [
    "f1_score = f1_score(true_articles, processed_predictions, average='macro')\n",
    "print(\"f1_score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ae5f158-1ea6-44ea-88b0-91aa8bf331a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af7d6f55-fb4f-4195-ac3f-93852ecd4b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-Mixtral-8x22B-v0.1-small-finetune-v14.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 754946 bytes to html/praxis-Mixtral-8x22B-v0.1-small-finetune-v14.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
