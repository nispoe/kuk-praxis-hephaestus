{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c2d2c9-4a4b-48ca-90a3-1ccd120ca08b",
   "metadata": {},
   "source": [
    "# Fine tuning using BERT Base Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0f2da2-54e7-42b7-b44e-f6b7e8a48787",
   "metadata": {},
   "source": [
    "### Supress warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "362a364e-20f5-4d16-af44-62aada774f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d23075-e94c-4aa9-969f-ba66e1278d5e",
   "metadata": {},
   "source": [
    "### Inspect the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7cedfd8-5efe-47e5-84df-f1c00cd7cbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"bert-base-cased\"\n",
    "checkpoint = \"google-bert/\"+model_name\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b0595d-6453-48e4-982f-0608c272cb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "BertForSequenceClassification                           --\n",
       "├─BertModel: 1-1                                        --\n",
       "│    └─BertEmbeddings: 2-1                              --\n",
       "│    │    └─Embedding: 3-1                              22,268,928\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─Embedding: 3-3                              1,536\n",
       "│    │    └─LayerNorm: 3-4                              1,536\n",
       "│    │    └─Dropout: 3-5                                --\n",
       "│    └─BertEncoder: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-6                             85,054,464\n",
       "│    └─BertPooler: 2-3                                  --\n",
       "│    │    └─Linear: 3-7                                 590,592\n",
       "│    │    └─Tanh: 3-8                                   --\n",
       "├─Dropout: 1-2                                          --\n",
       "├─Linear: 1-3                                           1,538\n",
       "================================================================================\n",
       "Total params: 108,311,810\n",
       "Trainable params: 108,311,810\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a29b9b-fd31-4bb9-ada1-e7264f8a85a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601449e5-fe1f-4194-b061-4951fc8bb86f",
   "metadata": {},
   "source": [
    "### Load the news dataset from pickle file\n",
    "If any of the check_files don't exist then load the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5365d1de-318f-46e6-b91b-609f9fd8cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one of the specified files already exists. Not loading new dataset.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "base_path = './data/'\n",
    "os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "file_name = 'news_small_dataset.pkl'\n",
    "file_path = base_path+file_name\n",
    "\n",
    "def pickle_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as file:\n",
    "        pickle.dump(dataset, file)\n",
    "        print(f\"Dataset has been pickled to: {file_path}\")\n",
    "\n",
    "def load_pickle_dataset(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        print(f\"Dataset has been loaded from: {file_path}\")\n",
    "    return dataset\n",
    "\n",
    "def check_files_exists(file_names):\n",
    "    for name in file_names:\n",
    "        file_path = os.path.join(base_path, name)\n",
    "        if os.path.exists(file_path):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# if these files exist we do not want to load the news_dataset.pkl to tokenize and make these files\n",
    "check_files = [model_name+'-small_tokenized_train_ds.pkl', model_name+'-small_tokenized_eval_ds.pkl', model_name+'-small_tokenized_test_ds.pkl']\n",
    "\n",
    "if check_files_exists(check_files):\n",
    "    print(\"At least one of the specified files already exists. Not loading new dataset.\")\n",
    "else:\n",
    "    news_split_ds = load_pickle_dataset(file_path)\n",
    "    print(news_split_ds)\n",
    "    total_rows = (news_split_ds['train'].num_rows +\n",
    "              news_split_ds['eval'].num_rows +\n",
    "              news_split_ds['test'].num_rows)\n",
    "    print(\"Total number of rows:\", total_rows)\n",
    "    print(\"Dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395adbb2-9498-49cc-baf1-f09e3fcfb39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4d77764491499aa8f7e0b16f0007f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160c208cf51c447fbbcc8d1bc4330ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f52829c49d4ff3906ac18dde50c287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize_fn(news):\n",
    "  return tokenizer(news['article'], padding=True, truncation=True, return_tensors=\"pt\", max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6234e6-1e6b-4f41-b3a2-5bb0adfae5a4",
   "metadata": {},
   "source": [
    "### Tokenize train, evaluation, and test datasets\n",
    "If any of the check files exist then don't run tokenization and save some time.\n",
    "Else load the pickle files that already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0b27288-1ea3-4230-9f9d-aeab3cd76b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, so load datasets\n",
      "Dataset has been loaded from: ./data/bert-base-cased-small_tokenized_train_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-base-cased-small_tokenized_eval_ds.pkl\n",
      "Dataset has been loaded from: ./data/bert-base-cased-small_tokenized_test_ds.pkl\n"
     ]
    }
   ],
   "source": [
    "if not check_files_exists(check_files):\n",
    "    tokenized_train_ds = news_split_ds['train'].map(tokenize_fn, batched=True)\n",
    "    tokenized_eval_ds = news_split_ds['eval'].map(tokenize_fn, batched=True)\n",
    "    tokenized_test_ds = news_split_ds['test'].map(tokenize_fn, batched=True)\n",
    "\n",
    "    print(tokenized_train_ds.features)\n",
    "    print(tokenized_eval_ds.features)\n",
    "    print(tokenized_test_ds.features)\n",
    "    \n",
    "    pickle_dataset(tokenized_train_ds, base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    pickle_dataset(tokenized_eval_ds, base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    pickle_dataset(tokenized_test_ds, base_path+model_name+'-small_tokenized_test_ds.pkl')\n",
    "else:\n",
    "    print(\"Files already exist, so load datasets\")\n",
    "    tokenized_train_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_train_ds.pkl')\n",
    "    tokenized_eval_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_eval_ds.pkl')\n",
    "    tokenized_test_ds = load_pickle_dataset(base_path+model_name+'-small_tokenized_test_ds.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b2ac6-7634-40c9-9ed8-222634448ce1",
   "metadata": {},
   "source": [
    "### Look at the tokenized data\n",
    "Notice what the actual data looks like, and then the tokenized data which is a bunch of numbers, and then the attention mask at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ca4f524-e97e-4cee-b28a-74f204f669b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in training dataset: 33611\n",
      "Number of records in evaluation dataset: 7203\n",
      "Number of records in test dataset: 7203\n",
      "Total number of records: 48017\n"
     ]
    }
   ],
   "source": [
    "count_train_records = len(tokenized_train_ds)\n",
    "count_eval_records = len(tokenized_eval_ds)\n",
    "count_test_records = len(tokenized_test_ds)\n",
    "print(f\"Number of records in training dataset: {count_train_records}\")\n",
    "print(f\"Number of records in evaluation dataset: {count_eval_records}\")\n",
    "print(f\"Number of records in test dataset: {count_test_records}\")\n",
    "count_total_records = count_train_records + count_eval_records + count_test_records\n",
    "print(f\"Total number of records: {count_total_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4e094a-6fc0-4e11-8ab2-8e394b7a07a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'article': \"In a year where homicides, rapes and robberies increased slightly, New York City still saw serious crime drop 1.7 percent in 2015, continuing an overall decline that began in the 1990s, NYPD Commissioner William Bratton said Monday.\\nAt a news conference with Mayor Bill de Blasio, Bratton touted last year’s crime statistics, which he said, when combined with an even larger decline in 2014, put to rest the fear that substantial decreases couldn’t continue under the new administration at City Hall.\\n“While we have had some fluctuation, some increases in certain categories, the overall trend in all our crime categories continues to go down,” Bratton told reporters. “It was a very good year for us, 2015.\\nHomicides increased by 4.5 percent in 2015, rising to 350 from 333 in the prior year, which was the lowest since 1994, said Deputy Commissioner Dermot Shea. Rapes increased 6 percent and robberies rose 2 percent, said Shea, who is in charge of data collection and operations for the NYPD.\\nThe lower overall crime statistics came about due to what Shea called “targeted enforcement,” where cops make quality arrests even though the overall number of apprehensions was the lowest in the city since 2003.\\nTwo boroughs — Manhattan and the Bronx — actually saw serious crimes increase by 3 percent and 4 percent, respectively, Shea said. Manhattan’s increase was driven by more robberies, while the Bronx, although seeing an overall crime increase, had what he said was a “phenomenal” reduction in shootings. Citywide, shootings were down in 2015 about 3 percent, to 1,103 from 1,172 in 2014.\\nShea largely attributed the 2015 increase in rapes to victims coming forward with complaints about attacks from years past.\\nSign up to get the latest updates Get Newsday's Breaking News alerts in your inbox. By clicking Sign up, you agree to our privacy policy.\\n“Twenty percent of these rapes didn’t happen in 2015,” he said.\\nThe NYPD has seen an increase in rapes involving single women who, after a night of drinking, get into cabs of all kinds and are attacked, Shea said.\\n“They get driven, and passing out and waking up in a desolate area, and they get sexually attacked. This is something, really, that people need to be exceptionally aware of, and like any case in New York City, the buddy system works,” said Shea, referring to the need for people to travel in pairs when taking a cab at night.\\nBratton and police brass hope to build upon the continuing drop in overall crime by using technology such as ShotSpotter and a newly minted GPS system for police cars.\\nJessica Tisch, NYPD deputy commissioner for technology, said ShotSpotter, an acoustical system that detects gunfire, identified gunshots in 1,672 cases, mostly in Brooklyn. Of those alerts, 74 percent didn’t have any 911 calls from the public associated with them.\\nTisch said ShotSpotter helped police recover ballistic evidence in 19 percent of the gunfire alerts. In 22 percent of those cases, Tisch said, cops were able to make positive matches of bullets with those from guns used in earlier shootings.\\nTisch also highlighted a special GPS system being tried in about 5,000 patrol cars that allows the NYPD to see where its vehicles are and to track their movements over a 24-hour period, as well as gather information about the officers’ driving.\\n\", 'label': 0, 'input_ids': [101, 1130, 170, 1214, 1187, 27029, 1116, 117, 9372, 1116, 1105, 187, 12809, 3169, 1905, 2569, 2776, 117, 1203, 1365, 1392, 1253, 1486, 3021, 3755, 3968, 122, 119, 128, 3029, 1107, 1410, 117, 5542, 1126, 2905, 6246, 1115, 1310, 1107, 1103, 3281, 117, 5883, 15481, 6520, 1613, 139, 7625, 1633, 1163, 6356, 119, 1335, 170, 2371, 3511, 1114, 4643, 2617, 1260, 139, 7580, 2660, 117, 139, 7625, 1633, 1106, 18527, 1314, 1214, 787, 188, 3755, 9161, 117, 1134, 1119, 1163, 117, 1165, 3490, 1114, 1126, 1256, 2610, 6246, 1107, 1387, 117, 1508, 1106, 1832, 1103, 2945, 1115, 6432, 19377, 1577, 787, 189, 2760, 1223, 1103, 1207, 3469, 1120, 1392, 1944, 119, 789, 1799, 1195, 1138, 1125, 1199, 23896, 5822, 10255, 117, 1199, 6986, 1107, 2218, 6788, 117, 1103, 2905, 10209, 1107, 1155, 1412, 3755, 6788, 3430, 1106, 1301, 1205, 117, 790, 139, 7625, 1633, 1500, 13509, 119, 789, 1135, 1108, 170, 1304, 1363, 1214, 1111, 1366, 117, 1410, 119, 9800, 7257, 8959, 2569, 1118, 125, 119, 126, 3029, 1107, 1410, 117, 4703, 1106, 8301, 1121, 23335, 1107, 1103, 2988, 1214, 117, 1134, 1108, 1103, 6905, 1290, 1898, 117, 1163, 4831, 6520, 9682, 21277, 18352, 119, 21196, 1279, 2569, 127, 3029, 1105, 187, 12809, 3169, 1905, 3152, 123, 3029, 117, 1163, 18352, 117, 1150, 1110, 1107, 2965, 1104, 2233, 2436, 1105, 2500, 1111, 1103, 5883, 15481, 119, 1109, 2211, 2905, 3755, 9161, 1338, 1164, 1496, 1106, 1184, 18352, 1270, 789, 9271, 7742, 117, 790, 1187, 11396, 1294, 3068, 19189, 1256, 1463, 1103, 2905, 1295, 1104, 12647, 17121, 1116, 1108, 1103, 6905, 1107, 1103, 1331, 1290, 1581, 119, 1960, 27126, 783, 6545, 1105, 1103, 15115, 783, 2140, 1486, 3021, 6969, 2773, 1118, 124, 3029, 1105, 125, 3029, 117, 3569, 117, 18352, 1163, 119, 6545, 787, 188, 2773, 1108, 4940, 1118, 1167, 187, 12809, 3169, 1905, 117, 1229, 1103, 15115, 117, 1780, 3195, 1126, 2905, 3755, 2773, 117, 1125, 1184, 1119, 1163, 1108, 170, 789, 14343, 1233, 790, 7234, 1107, 4598, 1116, 119, 1392, 15665, 117, 4598, 1116, 1127, 1205, 1107, 1410, 1164, 124, 3029, 117, 1106, 122, 117, 9550, 1121, 122, 117, 19639, 1107, 1387, 119, 18352, 3494, 6547, 1103, 1410, 2773, 1107, 9372, 1116, 1106, 5256, 1909, 1977, 1114, 11344, 1164, 3690, 1121, 1201, 1763, 119, 20979, 1146, 1106, 1243, 1103, 6270, 15549, 3949, 3128, 6194, 112, 188, 20048, 3128, 10427, 1116, 1107, 1240, 1107, 8757, 119, 1650, 25600, 20979, 1146, 117, 1128, 5340, 1106, 1412, 9909, 2818, 119, 789, 7714, 3029, 1104, 1292, 9372, 1116, 1238, 787, 189, 3333, 1107, 1410, 117, 790, 1119, 1163, 119, 1109, 5883, 15481, 1144, 1562, 1126, 2773, 1107, 9372, 1116, 5336, 1423, 1535, 1150, 117, 1170, 170, 1480, 1104, 5464, 117, 1243, 1154, 10347, 1116, 1104, 1155, 7553, 1105, 1132, 3623, 117, 18352, 1163, 119, 789, 1220, 1243, 4940, 117, 1105, 3744, 1149, 1105, 14047, 1146, 1107, 170, 3532, 14995, 1298, 117, 1105, 1152, 1243, 13014, 3623, 119, 1188, 1110, 1380, 117, 1541, 117, 1115, 1234, 1444, 1106, 1129, 18635, 4484, 1104, 117, 1105, 1176, 1251, 1692, 1107, 1203, 1365, 1392, 117, 1103, 16723, 1449, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "first_record = tokenized_train_ds[0]\n",
    "print(first_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a991f2e-0532-4424-90d4-bda2758ec289",
   "metadata": {},
   "source": [
    "### Turn on accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17623651-b9c0-4dbe-bbd1-07bb51eda8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef357cf6-2074-4ad9-892d-f2ce93221e16",
   "metadata": {},
   "source": [
    "### Look at hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9848d5f-f539-456c-a110-72ac08530f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 7\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "GPU 1: NVIDIA GeForce RTX 4090\n",
      "GPU 2: NVIDIA GeForce RTX 4090\n",
      "GPU 3: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 4: NVIDIA GeForce RTX 3090 Ti\n",
      "GPU 5: NVIDIA GeForce RTX 3090\n",
      "GPU 6: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b847246a-83e8-4982-a6c8-ba549f8e38a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed May 22 02:55:14 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| 31%   35C    P8             44W /  420W |    6056MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off |   00000000:02:00.0 Off |                  Off |\n",
      "|  0%   39C    P8             21W /  450W |    6688MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:2A:00.0 Off |                  Off |\n",
      "| 32%   40C    P8             24W /  450W |    6118MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA GeForce RTX 3090        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "| 32%   34C    P8             28W /  420W |    6044MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   4  NVIDIA GeForce RTX 4090        Off |   00000000:42:00.0 Off |                  Off |\n",
      "|  0%   45C    P8             22W /  450W |    6231MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   5  NVIDIA GeForce RTX 4090        Off |   00000000:61:00.0 Off |                  Off |\n",
      "|  0%   42C    P8             11W /  450W |    6231MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   6  NVIDIA GeForce RTX 3090 Ti     Off |   00000000:62:00.0 Off |                  Off |\n",
      "| 30%   37C    P8             25W /  450W |    6118MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    0   N/A  N/A   2386955      C   /usr/bin/python3                             6036MiB |\n",
      "|    1   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                             86MiB |\n",
      "|    1   N/A  N/A   1026178      G   /usr/bin/gnome-shell                           13MiB |\n",
      "|    1   N/A  N/A   2386955      C   /usr/bin/python3                             6312MiB |\n",
      "|    2   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    2   N/A  N/A   2386955      C   /usr/bin/python3                             6098MiB |\n",
      "|    3   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    3   N/A  N/A   2386955      C   /usr/bin/python3                             6024MiB |\n",
      "|    4   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    4   N/A  N/A   2386955      C   /usr/bin/python3                             6210MiB |\n",
      "|    5   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    5   N/A  N/A   2386955      C   /usr/bin/python3                             6210MiB |\n",
      "|    6   N/A  N/A   1026070      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    6   N/A  N/A   2386955      C   /usr/bin/python3                             6098MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fabb987-30a6-47bf-8048-b76477a8b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(logits_and_labels):\n",
    "    logits, labels = logits_and_labels\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    return {'accuracy': acc, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69b144bb-2c85-429c-8643-3d98a423613d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./praxis-bert-base-cased-small-finetune'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_name = \"praxis-\"+model_name+\"-small-finetune\"\n",
    "output_dir_path = \"./\" + project_name\n",
    "output_dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b7cb69-8dbe-457e-9449-1e81323374ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a71a13f-2748-4216-b0aa-8200d7fc45d7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "306b4000-44a5-4b1c-956e-8f86ee3f280d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 02:55:15.469366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 02:55:15.998241: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnispoe\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nispoe/kuk/Praxis/wandb/run-20240522_025517-3licic23</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nispoe/huggingface/runs/3licic23' target=\"_blank\">glad-shape-292</a></strong> to <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nispoe/huggingface' target=\"_blank\">https://wandb.ai/nispoe/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nispoe/huggingface/runs/3licic23' target=\"_blank\">https://wandb.ai/nispoe/huggingface/runs/3licic23</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6010' max='6010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6010/6010 53:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.444500</td>\n",
       "      <td>0.520057</td>\n",
       "      <td>0.746356</td>\n",
       "      <td>0.745938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.422200</td>\n",
       "      <td>0.821186</td>\n",
       "      <td>0.816812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.168300</td>\n",
       "      <td>0.487362</td>\n",
       "      <td>0.827294</td>\n",
       "      <td>0.805340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.039800</td>\n",
       "      <td>0.725667</td>\n",
       "      <td>0.829793</td>\n",
       "      <td>0.823925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>0.742037</td>\n",
       "      <td>0.837707</td>\n",
       "      <td>0.823463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>1.216491</td>\n",
       "      <td>0.811467</td>\n",
       "      <td>0.808151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>1.271679</td>\n",
       "      <td>0.825073</td>\n",
       "      <td>0.820291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.018200</td>\n",
       "      <td>1.234804</td>\n",
       "      <td>0.829238</td>\n",
       "      <td>0.824152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>1.230972</td>\n",
       "      <td>0.837707</td>\n",
       "      <td>0.831597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>1.273724</td>\n",
       "      <td>0.840344</td>\n",
       "      <td>0.833650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6010, training_loss=0.12472764975513845, metrics={'train_runtime': 3220.8798, 'train_samples_per_second': 104.353, 'train_steps_per_second': 1.866, 'total_flos': 8.84342568170496e+16, 'train_loss': 0.12472764975513845, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.set_seed(777)\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_ds,\n",
    "    eval_dataset=tokenized_eval_ds,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir_path,\n",
    "        num_train_epochs=10,\n",
    "        logging_steps=10,\n",
    "        logging_dir=output_dir_path+\"/logs\",\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model = accelerator.prepare_model(model)\n",
    "trainer.train(resume_from_checkpoint=False)  # Turn to True if power goes out..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9247c1-3c27-4cb0-b4d6-602dcb02488e",
   "metadata": {},
   "source": [
    "### Determine best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f00a0d4e-adfc-4877-af62-4d84992590dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 44\n",
      "drwxr-xr-x 2 nispoe nispoe 4096 May 22 02:55 logs\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:00 checkpoint-601\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:06 checkpoint-1202\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:11 checkpoint-1803\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:16 checkpoint-2404\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:22 checkpoint-3005\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:27 checkpoint-3606\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:32 checkpoint-4207\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:38 checkpoint-4808\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:43 checkpoint-5409\n",
      "drwxrwxr-x 2 nispoe nispoe 4096 May 22 03:48 checkpoint-6010\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr {output_dir_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf241b2f-b4ac-47a7-941a-9b66b9f3e440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from file: ./praxis-bert-base-cased-small-finetune/logs/events.out.tfevents.1716364517.hephaestus.2438872.0\n",
      "Step: 10, train/loss: 0.6805999875068665\n",
      "Step: 10, train/grad_norm: 1.2185193300247192\n",
      "Step: 10, train/learning_rate: 4.9916805437533185e-05\n",
      "Step: 10, train/epoch: 0.01663893461227417\n",
      "Step: 20, train/loss: 0.671500027179718\n",
      "Step: 20, train/grad_norm: 3.4744997024536133\n",
      "Step: 20, train/learning_rate: 4.983361213817261e-05\n",
      "Step: 20, train/epoch: 0.03327786922454834\n",
      "Step: 30, train/loss: 0.6603999733924866\n",
      "Step: 30, train/grad_norm: 1.0045667886734009\n",
      "Step: 30, train/learning_rate: 4.975041520083323e-05\n",
      "Step: 30, train/epoch: 0.04991680383682251\n",
      "Step: 40, train/loss: 0.6710000038146973\n",
      "Step: 40, train/grad_norm: 0.8362419605255127\n",
      "Step: 40, train/learning_rate: 4.966722190147266e-05\n",
      "Step: 40, train/epoch: 0.06655573844909668\n",
      "Step: 50, train/loss: 0.6796000003814697\n",
      "Step: 50, train/grad_norm: 0.9347001910209656\n",
      "Step: 50, train/learning_rate: 4.958402496413328e-05\n",
      "Step: 50, train/epoch: 0.08319467306137085\n",
      "Step: 60, train/loss: 0.6694999933242798\n",
      "Step: 60, train/grad_norm: 2.8614513874053955\n",
      "Step: 60, train/learning_rate: 4.9500831664772704e-05\n",
      "Step: 60, train/epoch: 0.09983360767364502\n",
      "Step: 70, train/loss: 0.6446999907493591\n",
      "Step: 70, train/grad_norm: 1.377232313156128\n",
      "Step: 70, train/learning_rate: 4.941763836541213e-05\n",
      "Step: 70, train/epoch: 0.11647254228591919\n",
      "Step: 80, train/loss: 0.6754999756813049\n",
      "Step: 80, train/grad_norm: 1.123705267906189\n",
      "Step: 80, train/learning_rate: 4.933444142807275e-05\n",
      "Step: 80, train/epoch: 0.13311147689819336\n",
      "Step: 90, train/loss: 0.6636000275611877\n",
      "Step: 90, train/grad_norm: 2.7782630920410156\n",
      "Step: 90, train/learning_rate: 4.925124812871218e-05\n",
      "Step: 90, train/epoch: 0.14975041151046753\n",
      "Step: 100, train/loss: 0.6744999885559082\n",
      "Step: 100, train/grad_norm: 1.7479772567749023\n",
      "Step: 100, train/learning_rate: 4.9168054829351604e-05\n",
      "Step: 100, train/epoch: 0.1663893461227417\n",
      "Step: 110, train/loss: 0.6869999766349792\n",
      "Step: 110, train/grad_norm: 1.8480315208435059\n",
      "Step: 110, train/learning_rate: 4.9084857892012224e-05\n",
      "Step: 110, train/epoch: 0.18302828073501587\n",
      "Step: 120, train/loss: 0.6574000120162964\n",
      "Step: 120, train/grad_norm: 3.317185401916504\n",
      "Step: 120, train/learning_rate: 4.900166459265165e-05\n",
      "Step: 120, train/epoch: 0.19966721534729004\n",
      "Step: 130, train/loss: 0.6240000128746033\n",
      "Step: 130, train/grad_norm: 2.3918399810791016\n",
      "Step: 130, train/learning_rate: 4.891846765531227e-05\n",
      "Step: 130, train/epoch: 0.2163061499595642\n",
      "Step: 140, train/loss: 0.5946000218391418\n",
      "Step: 140, train/grad_norm: 1.388005018234253\n",
      "Step: 140, train/learning_rate: 4.88352743559517e-05\n",
      "Step: 140, train/epoch: 0.23294508457183838\n",
      "Step: 150, train/loss: 0.6473000049591064\n",
      "Step: 150, train/grad_norm: 5.758300304412842\n",
      "Step: 150, train/learning_rate: 4.875208105659112e-05\n",
      "Step: 150, train/epoch: 0.24958401918411255\n",
      "Step: 160, train/loss: 0.6193000078201294\n",
      "Step: 160, train/grad_norm: 3.262904167175293\n",
      "Step: 160, train/learning_rate: 4.866888411925174e-05\n",
      "Step: 160, train/epoch: 0.2662229537963867\n",
      "Step: 170, train/loss: 0.6212999820709229\n",
      "Step: 170, train/grad_norm: 3.0228705406188965\n",
      "Step: 170, train/learning_rate: 4.858569081989117e-05\n",
      "Step: 170, train/epoch: 0.2828618884086609\n",
      "Step: 180, train/loss: 0.6022999882698059\n",
      "Step: 180, train/grad_norm: 1.8837312459945679\n",
      "Step: 180, train/learning_rate: 4.8502497520530596e-05\n",
      "Step: 180, train/epoch: 0.29950082302093506\n",
      "Step: 190, train/loss: 0.5796999931335449\n",
      "Step: 190, train/grad_norm: 5.2912116050720215\n",
      "Step: 190, train/learning_rate: 4.8419300583191216e-05\n",
      "Step: 190, train/epoch: 0.31613975763320923\n",
      "Step: 200, train/loss: 0.548799991607666\n",
      "Step: 200, train/grad_norm: 3.983778715133667\n",
      "Step: 200, train/learning_rate: 4.833610728383064e-05\n",
      "Step: 200, train/epoch: 0.3327786922454834\n",
      "Step: 210, train/loss: 0.5270000100135803\n",
      "Step: 210, train/grad_norm: 1.972450613975525\n",
      "Step: 210, train/learning_rate: 4.825291034649126e-05\n",
      "Step: 210, train/epoch: 0.34941762685775757\n",
      "Step: 220, train/loss: 0.5579000115394592\n",
      "Step: 220, train/grad_norm: 6.894137859344482\n",
      "Step: 220, train/learning_rate: 4.816971704713069e-05\n",
      "Step: 220, train/epoch: 0.36605656147003174\n",
      "Step: 230, train/loss: 0.567300021648407\n",
      "Step: 230, train/grad_norm: 8.14639949798584\n",
      "Step: 230, train/learning_rate: 4.8086523747770116e-05\n",
      "Step: 230, train/epoch: 0.3826954960823059\n",
      "Step: 240, train/loss: 0.5329999923706055\n",
      "Step: 240, train/grad_norm: 2.607151985168457\n",
      "Step: 240, train/learning_rate: 4.8003326810430735e-05\n",
      "Step: 240, train/epoch: 0.3993344306945801\n",
      "Step: 250, train/loss: 0.5364000201225281\n",
      "Step: 250, train/grad_norm: 5.844500541687012\n",
      "Step: 250, train/learning_rate: 4.792013351107016e-05\n",
      "Step: 250, train/epoch: 0.41597336530685425\n",
      "Step: 260, train/loss: 0.5403000116348267\n",
      "Step: 260, train/grad_norm: 6.682040691375732\n",
      "Step: 260, train/learning_rate: 4.783694021170959e-05\n",
      "Step: 260, train/epoch: 0.4326122999191284\n",
      "Step: 270, train/loss: 0.5509999990463257\n",
      "Step: 270, train/grad_norm: 8.219292640686035\n",
      "Step: 270, train/learning_rate: 4.775374327437021e-05\n",
      "Step: 270, train/epoch: 0.4492512345314026\n",
      "Step: 280, train/loss: 0.4984999895095825\n",
      "Step: 280, train/grad_norm: 9.326696395874023\n",
      "Step: 280, train/learning_rate: 4.7670549975009635e-05\n",
      "Step: 280, train/epoch: 0.46589016914367676\n",
      "Step: 290, train/loss: 0.5856999754905701\n",
      "Step: 290, train/grad_norm: 1.9240909814834595\n",
      "Step: 290, train/learning_rate: 4.7587353037670255e-05\n",
      "Step: 290, train/epoch: 0.4825291037559509\n",
      "Step: 300, train/loss: 0.49300000071525574\n",
      "Step: 300, train/grad_norm: 6.854426860809326\n",
      "Step: 300, train/learning_rate: 4.750415973830968e-05\n",
      "Step: 300, train/epoch: 0.4991680383682251\n",
      "Step: 310, train/loss: 0.5350000262260437\n",
      "Step: 310, train/grad_norm: 7.291975498199463\n",
      "Step: 310, train/learning_rate: 4.742096643894911e-05\n",
      "Step: 310, train/epoch: 0.5158069729804993\n",
      "Step: 320, train/loss: 0.5085999965667725\n",
      "Step: 320, train/grad_norm: 2.4094696044921875\n",
      "Step: 320, train/learning_rate: 4.733776950160973e-05\n",
      "Step: 320, train/epoch: 0.5324459075927734\n",
      "Step: 330, train/loss: 0.46700000762939453\n",
      "Step: 330, train/grad_norm: 2.428995132446289\n",
      "Step: 330, train/learning_rate: 4.7254576202249154e-05\n",
      "Step: 330, train/epoch: 0.5490848422050476\n",
      "Step: 340, train/loss: 0.46129998564720154\n",
      "Step: 340, train/grad_norm: 2.4497103691101074\n",
      "Step: 340, train/learning_rate: 4.7171379264909774e-05\n",
      "Step: 340, train/epoch: 0.5657237768173218\n",
      "Step: 350, train/loss: 0.46320000290870667\n",
      "Step: 350, train/grad_norm: 4.0042877197265625\n",
      "Step: 350, train/learning_rate: 4.70881859655492e-05\n",
      "Step: 350, train/epoch: 0.582362711429596\n",
      "Step: 360, train/loss: 0.4927999973297119\n",
      "Step: 360, train/grad_norm: 6.185581684112549\n",
      "Step: 360, train/learning_rate: 4.700499266618863e-05\n",
      "Step: 360, train/epoch: 0.5990016460418701\n",
      "Step: 370, train/loss: 0.5170999765396118\n",
      "Step: 370, train/grad_norm: 5.576723098754883\n",
      "Step: 370, train/learning_rate: 4.692179572884925e-05\n",
      "Step: 370, train/epoch: 0.6156405806541443\n",
      "Step: 380, train/loss: 0.526199996471405\n",
      "Step: 380, train/grad_norm: 4.562114238739014\n",
      "Step: 380, train/learning_rate: 4.6838602429488674e-05\n",
      "Step: 380, train/epoch: 0.6322795152664185\n",
      "Step: 390, train/loss: 0.5192999839782715\n",
      "Step: 390, train/grad_norm: 12.347758293151855\n",
      "Step: 390, train/learning_rate: 4.67554091301281e-05\n",
      "Step: 390, train/epoch: 0.6489184498786926\n",
      "Step: 400, train/loss: 0.5015000104904175\n",
      "Step: 400, train/grad_norm: 5.818699836730957\n",
      "Step: 400, train/learning_rate: 4.667221219278872e-05\n",
      "Step: 400, train/epoch: 0.6655573844909668\n",
      "Step: 410, train/loss: 0.49470001459121704\n",
      "Step: 410, train/grad_norm: 3.6467785835266113\n",
      "Step: 410, train/learning_rate: 4.658901889342815e-05\n",
      "Step: 410, train/epoch: 0.682196319103241\n",
      "Step: 420, train/loss: 0.43869999051094055\n",
      "Step: 420, train/grad_norm: 3.929943084716797\n",
      "Step: 420, train/learning_rate: 4.6505821956088766e-05\n",
      "Step: 420, train/epoch: 0.6988352537155151\n",
      "Step: 430, train/loss: 0.5235000252723694\n",
      "Step: 430, train/grad_norm: 4.123644828796387\n",
      "Step: 430, train/learning_rate: 4.642262865672819e-05\n",
      "Step: 430, train/epoch: 0.7154741883277893\n",
      "Step: 440, train/loss: 0.41429999470710754\n",
      "Step: 440, train/grad_norm: 4.635606288909912\n",
      "Step: 440, train/learning_rate: 4.633943535736762e-05\n",
      "Step: 440, train/epoch: 0.7321131229400635\n",
      "Step: 450, train/loss: 0.4797999858856201\n",
      "Step: 450, train/grad_norm: 3.488635301589966\n",
      "Step: 450, train/learning_rate: 4.625623842002824e-05\n",
      "Step: 450, train/epoch: 0.7487520575523376\n",
      "Step: 460, train/loss: 0.47049999237060547\n",
      "Step: 460, train/grad_norm: 3.0297932624816895\n",
      "Step: 460, train/learning_rate: 4.6173045120667666e-05\n",
      "Step: 460, train/epoch: 0.7653909921646118\n",
      "Step: 470, train/loss: 0.4246000051498413\n",
      "Step: 470, train/grad_norm: 5.666871070861816\n",
      "Step: 470, train/learning_rate: 4.608985182130709e-05\n",
      "Step: 470, train/epoch: 0.782029926776886\n",
      "Step: 480, train/loss: 0.4426000118255615\n",
      "Step: 480, train/grad_norm: 7.730681896209717\n",
      "Step: 480, train/learning_rate: 4.600665488396771e-05\n",
      "Step: 480, train/epoch: 0.7986688613891602\n",
      "Step: 490, train/loss: 0.5331000089645386\n",
      "Step: 490, train/grad_norm: 9.953740119934082\n",
      "Step: 490, train/learning_rate: 4.592346158460714e-05\n",
      "Step: 490, train/epoch: 0.8153077960014343\n",
      "Step: 500, train/loss: 0.4966000020503998\n",
      "Step: 500, train/grad_norm: 5.756446361541748\n",
      "Step: 500, train/learning_rate: 4.584026464726776e-05\n",
      "Step: 500, train/epoch: 0.8319467306137085\n",
      "Step: 510, train/loss: 0.48019999265670776\n",
      "Step: 510, train/grad_norm: 11.543074607849121\n",
      "Step: 510, train/learning_rate: 4.5757071347907186e-05\n",
      "Step: 510, train/epoch: 0.8485856652259827\n",
      "Step: 520, train/loss: 0.4496999979019165\n",
      "Step: 520, train/grad_norm: 10.226043701171875\n",
      "Step: 520, train/learning_rate: 4.567387804854661e-05\n",
      "Step: 520, train/epoch: 0.8652245998382568\n",
      "Step: 530, train/loss: 0.42480000853538513\n",
      "Step: 530, train/grad_norm: 2.7341785430908203\n",
      "Step: 530, train/learning_rate: 4.559068111120723e-05\n",
      "Step: 530, train/epoch: 0.881863534450531\n",
      "Step: 540, train/loss: 0.42289999127388\n",
      "Step: 540, train/grad_norm: 3.9111311435699463\n",
      "Step: 540, train/learning_rate: 4.550748781184666e-05\n",
      "Step: 540, train/epoch: 0.8985024690628052\n",
      "Step: 550, train/loss: 0.4108999967575073\n",
      "Step: 550, train/grad_norm: 4.3234639167785645\n",
      "Step: 550, train/learning_rate: 4.5424294512486085e-05\n",
      "Step: 550, train/epoch: 0.9151414036750793\n",
      "Step: 560, train/loss: 0.43720000982284546\n",
      "Step: 560, train/grad_norm: 9.199363708496094\n",
      "Step: 560, train/learning_rate: 4.5341097575146705e-05\n",
      "Step: 560, train/epoch: 0.9317803382873535\n",
      "Step: 570, train/loss: 0.41839998960494995\n",
      "Step: 570, train/grad_norm: 10.760007858276367\n",
      "Step: 570, train/learning_rate: 4.525790427578613e-05\n",
      "Step: 570, train/epoch: 0.9484192728996277\n",
      "Step: 580, train/loss: 0.44510000944137573\n",
      "Step: 580, train/grad_norm: 13.261907577514648\n",
      "Step: 580, train/learning_rate: 4.517470733844675e-05\n",
      "Step: 580, train/epoch: 0.9650582075119019\n",
      "Step: 590, train/loss: 0.4311000108718872\n",
      "Step: 590, train/grad_norm: 3.2945849895477295\n",
      "Step: 590, train/learning_rate: 4.509151403908618e-05\n",
      "Step: 590, train/epoch: 0.981697142124176\n",
      "Step: 600, train/loss: 0.44449999928474426\n",
      "Step: 600, train/grad_norm: 11.549757957458496\n",
      "Step: 600, train/learning_rate: 4.5008320739725605e-05\n",
      "Step: 600, train/epoch: 0.9983360767364502\n",
      "Step: 601, eval/loss: 0.5200570225715637\n",
      "Step: 601, eval/accuracy: 0.7463557124137878\n",
      "Step: 601, eval/f1: 0.7459381818771362\n",
      "Step: 601, eval/runtime: 28.75200080871582\n",
      "Step: 601, eval/samples_per_second: 250.52200317382812\n",
      "Step: 601, eval/steps_per_second: 4.486999988555908\n",
      "Step: 601, train/epoch: 1.0\n",
      "Step: 610, train/loss: 0.3962000012397766\n",
      "Step: 610, train/grad_norm: 19.987430572509766\n",
      "Step: 610, train/learning_rate: 4.4925123802386224e-05\n",
      "Step: 610, train/epoch: 1.0149750709533691\n",
      "Step: 620, train/loss: 0.35100001096725464\n",
      "Step: 620, train/grad_norm: 5.854929447174072\n",
      "Step: 620, train/learning_rate: 4.484193050302565e-05\n",
      "Step: 620, train/epoch: 1.0316139459609985\n",
      "Step: 630, train/loss: 0.27480000257492065\n",
      "Step: 630, train/grad_norm: 3.3956687450408936\n",
      "Step: 630, train/learning_rate: 4.475873720366508e-05\n",
      "Step: 630, train/epoch: 1.0482529401779175\n",
      "Step: 640, train/loss: 0.24269999563694\n",
      "Step: 640, train/grad_norm: 14.134696960449219\n",
      "Step: 640, train/learning_rate: 4.46755402663257e-05\n",
      "Step: 640, train/epoch: 1.0648918151855469\n",
      "Step: 650, train/loss: 0.3612000048160553\n",
      "Step: 650, train/grad_norm: 6.17233943939209\n",
      "Step: 650, train/learning_rate: 4.4592346966965124e-05\n",
      "Step: 650, train/epoch: 1.0815308094024658\n",
      "Step: 660, train/loss: 0.31700000166893005\n",
      "Step: 660, train/grad_norm: 3.487478494644165\n",
      "Step: 660, train/learning_rate: 4.4509150029625744e-05\n",
      "Step: 660, train/epoch: 1.0981696844100952\n",
      "Step: 670, train/loss: 0.36739999055862427\n",
      "Step: 670, train/grad_norm: 12.836215019226074\n",
      "Step: 670, train/learning_rate: 4.442595673026517e-05\n",
      "Step: 670, train/epoch: 1.1148086786270142\n",
      "Step: 680, train/loss: 0.3808000087738037\n",
      "Step: 680, train/grad_norm: 4.643500804901123\n",
      "Step: 680, train/learning_rate: 4.43427634309046e-05\n",
      "Step: 680, train/epoch: 1.1314475536346436\n",
      "Step: 690, train/loss: 0.3353999853134155\n",
      "Step: 690, train/grad_norm: 3.059281826019287\n",
      "Step: 690, train/learning_rate: 4.425956649356522e-05\n",
      "Step: 690, train/epoch: 1.1480865478515625\n",
      "Step: 700, train/loss: 0.33899998664855957\n",
      "Step: 700, train/grad_norm: 7.361673355102539\n",
      "Step: 700, train/learning_rate: 4.417637319420464e-05\n",
      "Step: 700, train/epoch: 1.164725422859192\n",
      "Step: 710, train/loss: 0.3635999858379364\n",
      "Step: 710, train/grad_norm: 4.8121137619018555\n",
      "Step: 710, train/learning_rate: 4.409317625686526e-05\n",
      "Step: 710, train/epoch: 1.1813644170761108\n",
      "Step: 720, train/loss: 0.3050999939441681\n",
      "Step: 720, train/grad_norm: 4.900282859802246\n",
      "Step: 720, train/learning_rate: 4.400998295750469e-05\n",
      "Step: 720, train/epoch: 1.1980032920837402\n",
      "Step: 730, train/loss: 0.3027999997138977\n",
      "Step: 730, train/grad_norm: 10.191965103149414\n",
      "Step: 730, train/learning_rate: 4.3926789658144116e-05\n",
      "Step: 730, train/epoch: 1.2146422863006592\n",
      "Step: 740, train/loss: 0.3061000108718872\n",
      "Step: 740, train/grad_norm: 4.7323102951049805\n",
      "Step: 740, train/learning_rate: 4.3843592720804736e-05\n",
      "Step: 740, train/epoch: 1.2312811613082886\n",
      "Step: 750, train/loss: 0.36419999599456787\n",
      "Step: 750, train/grad_norm: 6.00881290435791\n",
      "Step: 750, train/learning_rate: 4.376039942144416e-05\n",
      "Step: 750, train/epoch: 1.2479201555252075\n",
      "Step: 760, train/loss: 0.33169999718666077\n",
      "Step: 760, train/grad_norm: 14.19477653503418\n",
      "Step: 760, train/learning_rate: 4.367720612208359e-05\n",
      "Step: 760, train/epoch: 1.264559030532837\n",
      "Step: 770, train/loss: 0.3736000061035156\n",
      "Step: 770, train/grad_norm: 17.354942321777344\n",
      "Step: 770, train/learning_rate: 4.359400918474421e-05\n",
      "Step: 770, train/epoch: 1.2811980247497559\n",
      "Step: 780, train/loss: 0.3531000018119812\n",
      "Step: 780, train/grad_norm: 9.503819465637207\n",
      "Step: 780, train/learning_rate: 4.3510815885383636e-05\n",
      "Step: 780, train/epoch: 1.2978368997573853\n",
      "Step: 790, train/loss: 0.36149999499320984\n",
      "Step: 790, train/grad_norm: 14.06697940826416\n",
      "Step: 790, train/learning_rate: 4.3427618948044255e-05\n",
      "Step: 790, train/epoch: 1.3144758939743042\n",
      "Step: 800, train/loss: 0.3375000059604645\n",
      "Step: 800, train/grad_norm: 7.653768539428711\n",
      "Step: 800, train/learning_rate: 4.334442564868368e-05\n",
      "Step: 800, train/epoch: 1.3311147689819336\n",
      "Step: 810, train/loss: 0.2777000069618225\n",
      "Step: 810, train/grad_norm: 8.306804656982422\n",
      "Step: 810, train/learning_rate: 4.326123234932311e-05\n",
      "Step: 810, train/epoch: 1.3477537631988525\n",
      "Step: 820, train/loss: 0.3249000012874603\n",
      "Step: 820, train/grad_norm: 9.872391700744629\n",
      "Step: 820, train/learning_rate: 4.317803541198373e-05\n",
      "Step: 820, train/epoch: 1.364392638206482\n",
      "Step: 830, train/loss: 0.3037000000476837\n",
      "Step: 830, train/grad_norm: 12.147192001342773\n",
      "Step: 830, train/learning_rate: 4.3094842112623155e-05\n",
      "Step: 830, train/epoch: 1.3810316324234009\n",
      "Step: 840, train/loss: 0.3589000105857849\n",
      "Step: 840, train/grad_norm: 7.918014049530029\n",
      "Step: 840, train/learning_rate: 4.301164881326258e-05\n",
      "Step: 840, train/epoch: 1.3976705074310303\n",
      "Step: 850, train/loss: 0.30309998989105225\n",
      "Step: 850, train/grad_norm: 3.26359224319458\n",
      "Step: 850, train/learning_rate: 4.29284518759232e-05\n",
      "Step: 850, train/epoch: 1.4143095016479492\n",
      "Step: 860, train/loss: 0.34040001034736633\n",
      "Step: 860, train/grad_norm: 14.669902801513672\n",
      "Step: 860, train/learning_rate: 4.284525857656263e-05\n",
      "Step: 860, train/epoch: 1.4309483766555786\n",
      "Step: 870, train/loss: 0.4553000032901764\n",
      "Step: 870, train/grad_norm: 10.989775657653809\n",
      "Step: 870, train/learning_rate: 4.276206163922325e-05\n",
      "Step: 870, train/epoch: 1.4475873708724976\n",
      "Step: 880, train/loss: 0.4097000062465668\n",
      "Step: 880, train/grad_norm: 6.95725679397583\n",
      "Step: 880, train/learning_rate: 4.2678868339862674e-05\n",
      "Step: 880, train/epoch: 1.464226245880127\n",
      "Step: 890, train/loss: 0.3625999987125397\n",
      "Step: 890, train/grad_norm: 11.912299156188965\n",
      "Step: 890, train/learning_rate: 4.25956750405021e-05\n",
      "Step: 890, train/epoch: 1.480865240097046\n",
      "Step: 900, train/loss: 0.3125\n",
      "Step: 900, train/grad_norm: 4.988384246826172\n",
      "Step: 900, train/learning_rate: 4.251247810316272e-05\n",
      "Step: 900, train/epoch: 1.4975041151046753\n",
      "Step: 910, train/loss: 0.34700000286102295\n",
      "Step: 910, train/grad_norm: 9.651971817016602\n",
      "Step: 910, train/learning_rate: 4.242928480380215e-05\n",
      "Step: 910, train/epoch: 1.5141431093215942\n",
      "Step: 920, train/loss: 0.28790000081062317\n",
      "Step: 920, train/grad_norm: 8.35229206085205\n",
      "Step: 920, train/learning_rate: 4.2346091504441574e-05\n",
      "Step: 920, train/epoch: 1.5307819843292236\n",
      "Step: 930, train/loss: 0.29409998655319214\n",
      "Step: 930, train/grad_norm: 8.313508033752441\n",
      "Step: 930, train/learning_rate: 4.2262894567102194e-05\n",
      "Step: 930, train/epoch: 1.5474209785461426\n",
      "Step: 940, train/loss: 0.29089999198913574\n",
      "Step: 940, train/grad_norm: 9.262248992919922\n",
      "Step: 940, train/learning_rate: 4.217970126774162e-05\n",
      "Step: 940, train/epoch: 1.564059853553772\n",
      "Step: 950, train/loss: 0.32850000262260437\n",
      "Step: 950, train/grad_norm: 5.153429985046387\n",
      "Step: 950, train/learning_rate: 4.209650433040224e-05\n",
      "Step: 950, train/epoch: 1.580698847770691\n",
      "Step: 960, train/loss: 0.367000013589859\n",
      "Step: 960, train/grad_norm: 11.793573379516602\n",
      "Step: 960, train/learning_rate: 4.201331103104167e-05\n",
      "Step: 960, train/epoch: 1.5973377227783203\n",
      "Step: 970, train/loss: 0.43070000410079956\n",
      "Step: 970, train/grad_norm: 6.716194152832031\n",
      "Step: 970, train/learning_rate: 4.1930117731681094e-05\n",
      "Step: 970, train/epoch: 1.6139767169952393\n",
      "Step: 980, train/loss: 0.31839999556541443\n",
      "Step: 980, train/grad_norm: 3.731161117553711\n",
      "Step: 980, train/learning_rate: 4.184692079434171e-05\n",
      "Step: 980, train/epoch: 1.6306155920028687\n",
      "Step: 990, train/loss: 0.2632000148296356\n",
      "Step: 990, train/grad_norm: 6.453796863555908\n",
      "Step: 990, train/learning_rate: 4.176372749498114e-05\n",
      "Step: 990, train/epoch: 1.6472545862197876\n",
      "Step: 1000, train/loss: 0.33009999990463257\n",
      "Step: 1000, train/grad_norm: 10.395400047302246\n",
      "Step: 1000, train/learning_rate: 4.1680534195620567e-05\n",
      "Step: 1000, train/epoch: 1.663893461227417\n",
      "Step: 1010, train/loss: 0.29600000381469727\n",
      "Step: 1010, train/grad_norm: 12.713682174682617\n",
      "Step: 1010, train/learning_rate: 4.1597337258281186e-05\n",
      "Step: 1010, train/epoch: 1.680532455444336\n",
      "Step: 1020, train/loss: 0.2913999855518341\n",
      "Step: 1020, train/grad_norm: 8.809394836425781\n",
      "Step: 1020, train/learning_rate: 4.151414395892061e-05\n",
      "Step: 1020, train/epoch: 1.6971713304519653\n",
      "Step: 1030, train/loss: 0.31299999356269836\n",
      "Step: 1030, train/grad_norm: 6.846783638000488\n",
      "Step: 1030, train/learning_rate: 4.143094702158123e-05\n",
      "Step: 1030, train/epoch: 1.7138103246688843\n",
      "Step: 1040, train/loss: 0.32850000262260437\n",
      "Step: 1040, train/grad_norm: 7.388290882110596\n",
      "Step: 1040, train/learning_rate: 4.134775372222066e-05\n",
      "Step: 1040, train/epoch: 1.7304491996765137\n",
      "Step: 1050, train/loss: 0.3594000041484833\n",
      "Step: 1050, train/grad_norm: 3.246487617492676\n",
      "Step: 1050, train/learning_rate: 4.1264560422860086e-05\n",
      "Step: 1050, train/epoch: 1.7470881938934326\n",
      "Step: 1060, train/loss: 0.33899998664855957\n",
      "Step: 1060, train/grad_norm: 12.869612693786621\n",
      "Step: 1060, train/learning_rate: 4.1181363485520706e-05\n",
      "Step: 1060, train/epoch: 1.763727068901062\n",
      "Step: 1070, train/loss: 0.29350000619888306\n",
      "Step: 1070, train/grad_norm: 7.617642879486084\n",
      "Step: 1070, train/learning_rate: 4.109817018616013e-05\n",
      "Step: 1070, train/epoch: 1.780366063117981\n",
      "Step: 1080, train/loss: 0.3346000015735626\n",
      "Step: 1080, train/grad_norm: 11.976567268371582\n",
      "Step: 1080, train/learning_rate: 4.101497324882075e-05\n",
      "Step: 1080, train/epoch: 1.7970049381256104\n",
      "Step: 1090, train/loss: 0.3221000134944916\n",
      "Step: 1090, train/grad_norm: 7.09283971786499\n",
      "Step: 1090, train/learning_rate: 4.093177994946018e-05\n",
      "Step: 1090, train/epoch: 1.8136439323425293\n",
      "Step: 1100, train/loss: 0.3003000020980835\n",
      "Step: 1100, train/grad_norm: 6.996138572692871\n",
      "Step: 1100, train/learning_rate: 4.0848586650099605e-05\n",
      "Step: 1100, train/epoch: 1.8302828073501587\n",
      "Step: 1110, train/loss: 0.28029999136924744\n",
      "Step: 1110, train/grad_norm: 7.369574069976807\n",
      "Step: 1110, train/learning_rate: 4.0765389712760225e-05\n",
      "Step: 1110, train/epoch: 1.8469218015670776\n",
      "Step: 1120, train/loss: 0.36149999499320984\n",
      "Step: 1120, train/grad_norm: 7.985232353210449\n",
      "Step: 1120, train/learning_rate: 4.068219641339965e-05\n",
      "Step: 1120, train/epoch: 1.863560676574707\n",
      "Step: 1130, train/loss: 0.30469998717308044\n",
      "Step: 1130, train/grad_norm: 3.3689255714416504\n",
      "Step: 1130, train/learning_rate: 4.059900311403908e-05\n",
      "Step: 1130, train/epoch: 1.880199670791626\n",
      "Step: 1140, train/loss: 0.31450000405311584\n",
      "Step: 1140, train/grad_norm: 7.673782825469971\n",
      "Step: 1140, train/learning_rate: 4.05158061766997e-05\n",
      "Step: 1140, train/epoch: 1.8968385457992554\n",
      "Step: 1150, train/loss: 0.3560999929904938\n",
      "Step: 1150, train/grad_norm: 13.618118286132812\n",
      "Step: 1150, train/learning_rate: 4.0432612877339125e-05\n",
      "Step: 1150, train/epoch: 1.9134775400161743\n",
      "Step: 1160, train/loss: 0.290800005197525\n",
      "Step: 1160, train/grad_norm: 8.245072364807129\n",
      "Step: 1160, train/learning_rate: 4.0349415939999744e-05\n",
      "Step: 1160, train/epoch: 1.9301164150238037\n",
      "Step: 1170, train/loss: 0.35010001063346863\n",
      "Step: 1170, train/grad_norm: 11.657934188842773\n",
      "Step: 1170, train/learning_rate: 4.026622264063917e-05\n",
      "Step: 1170, train/epoch: 1.9467554092407227\n",
      "Step: 1180, train/loss: 0.33730000257492065\n",
      "Step: 1180, train/grad_norm: 3.9060513973236084\n",
      "Step: 1180, train/learning_rate: 4.01830293412786e-05\n",
      "Step: 1180, train/epoch: 1.963394284248352\n",
      "Step: 1190, train/loss: 0.2741999924182892\n",
      "Step: 1190, train/grad_norm: 3.4983646869659424\n",
      "Step: 1190, train/learning_rate: 4.009983240393922e-05\n",
      "Step: 1190, train/epoch: 1.980033278465271\n",
      "Step: 1200, train/loss: 0.3264000117778778\n",
      "Step: 1200, train/grad_norm: 3.927152395248413\n",
      "Step: 1200, train/learning_rate: 4.0016639104578644e-05\n",
      "Step: 1200, train/epoch: 1.9966721534729004\n",
      "Step: 1202, eval/loss: 0.42219990491867065\n",
      "Step: 1202, eval/accuracy: 0.8211855888366699\n",
      "Step: 1202, eval/f1: 0.8168118000030518\n",
      "Step: 1202, eval/runtime: 28.758100509643555\n",
      "Step: 1202, eval/samples_per_second: 250.46800231933594\n",
      "Step: 1202, eval/steps_per_second: 4.486000061035156\n",
      "Step: 1202, train/epoch: 2.0\n",
      "Step: 1210, train/loss: 0.24120000004768372\n",
      "Step: 1210, train/grad_norm: 8.831942558288574\n",
      "Step: 1210, train/learning_rate: 3.993344580521807e-05\n",
      "Step: 1210, train/epoch: 2.0133111476898193\n",
      "Step: 1220, train/loss: 0.1468999981880188\n",
      "Step: 1220, train/grad_norm: 7.638895034790039\n",
      "Step: 1220, train/learning_rate: 3.985024886787869e-05\n",
      "Step: 1220, train/epoch: 2.0299501419067383\n",
      "Step: 1230, train/loss: 0.19859999418258667\n",
      "Step: 1230, train/grad_norm: 11.298166275024414\n",
      "Step: 1230, train/learning_rate: 3.976705556851812e-05\n",
      "Step: 1230, train/epoch: 2.0465891361236572\n",
      "Step: 1240, train/loss: 0.20890000462532043\n",
      "Step: 1240, train/grad_norm: 6.111940860748291\n",
      "Step: 1240, train/learning_rate: 3.968385863117874e-05\n",
      "Step: 1240, train/epoch: 2.063227891921997\n",
      "Step: 1250, train/loss: 0.15000000596046448\n",
      "Step: 1250, train/grad_norm: 8.644292831420898\n",
      "Step: 1250, train/learning_rate: 3.9600665331818163e-05\n",
      "Step: 1250, train/epoch: 2.079866886138916\n",
      "Step: 1260, train/loss: 0.13300000131130219\n",
      "Step: 1260, train/grad_norm: 3.254742383956909\n",
      "Step: 1260, train/learning_rate: 3.951747203245759e-05\n",
      "Step: 1260, train/epoch: 2.096505880355835\n",
      "Step: 1270, train/loss: 0.2003999948501587\n",
      "Step: 1270, train/grad_norm: 6.7539801597595215\n",
      "Step: 1270, train/learning_rate: 3.943427509511821e-05\n",
      "Step: 1270, train/epoch: 2.113144874572754\n",
      "Step: 1280, train/loss: 0.19300000369548798\n",
      "Step: 1280, train/grad_norm: 4.97575044631958\n",
      "Step: 1280, train/learning_rate: 3.9351081795757636e-05\n",
      "Step: 1280, train/epoch: 2.1297836303710938\n",
      "Step: 1290, train/loss: 0.13729999959468842\n",
      "Step: 1290, train/grad_norm: 5.078775405883789\n",
      "Step: 1290, train/learning_rate: 3.926788849639706e-05\n",
      "Step: 1290, train/epoch: 2.1464226245880127\n",
      "Step: 1300, train/loss: 0.13330000638961792\n",
      "Step: 1300, train/grad_norm: 4.133499622344971\n",
      "Step: 1300, train/learning_rate: 3.918469155905768e-05\n",
      "Step: 1300, train/epoch: 2.1630616188049316\n",
      "Step: 1310, train/loss: 0.1265999972820282\n",
      "Step: 1310, train/grad_norm: 5.3044304847717285\n",
      "Step: 1310, train/learning_rate: 3.910149825969711e-05\n",
      "Step: 1310, train/epoch: 2.1797006130218506\n",
      "Step: 1320, train/loss: 0.2037000060081482\n",
      "Step: 1320, train/grad_norm: 4.099651336669922\n",
      "Step: 1320, train/learning_rate: 3.901830132235773e-05\n",
      "Step: 1320, train/epoch: 2.1963393688201904\n",
      "Step: 1330, train/loss: 0.16699999570846558\n",
      "Step: 1330, train/grad_norm: 4.9163947105407715\n",
      "Step: 1330, train/learning_rate: 3.8935108022997156e-05\n",
      "Step: 1330, train/epoch: 2.2129783630371094\n",
      "Step: 1340, train/loss: 0.14550000429153442\n",
      "Step: 1340, train/grad_norm: 5.952419757843018\n",
      "Step: 1340, train/learning_rate: 3.885191472363658e-05\n",
      "Step: 1340, train/epoch: 2.2296173572540283\n",
      "Step: 1350, train/loss: 0.225600004196167\n",
      "Step: 1350, train/grad_norm: 11.697648048400879\n",
      "Step: 1350, train/learning_rate: 3.87687177862972e-05\n",
      "Step: 1350, train/epoch: 2.2462563514709473\n",
      "Step: 1360, train/loss: 0.16940000653266907\n",
      "Step: 1360, train/grad_norm: 11.33061695098877\n",
      "Step: 1360, train/learning_rate: 3.868552448693663e-05\n",
      "Step: 1360, train/epoch: 2.262895107269287\n",
      "Step: 1370, train/loss: 0.16869999468326569\n",
      "Step: 1370, train/grad_norm: 4.162022113800049\n",
      "Step: 1370, train/learning_rate: 3.8602331187576056e-05\n",
      "Step: 1370, train/epoch: 2.279534101486206\n",
      "Step: 1380, train/loss: 0.1543000042438507\n",
      "Step: 1380, train/grad_norm: 4.047764778137207\n",
      "Step: 1380, train/learning_rate: 3.8519134250236675e-05\n",
      "Step: 1380, train/epoch: 2.296173095703125\n",
      "Step: 1390, train/loss: 0.14990000426769257\n",
      "Step: 1390, train/grad_norm: 8.887873649597168\n",
      "Step: 1390, train/learning_rate: 3.84359409508761e-05\n",
      "Step: 1390, train/epoch: 2.312812089920044\n",
      "Step: 1400, train/loss: 0.1379999965429306\n",
      "Step: 1400, train/grad_norm: 10.256150245666504\n",
      "Step: 1400, train/learning_rate: 3.835274401353672e-05\n",
      "Step: 1400, train/epoch: 2.329450845718384\n",
      "Step: 1410, train/loss: 0.16680000722408295\n",
      "Step: 1410, train/grad_norm: 7.473659515380859\n",
      "Step: 1410, train/learning_rate: 3.826955071417615e-05\n",
      "Step: 1410, train/epoch: 2.3460898399353027\n",
      "Step: 1420, train/loss: 0.16899999976158142\n",
      "Step: 1420, train/grad_norm: 5.637631416320801\n",
      "Step: 1420, train/learning_rate: 3.8186357414815575e-05\n",
      "Step: 1420, train/epoch: 2.3627288341522217\n",
      "Step: 1430, train/loss: 0.15000000596046448\n",
      "Step: 1430, train/grad_norm: 3.4681997299194336\n",
      "Step: 1430, train/learning_rate: 3.8103160477476195e-05\n",
      "Step: 1430, train/epoch: 2.3793678283691406\n",
      "Step: 1440, train/loss: 0.20080000162124634\n",
      "Step: 1440, train/grad_norm: 10.708002090454102\n",
      "Step: 1440, train/learning_rate: 3.801996717811562e-05\n",
      "Step: 1440, train/epoch: 2.3960065841674805\n",
      "Step: 1450, train/loss: 0.16699999570846558\n",
      "Step: 1450, train/grad_norm: 6.174710273742676\n",
      "Step: 1450, train/learning_rate: 3.793677024077624e-05\n",
      "Step: 1450, train/epoch: 2.4126455783843994\n",
      "Step: 1460, train/loss: 0.16920000314712524\n",
      "Step: 1460, train/grad_norm: 16.35976219177246\n",
      "Step: 1460, train/learning_rate: 3.785357694141567e-05\n",
      "Step: 1460, train/epoch: 2.4292845726013184\n",
      "Step: 1470, train/loss: 0.19920000433921814\n",
      "Step: 1470, train/grad_norm: 7.133225917816162\n",
      "Step: 1470, train/learning_rate: 3.7770383642055094e-05\n",
      "Step: 1470, train/epoch: 2.4459235668182373\n",
      "Step: 1480, train/loss: 0.19949999451637268\n",
      "Step: 1480, train/grad_norm: 2.642878532409668\n",
      "Step: 1480, train/learning_rate: 3.7687186704715714e-05\n",
      "Step: 1480, train/epoch: 2.462562322616577\n",
      "Step: 1490, train/loss: 0.13199999928474426\n",
      "Step: 1490, train/grad_norm: 9.313392639160156\n",
      "Step: 1490, train/learning_rate: 3.760399340535514e-05\n",
      "Step: 1490, train/epoch: 2.479201316833496\n",
      "Step: 1500, train/loss: 0.14419999718666077\n",
      "Step: 1500, train/grad_norm: 1.750160574913025\n",
      "Step: 1500, train/learning_rate: 3.752080010599457e-05\n",
      "Step: 1500, train/epoch: 2.495840311050415\n",
      "Step: 1510, train/loss: 0.19789999723434448\n",
      "Step: 1510, train/grad_norm: 13.193190574645996\n",
      "Step: 1510, train/learning_rate: 3.743760316865519e-05\n",
      "Step: 1510, train/epoch: 2.512479305267334\n",
      "Step: 1520, train/loss: 0.17219999432563782\n",
      "Step: 1520, train/grad_norm: 6.436738967895508\n",
      "Step: 1520, train/learning_rate: 3.7354409869294614e-05\n",
      "Step: 1520, train/epoch: 2.529118061065674\n",
      "Step: 1530, train/loss: 0.14409999549388885\n",
      "Step: 1530, train/grad_norm: 8.982389450073242\n",
      "Step: 1530, train/learning_rate: 3.727121293195523e-05\n",
      "Step: 1530, train/epoch: 2.5457570552825928\n",
      "Step: 1540, train/loss: 0.20110000669956207\n",
      "Step: 1540, train/grad_norm: 11.154569625854492\n",
      "Step: 1540, train/learning_rate: 3.718801963259466e-05\n",
      "Step: 1540, train/epoch: 2.5623960494995117\n",
      "Step: 1550, train/loss: 0.18719999492168427\n",
      "Step: 1550, train/grad_norm: 5.420919418334961\n",
      "Step: 1550, train/learning_rate: 3.710482633323409e-05\n",
      "Step: 1550, train/epoch: 2.5790350437164307\n",
      "Step: 1560, train/loss: 0.13189999759197235\n",
      "Step: 1560, train/grad_norm: 2.2719295024871826\n",
      "Step: 1560, train/learning_rate: 3.7021629395894706e-05\n",
      "Step: 1560, train/epoch: 2.5956737995147705\n",
      "Step: 1570, train/loss: 0.14429999887943268\n",
      "Step: 1570, train/grad_norm: 11.581006050109863\n",
      "Step: 1570, train/learning_rate: 3.693843609653413e-05\n",
      "Step: 1570, train/epoch: 2.6123127937316895\n",
      "Step: 1580, train/loss: 0.19979999959468842\n",
      "Step: 1580, train/grad_norm: 11.72584056854248\n",
      "Step: 1580, train/learning_rate: 3.685524279717356e-05\n",
      "Step: 1580, train/epoch: 2.6289517879486084\n",
      "Step: 1590, train/loss: 0.13269999623298645\n",
      "Step: 1590, train/grad_norm: 11.606797218322754\n",
      "Step: 1590, train/learning_rate: 3.677204585983418e-05\n",
      "Step: 1590, train/epoch: 2.6455907821655273\n",
      "Step: 1600, train/loss: 0.18639999628067017\n",
      "Step: 1600, train/grad_norm: 18.243982315063477\n",
      "Step: 1600, train/learning_rate: 3.6688852560473606e-05\n",
      "Step: 1600, train/epoch: 2.662229537963867\n",
      "Step: 1610, train/loss: 0.1761000007390976\n",
      "Step: 1610, train/grad_norm: 5.64182186126709\n",
      "Step: 1610, train/learning_rate: 3.6605655623134226e-05\n",
      "Step: 1610, train/epoch: 2.678868532180786\n",
      "Step: 1620, train/loss: 0.19740000367164612\n",
      "Step: 1620, train/grad_norm: 5.099708557128906\n",
      "Step: 1620, train/learning_rate: 3.652246232377365e-05\n",
      "Step: 1620, train/epoch: 2.695507526397705\n",
      "Step: 1630, train/loss: 0.16339999437332153\n",
      "Step: 1630, train/grad_norm: 4.300509452819824\n",
      "Step: 1630, train/learning_rate: 3.643926902441308e-05\n",
      "Step: 1630, train/epoch: 2.712146520614624\n",
      "Step: 1640, train/loss: 0.19449999928474426\n",
      "Step: 1640, train/grad_norm: 8.377557754516602\n",
      "Step: 1640, train/learning_rate: 3.63560720870737e-05\n",
      "Step: 1640, train/epoch: 2.728785276412964\n",
      "Step: 1650, train/loss: 0.15279999375343323\n",
      "Step: 1650, train/grad_norm: 4.1452813148498535\n",
      "Step: 1650, train/learning_rate: 3.6272878787713125e-05\n",
      "Step: 1650, train/epoch: 2.745424270629883\n",
      "Step: 1660, train/loss: 0.16249999403953552\n",
      "Step: 1660, train/grad_norm: 9.169508934020996\n",
      "Step: 1660, train/learning_rate: 3.618968548835255e-05\n",
      "Step: 1660, train/epoch: 2.7620632648468018\n",
      "Step: 1670, train/loss: 0.12409999966621399\n",
      "Step: 1670, train/grad_norm: 13.140326499938965\n",
      "Step: 1670, train/learning_rate: 3.610648855101317e-05\n",
      "Step: 1670, train/epoch: 2.7787022590637207\n",
      "Step: 1680, train/loss: 0.13819999992847443\n",
      "Step: 1680, train/grad_norm: 5.398627281188965\n",
      "Step: 1680, train/learning_rate: 3.60232952516526e-05\n",
      "Step: 1680, train/epoch: 2.7953410148620605\n",
      "Step: 1690, train/loss: 0.14139999449253082\n",
      "Step: 1690, train/grad_norm: 12.491791725158691\n",
      "Step: 1690, train/learning_rate: 3.594009831431322e-05\n",
      "Step: 1690, train/epoch: 2.8119800090789795\n",
      "Step: 1700, train/loss: 0.21619999408721924\n",
      "Step: 1700, train/grad_norm: 8.51540756225586\n",
      "Step: 1700, train/learning_rate: 3.5856905014952645e-05\n",
      "Step: 1700, train/epoch: 2.8286190032958984\n",
      "Step: 1710, train/loss: 0.1356000006198883\n",
      "Step: 1710, train/grad_norm: 2.804327964782715\n",
      "Step: 1710, train/learning_rate: 3.577371171559207e-05\n",
      "Step: 1710, train/epoch: 2.8452579975128174\n",
      "Step: 1720, train/loss: 0.11339999735355377\n",
      "Step: 1720, train/grad_norm: 10.131047248840332\n",
      "Step: 1720, train/learning_rate: 3.569051477825269e-05\n",
      "Step: 1720, train/epoch: 2.8618967533111572\n",
      "Step: 1730, train/loss: 0.21089999377727509\n",
      "Step: 1730, train/grad_norm: 11.532632827758789\n",
      "Step: 1730, train/learning_rate: 3.560732147889212e-05\n",
      "Step: 1730, train/epoch: 2.878535747528076\n",
      "Step: 1740, train/loss: 0.17110000550746918\n",
      "Step: 1740, train/grad_norm: 10.750153541564941\n",
      "Step: 1740, train/learning_rate: 3.5524128179531544e-05\n",
      "Step: 1740, train/epoch: 2.895174741744995\n",
      "Step: 1750, train/loss: 0.2563000023365021\n",
      "Step: 1750, train/grad_norm: 8.946566581726074\n",
      "Step: 1750, train/learning_rate: 3.5440931242192164e-05\n",
      "Step: 1750, train/epoch: 2.911813735961914\n",
      "Step: 1760, train/loss: 0.12070000171661377\n",
      "Step: 1760, train/grad_norm: 2.9121344089508057\n",
      "Step: 1760, train/learning_rate: 3.535773794283159e-05\n",
      "Step: 1760, train/epoch: 2.928452491760254\n",
      "Step: 1770, train/loss: 0.15700000524520874\n",
      "Step: 1770, train/grad_norm: 7.564540863037109\n",
      "Step: 1770, train/learning_rate: 3.527454100549221e-05\n",
      "Step: 1770, train/epoch: 2.945091485977173\n",
      "Step: 1780, train/loss: 0.2142000049352646\n",
      "Step: 1780, train/grad_norm: 12.321362495422363\n",
      "Step: 1780, train/learning_rate: 3.519134770613164e-05\n",
      "Step: 1780, train/epoch: 2.961730480194092\n",
      "Step: 1790, train/loss: 0.19920000433921814\n",
      "Step: 1790, train/grad_norm: 11.605929374694824\n",
      "Step: 1790, train/learning_rate: 3.5108154406771064e-05\n",
      "Step: 1790, train/epoch: 2.9783694744110107\n",
      "Step: 1800, train/loss: 0.16830000281333923\n",
      "Step: 1800, train/grad_norm: 3.7121832370758057\n",
      "Step: 1800, train/learning_rate: 3.5024957469431683e-05\n",
      "Step: 1800, train/epoch: 2.9950082302093506\n",
      "Step: 1803, eval/loss: 0.4873616695404053\n",
      "Step: 1803, eval/accuracy: 0.8272941708564758\n",
      "Step: 1803, eval/f1: 0.8053397536277771\n",
      "Step: 1803, eval/runtime: 28.677499771118164\n",
      "Step: 1803, eval/samples_per_second: 251.17300415039062\n",
      "Step: 1803, eval/steps_per_second: 4.498000144958496\n",
      "Step: 1803, train/epoch: 3.0\n",
      "Step: 1810, train/loss: 0.1738000065088272\n",
      "Step: 1810, train/grad_norm: 31.57224464416504\n",
      "Step: 1810, train/learning_rate: 3.494176417007111e-05\n",
      "Step: 1810, train/epoch: 3.0116472244262695\n",
      "Step: 1820, train/loss: 0.14880000054836273\n",
      "Step: 1820, train/grad_norm: 1.915030837059021\n",
      "Step: 1820, train/learning_rate: 3.485856723273173e-05\n",
      "Step: 1820, train/epoch: 3.0282862186431885\n",
      "Step: 1830, train/loss: 0.12359999865293503\n",
      "Step: 1830, train/grad_norm: 11.460773468017578\n",
      "Step: 1830, train/learning_rate: 3.4775373933371156e-05\n",
      "Step: 1830, train/epoch: 3.0449252128601074\n",
      "Step: 1840, train/loss: 0.0544000007212162\n",
      "Step: 1840, train/grad_norm: 7.304795742034912\n",
      "Step: 1840, train/learning_rate: 3.469218063401058e-05\n",
      "Step: 1840, train/epoch: 3.0615639686584473\n",
      "Step: 1850, train/loss: 0.039500001817941666\n",
      "Step: 1850, train/grad_norm: 9.171396255493164\n",
      "Step: 1850, train/learning_rate: 3.46089836966712e-05\n",
      "Step: 1850, train/epoch: 3.078202962875366\n",
      "Step: 1860, train/loss: 0.06360000371932983\n",
      "Step: 1860, train/grad_norm: 0.6439471244812012\n",
      "Step: 1860, train/learning_rate: 3.452579039731063e-05\n",
      "Step: 1860, train/epoch: 3.094841957092285\n",
      "Step: 1870, train/loss: 0.1606999933719635\n",
      "Step: 1870, train/grad_norm: 36.18551254272461\n",
      "Step: 1870, train/learning_rate: 3.4442597097950056e-05\n",
      "Step: 1870, train/epoch: 3.111480951309204\n",
      "Step: 1880, train/loss: 0.22699999809265137\n",
      "Step: 1880, train/grad_norm: 11.09181022644043\n",
      "Step: 1880, train/learning_rate: 3.4359400160610676e-05\n",
      "Step: 1880, train/epoch: 3.128119707107544\n",
      "Step: 1890, train/loss: 0.1014999970793724\n",
      "Step: 1890, train/grad_norm: 3.6894495487213135\n",
      "Step: 1890, train/learning_rate: 3.42762068612501e-05\n",
      "Step: 1890, train/epoch: 3.144758701324463\n",
      "Step: 1900, train/loss: 0.07540000230073929\n",
      "Step: 1900, train/grad_norm: 6.551487922668457\n",
      "Step: 1900, train/learning_rate: 3.419300992391072e-05\n",
      "Step: 1900, train/epoch: 3.161397695541382\n",
      "Step: 1910, train/loss: 0.07930000126361847\n",
      "Step: 1910, train/grad_norm: 15.184182167053223\n",
      "Step: 1910, train/learning_rate: 3.410981662455015e-05\n",
      "Step: 1910, train/epoch: 3.178036689758301\n",
      "Step: 1920, train/loss: 0.06809999793767929\n",
      "Step: 1920, train/grad_norm: 4.2934746742248535\n",
      "Step: 1920, train/learning_rate: 3.4026623325189576e-05\n",
      "Step: 1920, train/epoch: 3.1946754455566406\n",
      "Step: 1930, train/loss: 0.05119999870657921\n",
      "Step: 1930, train/grad_norm: 4.721426963806152\n",
      "Step: 1930, train/learning_rate: 3.3943426387850195e-05\n",
      "Step: 1930, train/epoch: 3.2113144397735596\n",
      "Step: 1940, train/loss: 0.09549999982118607\n",
      "Step: 1940, train/grad_norm: 5.218594551086426\n",
      "Step: 1940, train/learning_rate: 3.386023308848962e-05\n",
      "Step: 1940, train/epoch: 3.2279534339904785\n",
      "Step: 1950, train/loss: 0.09380000084638596\n",
      "Step: 1950, train/grad_norm: 3.790128469467163\n",
      "Step: 1950, train/learning_rate: 3.377703978912905e-05\n",
      "Step: 1950, train/epoch: 3.2445924282073975\n",
      "Step: 1960, train/loss: 0.04399999976158142\n",
      "Step: 1960, train/grad_norm: 8.500609397888184\n",
      "Step: 1960, train/learning_rate: 3.369384285178967e-05\n",
      "Step: 1960, train/epoch: 3.2612311840057373\n",
      "Step: 1970, train/loss: 0.07989999651908875\n",
      "Step: 1970, train/grad_norm: 15.924421310424805\n",
      "Step: 1970, train/learning_rate: 3.3610649552429095e-05\n",
      "Step: 1970, train/epoch: 3.2778701782226562\n",
      "Step: 1980, train/loss: 0.10109999775886536\n",
      "Step: 1980, train/grad_norm: 9.052703857421875\n",
      "Step: 1980, train/learning_rate: 3.3527452615089715e-05\n",
      "Step: 1980, train/epoch: 3.294509172439575\n",
      "Step: 1990, train/loss: 0.07419999688863754\n",
      "Step: 1990, train/grad_norm: 9.654245376586914\n",
      "Step: 1990, train/learning_rate: 3.344425931572914e-05\n",
      "Step: 1990, train/epoch: 3.311148166656494\n",
      "Step: 2000, train/loss: 0.042100001126527786\n",
      "Step: 2000, train/grad_norm: 4.246302604675293\n",
      "Step: 2000, train/learning_rate: 3.336106601636857e-05\n",
      "Step: 2000, train/epoch: 3.327786922454834\n",
      "Step: 2010, train/loss: 0.07850000262260437\n",
      "Step: 2010, train/grad_norm: 9.858084678649902\n",
      "Step: 2010, train/learning_rate: 3.327786907902919e-05\n",
      "Step: 2010, train/epoch: 3.344425916671753\n",
      "Step: 2020, train/loss: 0.0997999981045723\n",
      "Step: 2020, train/grad_norm: 3.7550573348999023\n",
      "Step: 2020, train/learning_rate: 3.3194675779668614e-05\n",
      "Step: 2020, train/epoch: 3.361064910888672\n",
      "Step: 2030, train/loss: 0.047600001096725464\n",
      "Step: 2030, train/grad_norm: 10.280046463012695\n",
      "Step: 2030, train/learning_rate: 3.311148248030804e-05\n",
      "Step: 2030, train/epoch: 3.377703905105591\n",
      "Step: 2040, train/loss: 0.07840000092983246\n",
      "Step: 2040, train/grad_norm: 9.45359992980957\n",
      "Step: 2040, train/learning_rate: 3.302828554296866e-05\n",
      "Step: 2040, train/epoch: 3.3943426609039307\n",
      "Step: 2050, train/loss: 0.05770000070333481\n",
      "Step: 2050, train/grad_norm: 5.253108024597168\n",
      "Step: 2050, train/learning_rate: 3.294509224360809e-05\n",
      "Step: 2050, train/epoch: 3.4109816551208496\n",
      "Step: 2060, train/loss: 0.08749999850988388\n",
      "Step: 2060, train/grad_norm: 8.6118803024292\n",
      "Step: 2060, train/learning_rate: 3.286189530626871e-05\n",
      "Step: 2060, train/epoch: 3.4276206493377686\n",
      "Step: 2070, train/loss: 0.09210000187158585\n",
      "Step: 2070, train/grad_norm: 13.405488967895508\n",
      "Step: 2070, train/learning_rate: 3.2778702006908134e-05\n",
      "Step: 2070, train/epoch: 3.4442596435546875\n",
      "Step: 2080, train/loss: 0.06400000303983688\n",
      "Step: 2080, train/grad_norm: 5.104753494262695\n",
      "Step: 2080, train/learning_rate: 3.269550870754756e-05\n",
      "Step: 2080, train/epoch: 3.4608983993530273\n",
      "Step: 2090, train/loss: 0.11999999731779099\n",
      "Step: 2090, train/grad_norm: 24.138099670410156\n",
      "Step: 2090, train/learning_rate: 3.261231177020818e-05\n",
      "Step: 2090, train/epoch: 3.4775373935699463\n",
      "Step: 2100, train/loss: 0.09539999812841415\n",
      "Step: 2100, train/grad_norm: 14.482955932617188\n",
      "Step: 2100, train/learning_rate: 3.252911847084761e-05\n",
      "Step: 2100, train/epoch: 3.4941763877868652\n",
      "Step: 2110, train/loss: 0.10080000013113022\n",
      "Step: 2110, train/grad_norm: 6.080683708190918\n",
      "Step: 2110, train/learning_rate: 3.244592517148703e-05\n",
      "Step: 2110, train/epoch: 3.510815382003784\n",
      "Step: 2120, train/loss: 0.12880000472068787\n",
      "Step: 2120, train/grad_norm: 19.127553939819336\n",
      "Step: 2120, train/learning_rate: 3.236272823414765e-05\n",
      "Step: 2120, train/epoch: 3.527454137802124\n",
      "Step: 2130, train/loss: 0.10769999772310257\n",
      "Step: 2130, train/grad_norm: 5.321906089782715\n",
      "Step: 2130, train/learning_rate: 3.227953493478708e-05\n",
      "Step: 2130, train/epoch: 3.544093132019043\n",
      "Step: 2140, train/loss: 0.05530000105500221\n",
      "Step: 2140, train/grad_norm: 3.909623861312866\n",
      "Step: 2140, train/learning_rate: 3.21963379974477e-05\n",
      "Step: 2140, train/epoch: 3.560732126235962\n",
      "Step: 2150, train/loss: 0.07419999688863754\n",
      "Step: 2150, train/grad_norm: 6.900656223297119\n",
      "Step: 2150, train/learning_rate: 3.2113144698087126e-05\n",
      "Step: 2150, train/epoch: 3.577371120452881\n",
      "Step: 2160, train/loss: 0.04769999906420708\n",
      "Step: 2160, train/grad_norm: 0.8804935216903687\n",
      "Step: 2160, train/learning_rate: 3.202995139872655e-05\n",
      "Step: 2160, train/epoch: 3.5940098762512207\n",
      "Step: 2170, train/loss: 0.07609999924898148\n",
      "Step: 2170, train/grad_norm: 5.9806694984436035\n",
      "Step: 2170, train/learning_rate: 3.194675446138717e-05\n",
      "Step: 2170, train/epoch: 3.6106488704681396\n",
      "Step: 2180, train/loss: 0.12210000306367874\n",
      "Step: 2180, train/grad_norm: 13.798452377319336\n",
      "Step: 2180, train/learning_rate: 3.18635611620266e-05\n",
      "Step: 2180, train/epoch: 3.6272878646850586\n",
      "Step: 2190, train/loss: 0.08630000054836273\n",
      "Step: 2190, train/grad_norm: 12.694903373718262\n",
      "Step: 2190, train/learning_rate: 3.1780367862666026e-05\n",
      "Step: 2190, train/epoch: 3.6439268589019775\n",
      "Step: 2200, train/loss: 0.09430000185966492\n",
      "Step: 2200, train/grad_norm: 11.856966018676758\n",
      "Step: 2200, train/learning_rate: 3.1697170925326645e-05\n",
      "Step: 2200, train/epoch: 3.6605656147003174\n",
      "Step: 2210, train/loss: 0.060499999672174454\n",
      "Step: 2210, train/grad_norm: 10.82177734375\n",
      "Step: 2210, train/learning_rate: 3.161397762596607e-05\n",
      "Step: 2210, train/epoch: 3.6772046089172363\n",
      "Step: 2220, train/loss: 0.08919999748468399\n",
      "Step: 2220, train/grad_norm: 5.936234951019287\n",
      "Step: 2220, train/learning_rate: 3.153078068862669e-05\n",
      "Step: 2220, train/epoch: 3.6938436031341553\n",
      "Step: 2230, train/loss: 0.08709999918937683\n",
      "Step: 2230, train/grad_norm: 1.3803023099899292\n",
      "Step: 2230, train/learning_rate: 3.144758738926612e-05\n",
      "Step: 2230, train/epoch: 3.710482597351074\n",
      "Step: 2240, train/loss: 0.07419999688863754\n",
      "Step: 2240, train/grad_norm: 6.883105754852295\n",
      "Step: 2240, train/learning_rate: 3.1364394089905545e-05\n",
      "Step: 2240, train/epoch: 3.727121353149414\n",
      "Step: 2250, train/loss: 0.12049999833106995\n",
      "Step: 2250, train/grad_norm: 13.984737396240234\n",
      "Step: 2250, train/learning_rate: 3.1281197152566165e-05\n",
      "Step: 2250, train/epoch: 3.743760347366333\n",
      "Step: 2260, train/loss: 0.09669999778270721\n",
      "Step: 2260, train/grad_norm: 2.3087239265441895\n",
      "Step: 2260, train/learning_rate: 3.119800385320559e-05\n",
      "Step: 2260, train/epoch: 3.760399341583252\n",
      "Step: 2270, train/loss: 0.08659999817609787\n",
      "Step: 2270, train/grad_norm: 10.53843879699707\n",
      "Step: 2270, train/learning_rate: 3.111480691586621e-05\n",
      "Step: 2270, train/epoch: 3.777038335800171\n",
      "Step: 2280, train/loss: 0.11640000343322754\n",
      "Step: 2280, train/grad_norm: 5.9289116859436035\n",
      "Step: 2280, train/learning_rate: 3.103161361650564e-05\n",
      "Step: 2280, train/epoch: 3.7936770915985107\n",
      "Step: 2290, train/loss: 0.0731000006198883\n",
      "Step: 2290, train/grad_norm: 2.109600305557251\n",
      "Step: 2290, train/learning_rate: 3.0948420317145064e-05\n",
      "Step: 2290, train/epoch: 3.8103160858154297\n",
      "Step: 2300, train/loss: 0.08299999684095383\n",
      "Step: 2300, train/grad_norm: 5.248340129852295\n",
      "Step: 2300, train/learning_rate: 3.0865223379805684e-05\n",
      "Step: 2300, train/epoch: 3.8269550800323486\n",
      "Step: 2310, train/loss: 0.09669999778270721\n",
      "Step: 2310, train/grad_norm: 28.904373168945312\n",
      "Step: 2310, train/learning_rate: 3.078203008044511e-05\n",
      "Step: 2310, train/epoch: 3.8435940742492676\n",
      "Step: 2320, train/loss: 0.07850000262260437\n",
      "Step: 2320, train/grad_norm: 6.544432163238525\n",
      "Step: 2320, train/learning_rate: 3.069883678108454e-05\n",
      "Step: 2320, train/epoch: 3.8602328300476074\n",
      "Step: 2330, train/loss: 0.06610000133514404\n",
      "Step: 2330, train/grad_norm: 16.184890747070312\n",
      "Step: 2330, train/learning_rate: 3.061563984374516e-05\n",
      "Step: 2330, train/epoch: 3.8768718242645264\n",
      "Step: 2340, train/loss: 0.05860000103712082\n",
      "Step: 2340, train/grad_norm: 6.480727672576904\n",
      "Step: 2340, train/learning_rate: 3.0532446544384584e-05\n",
      "Step: 2340, train/epoch: 3.8935108184814453\n",
      "Step: 2350, train/loss: 0.07880000025033951\n",
      "Step: 2350, train/grad_norm: 11.86993408203125\n",
      "Step: 2350, train/learning_rate: 3.0449251426034607e-05\n",
      "Step: 2350, train/epoch: 3.9101498126983643\n",
      "Step: 2360, train/loss: 0.11230000108480453\n",
      "Step: 2360, train/grad_norm: 10.293923377990723\n",
      "Step: 2360, train/learning_rate: 3.036605630768463e-05\n",
      "Step: 2360, train/epoch: 3.926788568496704\n",
      "Step: 2370, train/loss: 0.12939999997615814\n",
      "Step: 2370, train/grad_norm: 8.856117248535156\n",
      "Step: 2370, train/learning_rate: 3.0282861189334653e-05\n",
      "Step: 2370, train/epoch: 3.943427562713623\n",
      "Step: 2380, train/loss: 0.09189999848604202\n",
      "Step: 2380, train/grad_norm: 7.89570951461792\n",
      "Step: 2380, train/learning_rate: 3.019966788997408e-05\n",
      "Step: 2380, train/epoch: 3.960066556930542\n",
      "Step: 2390, train/loss: 0.07540000230073929\n",
      "Step: 2390, train/grad_norm: 2.4963221549987793\n",
      "Step: 2390, train/learning_rate: 3.0116472771624103e-05\n",
      "Step: 2390, train/epoch: 3.976705551147461\n",
      "Step: 2400, train/loss: 0.039799999445676804\n",
      "Step: 2400, train/grad_norm: 8.578962326049805\n",
      "Step: 2400, train/learning_rate: 3.0033277653274126e-05\n",
      "Step: 2400, train/epoch: 3.993344306945801\n",
      "Step: 2404, eval/loss: 0.725667417049408\n",
      "Step: 2404, eval/accuracy: 0.8297931551933289\n",
      "Step: 2404, eval/f1: 0.823924720287323\n",
      "Step: 2404, eval/runtime: 28.918899536132812\n",
      "Step: 2404, eval/samples_per_second: 249.0760040283203\n",
      "Step: 2404, eval/steps_per_second: 4.460999965667725\n",
      "Step: 2404, train/epoch: 4.0\n",
      "Step: 2410, train/loss: 0.04749999940395355\n",
      "Step: 2410, train/grad_norm: 1.7095019817352295\n",
      "Step: 2410, train/learning_rate: 2.995008253492415e-05\n",
      "Step: 2410, train/epoch: 4.009983539581299\n",
      "Step: 2420, train/loss: 0.03240000084042549\n",
      "Step: 2420, train/grad_norm: 0.13494771718978882\n",
      "Step: 2420, train/learning_rate: 2.9866889235563576e-05\n",
      "Step: 2420, train/epoch: 4.026622295379639\n",
      "Step: 2430, train/loss: 0.019999999552965164\n",
      "Step: 2430, train/grad_norm: 16.481887817382812\n",
      "Step: 2430, train/learning_rate: 2.97836941172136e-05\n",
      "Step: 2430, train/epoch: 4.0432610511779785\n",
      "Step: 2440, train/loss: 0.012500000186264515\n",
      "Step: 2440, train/grad_norm: 0.86507648229599\n",
      "Step: 2440, train/learning_rate: 2.9700498998863623e-05\n",
      "Step: 2440, train/epoch: 4.059900283813477\n",
      "Step: 2450, train/loss: 0.032999999821186066\n",
      "Step: 2450, train/grad_norm: 0.8198374509811401\n",
      "Step: 2450, train/learning_rate: 2.9617303880513646e-05\n",
      "Step: 2450, train/epoch: 4.076539039611816\n",
      "Step: 2460, train/loss: 0.13920000195503235\n",
      "Step: 2460, train/grad_norm: 18.623388290405273\n",
      "Step: 2460, train/learning_rate: 2.9534110581153072e-05\n",
      "Step: 2460, train/epoch: 4.0931782722473145\n",
      "Step: 2470, train/loss: 0.1460999995470047\n",
      "Step: 2470, train/grad_norm: 6.796301364898682\n",
      "Step: 2470, train/learning_rate: 2.9450915462803096e-05\n",
      "Step: 2470, train/epoch: 4.109817028045654\n",
      "Step: 2480, train/loss: 0.030799999833106995\n",
      "Step: 2480, train/grad_norm: 3.2569894790649414\n",
      "Step: 2480, train/learning_rate: 2.936772034445312e-05\n",
      "Step: 2480, train/epoch: 4.126455783843994\n",
      "Step: 2490, train/loss: 0.14560000598430634\n",
      "Step: 2490, train/grad_norm: 2.8261611461639404\n",
      "Step: 2490, train/learning_rate: 2.9284525226103142e-05\n",
      "Step: 2490, train/epoch: 4.143095016479492\n",
      "Step: 2500, train/loss: 0.05570000037550926\n",
      "Step: 2500, train/grad_norm: 6.840677738189697\n",
      "Step: 2500, train/learning_rate: 2.920133192674257e-05\n",
      "Step: 2500, train/epoch: 4.159733772277832\n",
      "Step: 2510, train/loss: 0.04600000008940697\n",
      "Step: 2510, train/grad_norm: 21.800857543945312\n",
      "Step: 2510, train/learning_rate: 2.9118136808392592e-05\n",
      "Step: 2510, train/epoch: 4.176372528076172\n",
      "Step: 2520, train/loss: 0.04170000180602074\n",
      "Step: 2520, train/grad_norm: 14.775078773498535\n",
      "Step: 2520, train/learning_rate: 2.9034941690042615e-05\n",
      "Step: 2520, train/epoch: 4.19301176071167\n",
      "Step: 2530, train/loss: 0.05480000004172325\n",
      "Step: 2530, train/grad_norm: 10.835260391235352\n",
      "Step: 2530, train/learning_rate: 2.8951746571692638e-05\n",
      "Step: 2530, train/epoch: 4.20965051651001\n",
      "Step: 2540, train/loss: 0.031700000166893005\n",
      "Step: 2540, train/grad_norm: 1.4467339515686035\n",
      "Step: 2540, train/learning_rate: 2.8868553272332065e-05\n",
      "Step: 2540, train/epoch: 4.226289749145508\n",
      "Step: 2550, train/loss: 0.02800000086426735\n",
      "Step: 2550, train/grad_norm: 0.2153049260377884\n",
      "Step: 2550, train/learning_rate: 2.8785358153982088e-05\n",
      "Step: 2550, train/epoch: 4.242928504943848\n",
      "Step: 2560, train/loss: 0.048700001090765\n",
      "Step: 2560, train/grad_norm: 9.676236152648926\n",
      "Step: 2560, train/learning_rate: 2.870216303563211e-05\n",
      "Step: 2560, train/epoch: 4.2595672607421875\n",
      "Step: 2570, train/loss: 0.05660000070929527\n",
      "Step: 2570, train/grad_norm: 8.93057918548584\n",
      "Step: 2570, train/learning_rate: 2.8618967917282134e-05\n",
      "Step: 2570, train/epoch: 4.2762064933776855\n",
      "Step: 2580, train/loss: 0.04089999943971634\n",
      "Step: 2580, train/grad_norm: 3.1561756134033203\n",
      "Step: 2580, train/learning_rate: 2.853577461792156e-05\n",
      "Step: 2580, train/epoch: 4.292845249176025\n",
      "Step: 2590, train/loss: 0.019500000402331352\n",
      "Step: 2590, train/grad_norm: 2.806549549102783\n",
      "Step: 2590, train/learning_rate: 2.8452579499571584e-05\n",
      "Step: 2590, train/epoch: 4.309484004974365\n",
      "Step: 2600, train/loss: 0.03099999949336052\n",
      "Step: 2600, train/grad_norm: 0.6389641165733337\n",
      "Step: 2600, train/learning_rate: 2.8369384381221607e-05\n",
      "Step: 2600, train/epoch: 4.326123237609863\n",
      "Step: 2610, train/loss: 0.027000000700354576\n",
      "Step: 2610, train/grad_norm: 7.693833351135254\n",
      "Step: 2610, train/learning_rate: 2.828618926287163e-05\n",
      "Step: 2610, train/epoch: 4.342761993408203\n",
      "Step: 2620, train/loss: 0.021700000390410423\n",
      "Step: 2620, train/grad_norm: 19.696996688842773\n",
      "Step: 2620, train/learning_rate: 2.8202994144521654e-05\n",
      "Step: 2620, train/epoch: 4.359401226043701\n",
      "Step: 2630, train/loss: 0.04360000044107437\n",
      "Step: 2630, train/grad_norm: 18.65670394897461\n",
      "Step: 2630, train/learning_rate: 2.811980084516108e-05\n",
      "Step: 2630, train/epoch: 4.376039981842041\n",
      "Step: 2640, train/loss: 0.11410000175237656\n",
      "Step: 2640, train/grad_norm: 8.609086036682129\n",
      "Step: 2640, train/learning_rate: 2.8036605726811104e-05\n",
      "Step: 2640, train/epoch: 4.392678737640381\n",
      "Step: 2650, train/loss: 0.1324000060558319\n",
      "Step: 2650, train/grad_norm: 0.03651546686887741\n",
      "Step: 2650, train/learning_rate: 2.7953410608461127e-05\n",
      "Step: 2650, train/epoch: 4.409317970275879\n",
      "Step: 2660, train/loss: 0.06340000033378601\n",
      "Step: 2660, train/grad_norm: 4.79802942276001\n",
      "Step: 2660, train/learning_rate: 2.787021549011115e-05\n",
      "Step: 2660, train/epoch: 4.425956726074219\n",
      "Step: 2670, train/loss: 0.08500000089406967\n",
      "Step: 2670, train/grad_norm: 5.990357398986816\n",
      "Step: 2670, train/learning_rate: 2.7787022190750577e-05\n",
      "Step: 2670, train/epoch: 4.442595481872559\n",
      "Step: 2680, train/loss: 0.03290000185370445\n",
      "Step: 2680, train/grad_norm: 4.863836288452148\n",
      "Step: 2680, train/learning_rate: 2.77038270724006e-05\n",
      "Step: 2680, train/epoch: 4.459234714508057\n",
      "Step: 2690, train/loss: 0.041600000113248825\n",
      "Step: 2690, train/grad_norm: 0.15112750232219696\n",
      "Step: 2690, train/learning_rate: 2.7620631954050623e-05\n",
      "Step: 2690, train/epoch: 4.4758734703063965\n",
      "Step: 2700, train/loss: 0.028999999165534973\n",
      "Step: 2700, train/grad_norm: 0.30799683928489685\n",
      "Step: 2700, train/learning_rate: 2.7537436835700646e-05\n",
      "Step: 2700, train/epoch: 4.4925127029418945\n",
      "Step: 2710, train/loss: 0.03460000082850456\n",
      "Step: 2710, train/grad_norm: 2.8545081615448\n",
      "Step: 2710, train/learning_rate: 2.7454243536340073e-05\n",
      "Step: 2710, train/epoch: 4.509151458740234\n",
      "Step: 2720, train/loss: 0.026799999177455902\n",
      "Step: 2720, train/grad_norm: 4.684261798858643\n",
      "Step: 2720, train/learning_rate: 2.7371048417990096e-05\n",
      "Step: 2720, train/epoch: 4.525790214538574\n",
      "Step: 2730, train/loss: 0.032999999821186066\n",
      "Step: 2730, train/grad_norm: 20.727094650268555\n",
      "Step: 2730, train/learning_rate: 2.728785329964012e-05\n",
      "Step: 2730, train/epoch: 4.542429447174072\n",
      "Step: 2740, train/loss: 0.04019999876618385\n",
      "Step: 2740, train/grad_norm: 3.6641461849212646\n",
      "Step: 2740, train/learning_rate: 2.7204658181290142e-05\n",
      "Step: 2740, train/epoch: 4.559068202972412\n",
      "Step: 2750, train/loss: 0.03359999880194664\n",
      "Step: 2750, train/grad_norm: 6.256107330322266\n",
      "Step: 2750, train/learning_rate: 2.712146488192957e-05\n",
      "Step: 2750, train/epoch: 4.575706958770752\n",
      "Step: 2760, train/loss: 0.021900000050663948\n",
      "Step: 2760, train/grad_norm: 0.45908311009407043\n",
      "Step: 2760, train/learning_rate: 2.7038269763579592e-05\n",
      "Step: 2760, train/epoch: 4.59234619140625\n",
      "Step: 2770, train/loss: 0.03590000048279762\n",
      "Step: 2770, train/grad_norm: 7.599671363830566\n",
      "Step: 2770, train/learning_rate: 2.6955074645229615e-05\n",
      "Step: 2770, train/epoch: 4.60898494720459\n",
      "Step: 2780, train/loss: 0.08129999786615372\n",
      "Step: 2780, train/grad_norm: 16.69073486328125\n",
      "Step: 2780, train/learning_rate: 2.687187952687964e-05\n",
      "Step: 2780, train/epoch: 4.625624179840088\n",
      "Step: 2790, train/loss: 0.07209999859333038\n",
      "Step: 2790, train/grad_norm: 8.184175491333008\n",
      "Step: 2790, train/learning_rate: 2.6788686227519065e-05\n",
      "Step: 2790, train/epoch: 4.642262935638428\n",
      "Step: 2800, train/loss: 0.05649999901652336\n",
      "Step: 2800, train/grad_norm: 10.034594535827637\n",
      "Step: 2800, train/learning_rate: 2.6705491109169088e-05\n",
      "Step: 2800, train/epoch: 4.658901691436768\n",
      "Step: 2810, train/loss: 0.048700001090765\n",
      "Step: 2810, train/grad_norm: 6.255300045013428\n",
      "Step: 2810, train/learning_rate: 2.662229599081911e-05\n",
      "Step: 2810, train/epoch: 4.675540924072266\n",
      "Step: 2820, train/loss: 0.05339999869465828\n",
      "Step: 2820, train/grad_norm: 8.076961517333984\n",
      "Step: 2820, train/learning_rate: 2.6539100872469135e-05\n",
      "Step: 2820, train/epoch: 4.6921796798706055\n",
      "Step: 2830, train/loss: 0.06360000371932983\n",
      "Step: 2830, train/grad_norm: 6.812539100646973\n",
      "Step: 2830, train/learning_rate: 2.645590757310856e-05\n",
      "Step: 2830, train/epoch: 4.708818435668945\n",
      "Step: 2840, train/loss: 0.03920000046491623\n",
      "Step: 2840, train/grad_norm: 0.4430861175060272\n",
      "Step: 2840, train/learning_rate: 2.6372712454758584e-05\n",
      "Step: 2840, train/epoch: 4.725457668304443\n",
      "Step: 2850, train/loss: 0.020600000396370888\n",
      "Step: 2850, train/grad_norm: 11.28526496887207\n",
      "Step: 2850, train/learning_rate: 2.6289517336408608e-05\n",
      "Step: 2850, train/epoch: 4.742096424102783\n",
      "Step: 2860, train/loss: 0.03680000081658363\n",
      "Step: 2860, train/grad_norm: 0.027227411046624184\n",
      "Step: 2860, train/learning_rate: 2.620632221805863e-05\n",
      "Step: 2860, train/epoch: 4.758735656738281\n",
      "Step: 2870, train/loss: 0.06759999692440033\n",
      "Step: 2870, train/grad_norm: 18.32949447631836\n",
      "Step: 2870, train/learning_rate: 2.6123128918698058e-05\n",
      "Step: 2870, train/epoch: 4.775374412536621\n",
      "Step: 2880, train/loss: 0.06669999659061432\n",
      "Step: 2880, train/grad_norm: 3.179655075073242\n",
      "Step: 2880, train/learning_rate: 2.603993380034808e-05\n",
      "Step: 2880, train/epoch: 4.792013168334961\n",
      "Step: 2890, train/loss: 0.047600001096725464\n",
      "Step: 2890, train/grad_norm: 0.04352052882313728\n",
      "Step: 2890, train/learning_rate: 2.5956738681998104e-05\n",
      "Step: 2890, train/epoch: 4.808652400970459\n",
      "Step: 2900, train/loss: 0.05249999836087227\n",
      "Step: 2900, train/grad_norm: 0.1097140684723854\n",
      "Step: 2900, train/learning_rate: 2.5873543563648127e-05\n",
      "Step: 2900, train/epoch: 4.825291156768799\n",
      "Step: 2910, train/loss: 0.05139999836683273\n",
      "Step: 2910, train/grad_norm: 8.7555570602417\n",
      "Step: 2910, train/learning_rate: 2.5790350264287554e-05\n",
      "Step: 2910, train/epoch: 4.841929912567139\n",
      "Step: 2920, train/loss: 0.047600001096725464\n",
      "Step: 2920, train/grad_norm: 7.349340915679932\n",
      "Step: 2920, train/learning_rate: 2.5707155145937577e-05\n",
      "Step: 2920, train/epoch: 4.858569145202637\n",
      "Step: 2930, train/loss: 0.06129999831318855\n",
      "Step: 2930, train/grad_norm: 26.461332321166992\n",
      "Step: 2930, train/learning_rate: 2.56239600275876e-05\n",
      "Step: 2930, train/epoch: 4.875207901000977\n",
      "Step: 2940, train/loss: 0.09790000319480896\n",
      "Step: 2940, train/grad_norm: 5.923471927642822\n",
      "Step: 2940, train/learning_rate: 2.5540764909237623e-05\n",
      "Step: 2940, train/epoch: 4.891847133636475\n",
      "Step: 2950, train/loss: 0.07940000295639038\n",
      "Step: 2950, train/grad_norm: 8.526649475097656\n",
      "Step: 2950, train/learning_rate: 2.545757160987705e-05\n",
      "Step: 2950, train/epoch: 4.9084858894348145\n",
      "Step: 2960, train/loss: 0.06390000134706497\n",
      "Step: 2960, train/grad_norm: 6.128751277923584\n",
      "Step: 2960, train/learning_rate: 2.5374376491527073e-05\n",
      "Step: 2960, train/epoch: 4.925124645233154\n",
      "Step: 2970, train/loss: 0.04259999841451645\n",
      "Step: 2970, train/grad_norm: 0.6315276622772217\n",
      "Step: 2970, train/learning_rate: 2.5291181373177096e-05\n",
      "Step: 2970, train/epoch: 4.941763877868652\n",
      "Step: 2980, train/loss: 0.06279999762773514\n",
      "Step: 2980, train/grad_norm: 2.046637773513794\n",
      "Step: 2980, train/learning_rate: 2.520798625482712e-05\n",
      "Step: 2980, train/epoch: 4.958402633666992\n",
      "Step: 2990, train/loss: 0.10170000046491623\n",
      "Step: 2990, train/grad_norm: 15.007683753967285\n",
      "Step: 2990, train/learning_rate: 2.5124791136477143e-05\n",
      "Step: 2990, train/epoch: 4.975041389465332\n",
      "Step: 3000, train/loss: 0.07609999924898148\n",
      "Step: 3000, train/grad_norm: 12.705545425415039\n",
      "Step: 3000, train/learning_rate: 2.504159783711657e-05\n",
      "Step: 3000, train/epoch: 4.99168062210083\n",
      "Step: 3005, eval/loss: 0.742036759853363\n",
      "Step: 3005, eval/accuracy: 0.8377065062522888\n",
      "Step: 3005, eval/f1: 0.8234629034996033\n",
      "Step: 3005, eval/runtime: 28.936199188232422\n",
      "Step: 3005, eval/samples_per_second: 248.927001953125\n",
      "Step: 3005, eval/steps_per_second: 4.458000183105469\n",
      "Step: 3005, train/epoch: 5.0\n",
      "Step: 3010, train/loss: 0.02280000038444996\n",
      "Step: 3010, train/grad_norm: 1.1228317022323608\n",
      "Step: 3010, train/learning_rate: 2.4958402718766592e-05\n",
      "Step: 3010, train/epoch: 5.00831937789917\n",
      "Step: 3020, train/loss: 0.053300000727176666\n",
      "Step: 3020, train/grad_norm: 3.0613386631011963\n",
      "Step: 3020, train/learning_rate: 2.4875207600416616e-05\n",
      "Step: 3020, train/epoch: 5.024958610534668\n",
      "Step: 3030, train/loss: 0.03920000046491623\n",
      "Step: 3030, train/grad_norm: 6.771531105041504\n",
      "Step: 3030, train/learning_rate: 2.479201248206664e-05\n",
      "Step: 3030, train/epoch: 5.041597366333008\n",
      "Step: 3040, train/loss: 0.025299999862909317\n",
      "Step: 3040, train/grad_norm: 4.304216384887695\n",
      "Step: 3040, train/learning_rate: 2.4708819182706065e-05\n",
      "Step: 3040, train/epoch: 5.058236122131348\n",
      "Step: 3050, train/loss: 0.012400000356137753\n",
      "Step: 3050, train/grad_norm: 9.576681137084961\n",
      "Step: 3050, train/learning_rate: 2.462562406435609e-05\n",
      "Step: 3050, train/epoch: 5.074875354766846\n",
      "Step: 3060, train/loss: 0.09220000356435776\n",
      "Step: 3060, train/grad_norm: 1.9684547185897827\n",
      "Step: 3060, train/learning_rate: 2.4542428946006112e-05\n",
      "Step: 3060, train/epoch: 5.0915141105651855\n",
      "Step: 3070, train/loss: 0.02019999921321869\n",
      "Step: 3070, train/grad_norm: 17.623302459716797\n",
      "Step: 3070, train/learning_rate: 2.4459233827656135e-05\n",
      "Step: 3070, train/epoch: 5.108152866363525\n",
      "Step: 3080, train/loss: 0.04320000112056732\n",
      "Step: 3080, train/grad_norm: 24.646780014038086\n",
      "Step: 3080, train/learning_rate: 2.437604052829556e-05\n",
      "Step: 3080, train/epoch: 5.124792098999023\n",
      "Step: 3090, train/loss: 0.02930000051856041\n",
      "Step: 3090, train/grad_norm: 12.375238418579102\n",
      "Step: 3090, train/learning_rate: 2.4292845409945585e-05\n",
      "Step: 3090, train/epoch: 5.141430854797363\n",
      "Step: 3100, train/loss: 0.019600000232458115\n",
      "Step: 3100, train/grad_norm: 0.14029811322689056\n",
      "Step: 3100, train/learning_rate: 2.4209650291595608e-05\n",
      "Step: 3100, train/epoch: 5.158070087432861\n",
      "Step: 3110, train/loss: 0.015599999576807022\n",
      "Step: 3110, train/grad_norm: 0.563778281211853\n",
      "Step: 3110, train/learning_rate: 2.412645517324563e-05\n",
      "Step: 3110, train/epoch: 5.174708843231201\n",
      "Step: 3120, train/loss: 0.033900000154972076\n",
      "Step: 3120, train/grad_norm: 15.21788501739502\n",
      "Step: 3120, train/learning_rate: 2.4043261873885058e-05\n",
      "Step: 3120, train/epoch: 5.191347599029541\n",
      "Step: 3130, train/loss: 0.02500000037252903\n",
      "Step: 3130, train/grad_norm: 10.025246620178223\n",
      "Step: 3130, train/learning_rate: 2.396006675553508e-05\n",
      "Step: 3130, train/epoch: 5.207986831665039\n",
      "Step: 3140, train/loss: 0.00430000014603138\n",
      "Step: 3140, train/grad_norm: 0.21078291535377502\n",
      "Step: 3140, train/learning_rate: 2.3876871637185104e-05\n",
      "Step: 3140, train/epoch: 5.224625587463379\n",
      "Step: 3150, train/loss: 0.04410000145435333\n",
      "Step: 3150, train/grad_norm: 0.1735595315694809\n",
      "Step: 3150, train/learning_rate: 2.3793676518835127e-05\n",
      "Step: 3150, train/epoch: 5.241264343261719\n",
      "Step: 3160, train/loss: 0.021900000050663948\n",
      "Step: 3160, train/grad_norm: 10.78060245513916\n",
      "Step: 3160, train/learning_rate: 2.3710483219474554e-05\n",
      "Step: 3160, train/epoch: 5.257903575897217\n",
      "Step: 3170, train/loss: 0.03889999911189079\n",
      "Step: 3170, train/grad_norm: 16.287694931030273\n",
      "Step: 3170, train/learning_rate: 2.3627288101124577e-05\n",
      "Step: 3170, train/epoch: 5.274542331695557\n",
      "Step: 3180, train/loss: 0.0568000003695488\n",
      "Step: 3180, train/grad_norm: 19.914464950561523\n",
      "Step: 3180, train/learning_rate: 2.35440929827746e-05\n",
      "Step: 3180, train/epoch: 5.291181564331055\n",
      "Step: 3190, train/loss: 0.05249999836087227\n",
      "Step: 3190, train/grad_norm: 0.9490501284599304\n",
      "Step: 3190, train/learning_rate: 2.3460897864424624e-05\n",
      "Step: 3190, train/epoch: 5.3078203201293945\n",
      "Step: 3200, train/loss: 0.005900000222027302\n",
      "Step: 3200, train/grad_norm: 0.02584269642829895\n",
      "Step: 3200, train/learning_rate: 2.337770456506405e-05\n",
      "Step: 3200, train/epoch: 5.324459075927734\n",
      "Step: 3210, train/loss: 0.061500001698732376\n",
      "Step: 3210, train/grad_norm: 24.433361053466797\n",
      "Step: 3210, train/learning_rate: 2.3294509446714073e-05\n",
      "Step: 3210, train/epoch: 5.341098308563232\n",
      "Step: 3220, train/loss: 0.03629999980330467\n",
      "Step: 3220, train/grad_norm: 13.894062042236328\n",
      "Step: 3220, train/learning_rate: 2.3211314328364097e-05\n",
      "Step: 3220, train/epoch: 5.357737064361572\n",
      "Step: 3230, train/loss: 0.017100000753998756\n",
      "Step: 3230, train/grad_norm: 0.07301013171672821\n",
      "Step: 3230, train/learning_rate: 2.312811921001412e-05\n",
      "Step: 3230, train/epoch: 5.374375820159912\n",
      "Step: 3240, train/loss: 0.027000000700354576\n",
      "Step: 3240, train/grad_norm: 0.2428072988986969\n",
      "Step: 3240, train/learning_rate: 2.3044925910653546e-05\n",
      "Step: 3240, train/epoch: 5.39101505279541\n",
      "Step: 3250, train/loss: 0.03539999946951866\n",
      "Step: 3250, train/grad_norm: 5.652534484863281\n",
      "Step: 3250, train/learning_rate: 2.296173079230357e-05\n",
      "Step: 3250, train/epoch: 5.40765380859375\n",
      "Step: 3260, train/loss: 0.05510000139474869\n",
      "Step: 3260, train/grad_norm: 11.67782211303711\n",
      "Step: 3260, train/learning_rate: 2.2878535673953593e-05\n",
      "Step: 3260, train/epoch: 5.424293041229248\n",
      "Step: 3270, train/loss: 0.03830000013113022\n",
      "Step: 3270, train/grad_norm: 0.14081072807312012\n",
      "Step: 3270, train/learning_rate: 2.2795340555603616e-05\n",
      "Step: 3270, train/epoch: 5.440931797027588\n",
      "Step: 3280, train/loss: 0.03060000017285347\n",
      "Step: 3280, train/grad_norm: 8.862749099731445\n",
      "Step: 3280, train/learning_rate: 2.2712147256243043e-05\n",
      "Step: 3280, train/epoch: 5.457570552825928\n",
      "Step: 3290, train/loss: 0.04360000044107437\n",
      "Step: 3290, train/grad_norm: 21.87161636352539\n",
      "Step: 3290, train/learning_rate: 2.2628952137893066e-05\n",
      "Step: 3290, train/epoch: 5.474209785461426\n",
      "Step: 3300, train/loss: 0.0272000003606081\n",
      "Step: 3300, train/grad_norm: 0.30885565280914307\n",
      "Step: 3300, train/learning_rate: 2.254575701954309e-05\n",
      "Step: 3300, train/epoch: 5.490848541259766\n",
      "Step: 3310, train/loss: 0.03550000116229057\n",
      "Step: 3310, train/grad_norm: 13.479832649230957\n",
      "Step: 3310, train/learning_rate: 2.2462561901193112e-05\n",
      "Step: 3310, train/epoch: 5.5074872970581055\n",
      "Step: 3320, train/loss: 0.010300000198185444\n",
      "Step: 3320, train/grad_norm: 26.758472442626953\n",
      "Step: 3320, train/learning_rate: 2.237936860183254e-05\n",
      "Step: 3320, train/epoch: 5.5241265296936035\n",
      "Step: 3330, train/loss: 0.03290000185370445\n",
      "Step: 3330, train/grad_norm: 27.099782943725586\n",
      "Step: 3330, train/learning_rate: 2.2296173483482562e-05\n",
      "Step: 3330, train/epoch: 5.540765285491943\n",
      "Step: 3340, train/loss: 0.013799999840557575\n",
      "Step: 3340, train/grad_norm: 2.7071633338928223\n",
      "Step: 3340, train/learning_rate: 2.2212978365132585e-05\n",
      "Step: 3340, train/epoch: 5.557404518127441\n",
      "Step: 3350, train/loss: 0.017400000244379044\n",
      "Step: 3350, train/grad_norm: 5.911084175109863\n",
      "Step: 3350, train/learning_rate: 2.212978324678261e-05\n",
      "Step: 3350, train/epoch: 5.574043273925781\n",
      "Step: 3360, train/loss: 0.055399999022483826\n",
      "Step: 3360, train/grad_norm: 0.025454625487327576\n",
      "Step: 3360, train/learning_rate: 2.204658812843263e-05\n",
      "Step: 3360, train/epoch: 5.590682029724121\n",
      "Step: 3370, train/loss: 0.025800000876188278\n",
      "Step: 3370, train/grad_norm: 1.1853713989257812\n",
      "Step: 3370, train/learning_rate: 2.1963394829072058e-05\n",
      "Step: 3370, train/epoch: 5.607321262359619\n",
      "Step: 3380, train/loss: 0.047600001096725464\n",
      "Step: 3380, train/grad_norm: 10.387971878051758\n",
      "Step: 3380, train/learning_rate: 2.188019971072208e-05\n",
      "Step: 3380, train/epoch: 5.623960018157959\n",
      "Step: 3390, train/loss: 0.008200000040233135\n",
      "Step: 3390, train/grad_norm: 0.0879172831773758\n",
      "Step: 3390, train/learning_rate: 2.1797004592372105e-05\n",
      "Step: 3390, train/epoch: 5.640598773956299\n",
      "Step: 3400, train/loss: 0.02160000056028366\n",
      "Step: 3400, train/grad_norm: 15.961402893066406\n",
      "Step: 3400, train/learning_rate: 2.1713809474022128e-05\n",
      "Step: 3400, train/epoch: 5.657238006591797\n",
      "Step: 3410, train/loss: 0.0357000008225441\n",
      "Step: 3410, train/grad_norm: 0.043756209313869476\n",
      "Step: 3410, train/learning_rate: 2.1630616174661554e-05\n",
      "Step: 3410, train/epoch: 5.673876762390137\n",
      "Step: 3420, train/loss: 0.023900000378489494\n",
      "Step: 3420, train/grad_norm: 0.010405867360532284\n",
      "Step: 3420, train/learning_rate: 2.1547421056311578e-05\n",
      "Step: 3420, train/epoch: 5.690515995025635\n",
      "Step: 3430, train/loss: 0.024700000882148743\n",
      "Step: 3430, train/grad_norm: 10.381543159484863\n",
      "Step: 3430, train/learning_rate: 2.14642259379616e-05\n",
      "Step: 3430, train/epoch: 5.707154750823975\n",
      "Step: 3440, train/loss: 0.026599999517202377\n",
      "Step: 3440, train/grad_norm: 0.7421347498893738\n",
      "Step: 3440, train/learning_rate: 2.1381030819611624e-05\n",
      "Step: 3440, train/epoch: 5.7237935066223145\n",
      "Step: 3450, train/loss: 0.014100000262260437\n",
      "Step: 3450, train/grad_norm: 17.816862106323242\n",
      "Step: 3450, train/learning_rate: 2.129783752025105e-05\n",
      "Step: 3450, train/epoch: 5.7404327392578125\n",
      "Step: 3460, train/loss: 0.00839999970048666\n",
      "Step: 3460, train/grad_norm: 0.037317853420972824\n",
      "Step: 3460, train/learning_rate: 2.1214642401901074e-05\n",
      "Step: 3460, train/epoch: 5.757071495056152\n",
      "Step: 3470, train/loss: 0.04690000042319298\n",
      "Step: 3470, train/grad_norm: 0.05239659920334816\n",
      "Step: 3470, train/learning_rate: 2.1131447283551097e-05\n",
      "Step: 3470, train/epoch: 5.773710250854492\n",
      "Step: 3480, train/loss: 0.03660000115633011\n",
      "Step: 3480, train/grad_norm: 0.2521175146102905\n",
      "Step: 3480, train/learning_rate: 2.104825216520112e-05\n",
      "Step: 3480, train/epoch: 5.79034948348999\n",
      "Step: 3490, train/loss: 0.019899999722838402\n",
      "Step: 3490, train/grad_norm: 17.2530460357666\n",
      "Step: 3490, train/learning_rate: 2.0965058865840547e-05\n",
      "Step: 3490, train/epoch: 5.80698823928833\n",
      "Step: 3500, train/loss: 0.02500000037252903\n",
      "Step: 3500, train/grad_norm: 29.943897247314453\n",
      "Step: 3500, train/learning_rate: 2.088186374749057e-05\n",
      "Step: 3500, train/epoch: 5.823627471923828\n",
      "Step: 3510, train/loss: 0.026499999687075615\n",
      "Step: 3510, train/grad_norm: 0.05744441598653793\n",
      "Step: 3510, train/learning_rate: 2.0798668629140593e-05\n",
      "Step: 3510, train/epoch: 5.840266227722168\n",
      "Step: 3520, train/loss: 0.015300000086426735\n",
      "Step: 3520, train/grad_norm: 0.02090250886976719\n",
      "Step: 3520, train/learning_rate: 2.0715473510790616e-05\n",
      "Step: 3520, train/epoch: 5.856904983520508\n",
      "Step: 3530, train/loss: 0.012799999676644802\n",
      "Step: 3530, train/grad_norm: 0.07161453366279602\n",
      "Step: 3530, train/learning_rate: 2.0632280211430043e-05\n",
      "Step: 3530, train/epoch: 5.873544216156006\n",
      "Step: 3540, train/loss: 0.015799999237060547\n",
      "Step: 3540, train/grad_norm: 0.03751860186457634\n",
      "Step: 3540, train/learning_rate: 2.0549085093080066e-05\n",
      "Step: 3540, train/epoch: 5.890182971954346\n",
      "Step: 3550, train/loss: 0.0020000000949949026\n",
      "Step: 3550, train/grad_norm: 0.009918388910591602\n",
      "Step: 3550, train/learning_rate: 2.046588997473009e-05\n",
      "Step: 3550, train/epoch: 5.9068217277526855\n",
      "Step: 3560, train/loss: 0.03680000081658363\n",
      "Step: 3560, train/grad_norm: 10.132723808288574\n",
      "Step: 3560, train/learning_rate: 2.0382694856380112e-05\n",
      "Step: 3560, train/epoch: 5.923460960388184\n",
      "Step: 3570, train/loss: 0.0019000000320374966\n",
      "Step: 3570, train/grad_norm: 0.22735469043254852\n",
      "Step: 3570, train/learning_rate: 2.029950155701954e-05\n",
      "Step: 3570, train/epoch: 5.940099716186523\n",
      "Step: 3580, train/loss: 0.06560000032186508\n",
      "Step: 3580, train/grad_norm: 12.4049072265625\n",
      "Step: 3580, train/learning_rate: 2.0216306438669562e-05\n",
      "Step: 3580, train/epoch: 5.9567389488220215\n",
      "Step: 3590, train/loss: 0.02710000053048134\n",
      "Step: 3590, train/grad_norm: 2.1055402755737305\n",
      "Step: 3590, train/learning_rate: 2.0133111320319586e-05\n",
      "Step: 3590, train/epoch: 5.973377704620361\n",
      "Step: 3600, train/loss: 0.02889999933540821\n",
      "Step: 3600, train/grad_norm: 11.114901542663574\n",
      "Step: 3600, train/learning_rate: 2.004991620196961e-05\n",
      "Step: 3600, train/epoch: 5.990016460418701\n",
      "Step: 3606, eval/loss: 1.216490626335144\n",
      "Step: 3606, eval/accuracy: 0.8114674687385559\n",
      "Step: 3606, eval/f1: 0.8081513047218323\n",
      "Step: 3606, eval/runtime: 28.731199264526367\n",
      "Step: 3606, eval/samples_per_second: 250.7030029296875\n",
      "Step: 3606, eval/steps_per_second: 4.489999771118164\n",
      "Step: 3606, train/epoch: 6.0\n",
      "Step: 3610, train/loss: 0.06530000269412994\n",
      "Step: 3610, train/grad_norm: 6.339132308959961\n",
      "Step: 3610, train/learning_rate: 1.9966722902609035e-05\n",
      "Step: 3610, train/epoch: 6.006655693054199\n",
      "Step: 3620, train/loss: 0.05829999968409538\n",
      "Step: 3620, train/grad_norm: 10.787761688232422\n",
      "Step: 3620, train/learning_rate: 1.988352778425906e-05\n",
      "Step: 3620, train/epoch: 6.023294448852539\n",
      "Step: 3630, train/loss: 0.013199999928474426\n",
      "Step: 3630, train/grad_norm: 2.1847939491271973\n",
      "Step: 3630, train/learning_rate: 1.9800332665909082e-05\n",
      "Step: 3630, train/epoch: 6.039933681488037\n",
      "Step: 3640, train/loss: 0.009999999776482582\n",
      "Step: 3640, train/grad_norm: 0.01368380431085825\n",
      "Step: 3640, train/learning_rate: 1.9717137547559105e-05\n",
      "Step: 3640, train/epoch: 6.056572437286377\n",
      "Step: 3650, train/loss: 0.004000000189989805\n",
      "Step: 3650, train/grad_norm: 0.017693467438220978\n",
      "Step: 3650, train/learning_rate: 1.963394424819853e-05\n",
      "Step: 3650, train/epoch: 6.073211193084717\n",
      "Step: 3660, train/loss: 0.007899999618530273\n",
      "Step: 3660, train/grad_norm: 0.017012566328048706\n",
      "Step: 3660, train/learning_rate: 1.9550749129848555e-05\n",
      "Step: 3660, train/epoch: 6.089850425720215\n",
      "Step: 3670, train/loss: 0.04749999940395355\n",
      "Step: 3670, train/grad_norm: 24.80915641784668\n",
      "Step: 3670, train/learning_rate: 1.9467554011498578e-05\n",
      "Step: 3670, train/epoch: 6.106489181518555\n",
      "Step: 3680, train/loss: 0.021199999377131462\n",
      "Step: 3680, train/grad_norm: 0.18601679801940918\n",
      "Step: 3680, train/learning_rate: 1.93843588931486e-05\n",
      "Step: 3680, train/epoch: 6.1231279373168945\n",
      "Step: 3690, train/loss: 0.015200000256299973\n",
      "Step: 3690, train/grad_norm: 20.861257553100586\n",
      "Step: 3690, train/learning_rate: 1.9301165593788028e-05\n",
      "Step: 3690, train/epoch: 6.139767169952393\n",
      "Step: 3700, train/loss: 0.0019000000320374966\n",
      "Step: 3700, train/grad_norm: 0.19953593611717224\n",
      "Step: 3700, train/learning_rate: 1.921797047543805e-05\n",
      "Step: 3700, train/epoch: 6.156405925750732\n",
      "Step: 3710, train/loss: 0.01860000006854534\n",
      "Step: 3710, train/grad_norm: 0.10555090755224228\n",
      "Step: 3710, train/learning_rate: 1.9134775357088074e-05\n",
      "Step: 3710, train/epoch: 6.1730451583862305\n",
      "Step: 3720, train/loss: 0.009800000116229057\n",
      "Step: 3720, train/grad_norm: 0.0324748270213604\n",
      "Step: 3720, train/learning_rate: 1.9051580238738097e-05\n",
      "Step: 3720, train/epoch: 6.18968391418457\n",
      "Step: 3730, train/loss: 0.02459999918937683\n",
      "Step: 3730, train/grad_norm: 0.10677874833345413\n",
      "Step: 3730, train/learning_rate: 1.896838512038812e-05\n",
      "Step: 3730, train/epoch: 6.20632266998291\n",
      "Step: 3740, train/loss: 0.002300000051036477\n",
      "Step: 3740, train/grad_norm: 21.791791915893555\n",
      "Step: 3740, train/learning_rate: 1.8885191821027547e-05\n",
      "Step: 3740, train/epoch: 6.222961902618408\n",
      "Step: 3750, train/loss: 0.014600000344216824\n",
      "Step: 3750, train/grad_norm: 0.009653404355049133\n",
      "Step: 3750, train/learning_rate: 1.880199670267757e-05\n",
      "Step: 3750, train/epoch: 6.239600658416748\n",
      "Step: 3760, train/loss: 0.004600000102072954\n",
      "Step: 3760, train/grad_norm: 0.028322968631982803\n",
      "Step: 3760, train/learning_rate: 1.8718801584327593e-05\n",
      "Step: 3760, train/epoch: 6.256239414215088\n",
      "Step: 3770, train/loss: 0.017500000074505806\n",
      "Step: 3770, train/grad_norm: 20.022306442260742\n",
      "Step: 3770, train/learning_rate: 1.8635606465977617e-05\n",
      "Step: 3770, train/epoch: 6.272878646850586\n",
      "Step: 3780, train/loss: 0.0024999999441206455\n",
      "Step: 3780, train/grad_norm: 0.2635870575904846\n",
      "Step: 3780, train/learning_rate: 1.8552413166617043e-05\n",
      "Step: 3780, train/epoch: 6.289517402648926\n",
      "Step: 3790, train/loss: 0.0013000000035390258\n",
      "Step: 3790, train/grad_norm: 0.017676806077361107\n",
      "Step: 3790, train/learning_rate: 1.8469218048267066e-05\n",
      "Step: 3790, train/epoch: 6.306156635284424\n",
      "Step: 3800, train/loss: 0.006599999964237213\n",
      "Step: 3800, train/grad_norm: 0.21944797039031982\n",
      "Step: 3800, train/learning_rate: 1.838602292991709e-05\n",
      "Step: 3800, train/epoch: 6.322795391082764\n",
      "Step: 3810, train/loss: 0.008200000040233135\n",
      "Step: 3810, train/grad_norm: 0.006366807036101818\n",
      "Step: 3810, train/learning_rate: 1.8302827811567113e-05\n",
      "Step: 3810, train/epoch: 6.3394341468811035\n",
      "Step: 3820, train/loss: 0.002199999988079071\n",
      "Step: 3820, train/grad_norm: 0.010684470646083355\n",
      "Step: 3820, train/learning_rate: 1.821963451220654e-05\n",
      "Step: 3820, train/epoch: 6.356073379516602\n",
      "Step: 3830, train/loss: 0.00039999998989515007\n",
      "Step: 3830, train/grad_norm: 0.0012538345763459802\n",
      "Step: 3830, train/learning_rate: 1.8136439393856563e-05\n",
      "Step: 3830, train/epoch: 6.372712135314941\n",
      "Step: 3840, train/loss: 0.00930000003427267\n",
      "Step: 3840, train/grad_norm: 19.57204246520996\n",
      "Step: 3840, train/learning_rate: 1.8053244275506586e-05\n",
      "Step: 3840, train/epoch: 6.389350891113281\n",
      "Step: 3850, train/loss: 0.0005000000237487257\n",
      "Step: 3850, train/grad_norm: 9.254712104797363\n",
      "Step: 3850, train/learning_rate: 1.797004915715661e-05\n",
      "Step: 3850, train/epoch: 6.405990123748779\n",
      "Step: 3860, train/loss: 0.037300001829862595\n",
      "Step: 3860, train/grad_norm: 22.576152801513672\n",
      "Step: 3860, train/learning_rate: 1.7886855857796036e-05\n",
      "Step: 3860, train/epoch: 6.422628879547119\n",
      "Step: 3870, train/loss: 0.02019999921321869\n",
      "Step: 3870, train/grad_norm: 0.03455366939306259\n",
      "Step: 3870, train/learning_rate: 1.780366073944606e-05\n",
      "Step: 3870, train/epoch: 6.439268112182617\n",
      "Step: 3880, train/loss: 0.007199999876320362\n",
      "Step: 3880, train/grad_norm: 0.0011043730191886425\n",
      "Step: 3880, train/learning_rate: 1.7720465621096082e-05\n",
      "Step: 3880, train/epoch: 6.455906867980957\n",
      "Step: 3890, train/loss: 0.012299999594688416\n",
      "Step: 3890, train/grad_norm: 0.021684208884835243\n",
      "Step: 3890, train/learning_rate: 1.7637270502746105e-05\n",
      "Step: 3890, train/epoch: 6.472545623779297\n",
      "Step: 3900, train/loss: 0.013399999588727951\n",
      "Step: 3900, train/grad_norm: 17.627958297729492\n",
      "Step: 3900, train/learning_rate: 1.7554077203385532e-05\n",
      "Step: 3900, train/epoch: 6.489184856414795\n",
      "Step: 3910, train/loss: 0.037300001829862595\n",
      "Step: 3910, train/grad_norm: 12.046667098999023\n",
      "Step: 3910, train/learning_rate: 1.7470882085035555e-05\n",
      "Step: 3910, train/epoch: 6.505823612213135\n",
      "Step: 3920, train/loss: 0.02810000069439411\n",
      "Step: 3920, train/grad_norm: 0.02451784536242485\n",
      "Step: 3920, train/learning_rate: 1.7387686966685578e-05\n",
      "Step: 3920, train/epoch: 6.522462368011475\n",
      "Step: 3930, train/loss: 0.031599998474121094\n",
      "Step: 3930, train/grad_norm: 0.9599453806877136\n",
      "Step: 3930, train/learning_rate: 1.73044918483356e-05\n",
      "Step: 3930, train/epoch: 6.539101600646973\n",
      "Step: 3940, train/loss: 0.003700000001117587\n",
      "Step: 3940, train/grad_norm: 0.20556333661079407\n",
      "Step: 3940, train/learning_rate: 1.7221298548975028e-05\n",
      "Step: 3940, train/epoch: 6.5557403564453125\n",
      "Step: 3950, train/loss: 0.0005000000237487257\n",
      "Step: 3950, train/grad_norm: 0.06804387271404266\n",
      "Step: 3950, train/learning_rate: 1.713810343062505e-05\n",
      "Step: 3950, train/epoch: 6.5723795890808105\n",
      "Step: 3960, train/loss: 0.000699999975040555\n",
      "Step: 3960, train/grad_norm: 0.00464837858453393\n",
      "Step: 3960, train/learning_rate: 1.7054908312275074e-05\n",
      "Step: 3960, train/epoch: 6.58901834487915\n",
      "Step: 3970, train/loss: 0.03150000050663948\n",
      "Step: 3970, train/grad_norm: 0.017818553373217583\n",
      "Step: 3970, train/learning_rate: 1.6971713193925098e-05\n",
      "Step: 3970, train/epoch: 6.60565710067749\n",
      "Step: 3980, train/loss: 0.03680000081658363\n",
      "Step: 3980, train/grad_norm: 0.07061228156089783\n",
      "Step: 3980, train/learning_rate: 1.6888519894564524e-05\n",
      "Step: 3980, train/epoch: 6.622296333312988\n",
      "Step: 3990, train/loss: 0.009700000286102295\n",
      "Step: 3990, train/grad_norm: 31.13102912902832\n",
      "Step: 3990, train/learning_rate: 1.6805324776214547e-05\n",
      "Step: 3990, train/epoch: 6.638935089111328\n",
      "Step: 4000, train/loss: 0.00800000037997961\n",
      "Step: 4000, train/grad_norm: 0.023273974657058716\n",
      "Step: 4000, train/learning_rate: 1.672212965786457e-05\n",
      "Step: 4000, train/epoch: 6.655573844909668\n",
      "Step: 4010, train/loss: 0.013299999758601189\n",
      "Step: 4010, train/grad_norm: 0.1020282655954361\n",
      "Step: 4010, train/learning_rate: 1.6638934539514594e-05\n",
      "Step: 4010, train/epoch: 6.672213077545166\n",
      "Step: 4020, train/loss: 0.000699999975040555\n",
      "Step: 4020, train/grad_norm: 9.6757173538208\n",
      "Step: 4020, train/learning_rate: 1.655574124015402e-05\n",
      "Step: 4020, train/epoch: 6.688851833343506\n",
      "Step: 4030, train/loss: 0.00019999999494757503\n",
      "Step: 4030, train/grad_norm: 0.003667514305561781\n",
      "Step: 4030, train/learning_rate: 1.6472546121804044e-05\n",
      "Step: 4030, train/epoch: 6.705491065979004\n",
      "Step: 4040, train/loss: 0.019899999722838402\n",
      "Step: 4040, train/grad_norm: 0.03287411853671074\n",
      "Step: 4040, train/learning_rate: 1.6389351003454067e-05\n",
      "Step: 4040, train/epoch: 6.722129821777344\n",
      "Step: 4050, train/loss: 0.03660000115633011\n",
      "Step: 4050, train/grad_norm: 0.018175926059484482\n",
      "Step: 4050, train/learning_rate: 1.630615588510409e-05\n",
      "Step: 4050, train/epoch: 6.738768577575684\n",
      "Step: 4060, train/loss: 0.03099999949336052\n",
      "Step: 4060, train/grad_norm: 0.016071582213044167\n",
      "Step: 4060, train/learning_rate: 1.6222962585743517e-05\n",
      "Step: 4060, train/epoch: 6.755407810211182\n",
      "Step: 4070, train/loss: 0.008799999952316284\n",
      "Step: 4070, train/grad_norm: 0.4486153721809387\n",
      "Step: 4070, train/learning_rate: 1.613976746739354e-05\n",
      "Step: 4070, train/epoch: 6.7720465660095215\n",
      "Step: 4080, train/loss: 0.016100000590085983\n",
      "Step: 4080, train/grad_norm: 0.06838152557611465\n",
      "Step: 4080, train/learning_rate: 1.6056572349043563e-05\n",
      "Step: 4080, train/epoch: 6.788685321807861\n",
      "Step: 4090, train/loss: 0.02800000086426735\n",
      "Step: 4090, train/grad_norm: 0.013889473862946033\n",
      "Step: 4090, train/learning_rate: 1.5973377230693586e-05\n",
      "Step: 4090, train/epoch: 6.805324554443359\n",
      "Step: 4100, train/loss: 0.0017999999690800905\n",
      "Step: 4100, train/grad_norm: 0.0052953483536839485\n",
      "Step: 4100, train/learning_rate: 1.5890183931333013e-05\n",
      "Step: 4100, train/epoch: 6.821963310241699\n",
      "Step: 4110, train/loss: 0.024700000882148743\n",
      "Step: 4110, train/grad_norm: 0.22264041006565094\n",
      "Step: 4110, train/learning_rate: 1.5806988812983036e-05\n",
      "Step: 4110, train/epoch: 6.838602542877197\n",
      "Step: 4120, train/loss: 0.05180000141263008\n",
      "Step: 4120, train/grad_norm: 28.261428833007812\n",
      "Step: 4120, train/learning_rate: 1.572379369463306e-05\n",
      "Step: 4120, train/epoch: 6.855241298675537\n",
      "Step: 4130, train/loss: 0.04780000075697899\n",
      "Step: 4130, train/grad_norm: 38.920536041259766\n",
      "Step: 4130, train/learning_rate: 1.5640598576283082e-05\n",
      "Step: 4130, train/epoch: 6.871880054473877\n",
      "Step: 4140, train/loss: 0.021299999207258224\n",
      "Step: 4140, train/grad_norm: 12.372496604919434\n",
      "Step: 4140, train/learning_rate: 1.5557403457933106e-05\n",
      "Step: 4140, train/epoch: 6.888519287109375\n",
      "Step: 4150, train/loss: 0.01209999993443489\n",
      "Step: 4150, train/grad_norm: 4.659459590911865\n",
      "Step: 4150, train/learning_rate: 1.5474210158572532e-05\n",
      "Step: 4150, train/epoch: 6.905158042907715\n",
      "Step: 4160, train/loss: 0.018699999898672104\n",
      "Step: 4160, train/grad_norm: 18.07029914855957\n",
      "Step: 4160, train/learning_rate: 1.5391015040222555e-05\n",
      "Step: 4160, train/epoch: 6.921796798706055\n",
      "Step: 4170, train/loss: 0.0005000000237487257\n",
      "Step: 4170, train/grad_norm: 0.011084171012043953\n",
      "Step: 4170, train/learning_rate: 1.530781992187258e-05\n",
      "Step: 4170, train/epoch: 6.938436031341553\n",
      "Step: 4180, train/loss: 0.024000000208616257\n",
      "Step: 4180, train/grad_norm: 0.16903485357761383\n",
      "Step: 4180, train/learning_rate: 1.5224625713017303e-05\n",
      "Step: 4180, train/epoch: 6.955074787139893\n",
      "Step: 4190, train/loss: 0.02449999935925007\n",
      "Step: 4190, train/grad_norm: 0.28646978735923767\n",
      "Step: 4190, train/learning_rate: 1.5141430594667327e-05\n",
      "Step: 4190, train/epoch: 6.971714019775391\n",
      "Step: 4200, train/loss: 0.003000000026077032\n",
      "Step: 4200, train/grad_norm: 0.004256904125213623\n",
      "Step: 4200, train/learning_rate: 1.5058236385812052e-05\n",
      "Step: 4200, train/epoch: 6.9883527755737305\n",
      "Step: 4207, eval/loss: 1.2716789245605469\n",
      "Step: 4207, eval/accuracy: 0.8250728845596313\n",
      "Step: 4207, eval/f1: 0.8202908039093018\n",
      "Step: 4207, eval/runtime: 29.05190086364746\n",
      "Step: 4207, eval/samples_per_second: 247.93600463867188\n",
      "Step: 4207, eval/steps_per_second: 4.440000057220459\n",
      "Step: 4207, train/epoch: 7.0\n",
      "Step: 4210, train/loss: 0.0005000000237487257\n",
      "Step: 4210, train/grad_norm: 0.20391148328781128\n",
      "Step: 4210, train/learning_rate: 1.4975041267462075e-05\n",
      "Step: 4210, train/epoch: 7.00499153137207\n",
      "Step: 4220, train/loss: 0.007499999832361937\n",
      "Step: 4220, train/grad_norm: 0.06651734560728073\n",
      "Step: 4220, train/learning_rate: 1.48918470586068e-05\n",
      "Step: 4220, train/epoch: 7.021630764007568\n",
      "Step: 4230, train/loss: 0.001500000013038516\n",
      "Step: 4230, train/grad_norm: 27.201416015625\n",
      "Step: 4230, train/learning_rate: 1.4808651940256823e-05\n",
      "Step: 4230, train/epoch: 7.038269519805908\n",
      "Step: 4240, train/loss: 0.007400000002235174\n",
      "Step: 4240, train/grad_norm: 0.0029443406965583563\n",
      "Step: 4240, train/learning_rate: 1.4725457731401548e-05\n",
      "Step: 4240, train/epoch: 7.054908275604248\n",
      "Step: 4250, train/loss: 0.010700000450015068\n",
      "Step: 4250, train/grad_norm: 0.003089585341513157\n",
      "Step: 4250, train/learning_rate: 1.4642262613051571e-05\n",
      "Step: 4250, train/epoch: 7.071547508239746\n",
      "Step: 4260, train/loss: 0.0006000000284984708\n",
      "Step: 4260, train/grad_norm: 4.921462059020996\n",
      "Step: 4260, train/learning_rate: 1.4559068404196296e-05\n",
      "Step: 4260, train/epoch: 7.088186264038086\n",
      "Step: 4270, train/loss: 0.01759999990463257\n",
      "Step: 4270, train/grad_norm: 0.3783043920993805\n",
      "Step: 4270, train/learning_rate: 1.4475873285846319e-05\n",
      "Step: 4270, train/epoch: 7.104825496673584\n",
      "Step: 4280, train/loss: 0.0032999999821186066\n",
      "Step: 4280, train/grad_norm: 0.0029464648105204105\n",
      "Step: 4280, train/learning_rate: 1.4392679076991044e-05\n",
      "Step: 4280, train/epoch: 7.121464252471924\n",
      "Step: 4290, train/loss: 0.007799999788403511\n",
      "Step: 4290, train/grad_norm: 0.016470111906528473\n",
      "Step: 4290, train/learning_rate: 1.4309483958641067e-05\n",
      "Step: 4290, train/epoch: 7.138103008270264\n",
      "Step: 4300, train/loss: 0.026499999687075615\n",
      "Step: 4300, train/grad_norm: 0.009278269484639168\n",
      "Step: 4300, train/learning_rate: 1.4226289749785792e-05\n",
      "Step: 4300, train/epoch: 7.154742240905762\n",
      "Step: 4310, train/loss: 0.006200000178068876\n",
      "Step: 4310, train/grad_norm: 0.015143980272114277\n",
      "Step: 4310, train/learning_rate: 1.4143094631435815e-05\n",
      "Step: 4310, train/epoch: 7.171380996704102\n",
      "Step: 4320, train/loss: 0.006500000134110451\n",
      "Step: 4320, train/grad_norm: 0.0032810138072818518\n",
      "Step: 4320, train/learning_rate: 1.405990042258054e-05\n",
      "Step: 4320, train/epoch: 7.188019752502441\n",
      "Step: 4330, train/loss: 0.023000000044703484\n",
      "Step: 4330, train/grad_norm: 0.002713356167078018\n",
      "Step: 4330, train/learning_rate: 1.3976705304230563e-05\n",
      "Step: 4330, train/epoch: 7.2046589851379395\n",
      "Step: 4340, train/loss: 0.03539999946951866\n",
      "Step: 4340, train/grad_norm: 9.102219581604004\n",
      "Step: 4340, train/learning_rate: 1.3893511095375288e-05\n",
      "Step: 4340, train/epoch: 7.221297740936279\n",
      "Step: 4350, train/loss: 0.017500000074505806\n",
      "Step: 4350, train/grad_norm: 0.008144552819430828\n",
      "Step: 4350, train/learning_rate: 1.3810315977025311e-05\n",
      "Step: 4350, train/epoch: 7.237936973571777\n",
      "Step: 4360, train/loss: 0.016499999910593033\n",
      "Step: 4360, train/grad_norm: 9.940381050109863\n",
      "Step: 4360, train/learning_rate: 1.3727121768170036e-05\n",
      "Step: 4360, train/epoch: 7.254575729370117\n",
      "Step: 4370, train/loss: 0.057999998331069946\n",
      "Step: 4370, train/grad_norm: 0.08703269809484482\n",
      "Step: 4370, train/learning_rate: 1.364392664982006e-05\n",
      "Step: 4370, train/epoch: 7.271214485168457\n",
      "Step: 4380, train/loss: 0.00139999995008111\n",
      "Step: 4380, train/grad_norm: 0.017797736451029778\n",
      "Step: 4380, train/learning_rate: 1.3560732440964784e-05\n",
      "Step: 4380, train/epoch: 7.287853717803955\n",
      "Step: 4390, train/loss: 0.012900000438094139\n",
      "Step: 4390, train/grad_norm: 21.048892974853516\n",
      "Step: 4390, train/learning_rate: 1.3477537322614808e-05\n",
      "Step: 4390, train/epoch: 7.304492473602295\n",
      "Step: 4400, train/loss: 0.0010000000474974513\n",
      "Step: 4400, train/grad_norm: 0.028803814202547073\n",
      "Step: 4400, train/learning_rate: 1.3394343113759533e-05\n",
      "Step: 4400, train/epoch: 7.321131229400635\n",
      "Step: 4410, train/loss: 0.004800000227987766\n",
      "Step: 4410, train/grad_norm: 0.015473073348402977\n",
      "Step: 4410, train/learning_rate: 1.3311147995409556e-05\n",
      "Step: 4410, train/epoch: 7.337770462036133\n",
      "Step: 4420, train/loss: 0.0006000000284984708\n",
      "Step: 4420, train/grad_norm: 0.0014867244753986597\n",
      "Step: 4420, train/learning_rate: 1.322795378655428e-05\n",
      "Step: 4420, train/epoch: 7.354409217834473\n",
      "Step: 4430, train/loss: 9.999999747378752e-05\n",
      "Step: 4430, train/grad_norm: 0.01850888691842556\n",
      "Step: 4430, train/learning_rate: 1.3144758668204304e-05\n",
      "Step: 4430, train/epoch: 7.371048450469971\n",
      "Step: 4440, train/loss: 0.017799999564886093\n",
      "Step: 4440, train/grad_norm: 0.0075882491655647755\n",
      "Step: 4440, train/learning_rate: 1.3061564459349029e-05\n",
      "Step: 4440, train/epoch: 7.3876872062683105\n",
      "Step: 4450, train/loss: 0.004100000020116568\n",
      "Step: 4450, train/grad_norm: 0.025670204311609268\n",
      "Step: 4450, train/learning_rate: 1.2978369340999052e-05\n",
      "Step: 4450, train/epoch: 7.40432596206665\n",
      "Step: 4460, train/loss: 0.0010999999940395355\n",
      "Step: 4460, train/grad_norm: 0.010581264272332191\n",
      "Step: 4460, train/learning_rate: 1.2895175132143777e-05\n",
      "Step: 4460, train/epoch: 7.420965194702148\n",
      "Step: 4470, train/loss: 0.005799999926239252\n",
      "Step: 4470, train/grad_norm: 0.07161131501197815\n",
      "Step: 4470, train/learning_rate: 1.28119800137938e-05\n",
      "Step: 4470, train/epoch: 7.437603950500488\n",
      "Step: 4480, train/loss: 0.0006000000284984708\n",
      "Step: 4480, train/grad_norm: 0.03371115028858185\n",
      "Step: 4480, train/learning_rate: 1.2728785804938525e-05\n",
      "Step: 4480, train/epoch: 7.454242706298828\n",
      "Step: 4490, train/loss: 0.014100000262260437\n",
      "Step: 4490, train/grad_norm: 0.004175396636128426\n",
      "Step: 4490, train/learning_rate: 1.2645590686588548e-05\n",
      "Step: 4490, train/epoch: 7.470881938934326\n",
      "Step: 4500, train/loss: 0.010599999688565731\n",
      "Step: 4500, train/grad_norm: 0.04606952145695686\n",
      "Step: 4500, train/learning_rate: 1.2562395568238571e-05\n",
      "Step: 4500, train/epoch: 7.487520694732666\n",
      "Step: 4510, train/loss: 0.003599999938160181\n",
      "Step: 4510, train/grad_norm: 13.40998649597168\n",
      "Step: 4510, train/learning_rate: 1.2479201359383296e-05\n",
      "Step: 4510, train/epoch: 7.504159927368164\n",
      "Step: 4520, train/loss: 0.01209999993443489\n",
      "Step: 4520, train/grad_norm: 0.01577785052359104\n",
      "Step: 4520, train/learning_rate: 1.239600624103332e-05\n",
      "Step: 4520, train/epoch: 7.520798683166504\n",
      "Step: 4530, train/loss: 0.0012000000569969416\n",
      "Step: 4530, train/grad_norm: 0.20725205540657043\n",
      "Step: 4530, train/learning_rate: 1.2312812032178044e-05\n",
      "Step: 4530, train/epoch: 7.537437438964844\n",
      "Step: 4540, train/loss: 0.012900000438094139\n",
      "Step: 4540, train/grad_norm: 0.019744770601391792\n",
      "Step: 4540, train/learning_rate: 1.2229616913828067e-05\n",
      "Step: 4540, train/epoch: 7.554076671600342\n",
      "Step: 4550, train/loss: 0.0272000003606081\n",
      "Step: 4550, train/grad_norm: 29.01206398010254\n",
      "Step: 4550, train/learning_rate: 1.2146422704972792e-05\n",
      "Step: 4550, train/epoch: 7.570715427398682\n",
      "Step: 4560, train/loss: 0.00019999999494757503\n",
      "Step: 4560, train/grad_norm: 0.007021067664027214\n",
      "Step: 4560, train/learning_rate: 1.2063227586622816e-05\n",
      "Step: 4560, train/epoch: 7.5873541831970215\n",
      "Step: 4570, train/loss: 0.0012000000569969416\n",
      "Step: 4570, train/grad_norm: 0.020852362737059593\n",
      "Step: 4570, train/learning_rate: 1.198003337776754e-05\n",
      "Step: 4570, train/epoch: 7.6039934158325195\n",
      "Step: 4580, train/loss: 0.0032999999821186066\n",
      "Step: 4580, train/grad_norm: 0.38028261065483093\n",
      "Step: 4580, train/learning_rate: 1.1896838259417564e-05\n",
      "Step: 4580, train/epoch: 7.620632171630859\n",
      "Step: 4590, train/loss: 0.006500000134110451\n",
      "Step: 4590, train/grad_norm: 28.295949935913086\n",
      "Step: 4590, train/learning_rate: 1.1813644050562289e-05\n",
      "Step: 4590, train/epoch: 7.637271404266357\n",
      "Step: 4600, train/loss: 0.009700000286102295\n",
      "Step: 4600, train/grad_norm: 0.009189384989440441\n",
      "Step: 4600, train/learning_rate: 1.1730448932212312e-05\n",
      "Step: 4600, train/epoch: 7.653910160064697\n",
      "Step: 4610, train/loss: 0.003100000089034438\n",
      "Step: 4610, train/grad_norm: 0.015582352876663208\n",
      "Step: 4610, train/learning_rate: 1.1647254723357037e-05\n",
      "Step: 4610, train/epoch: 7.670548915863037\n",
      "Step: 4620, train/loss: 0.00019999999494757503\n",
      "Step: 4620, train/grad_norm: 0.007017421070486307\n",
      "Step: 4620, train/learning_rate: 1.156405960500706e-05\n",
      "Step: 4620, train/epoch: 7.687188148498535\n",
      "Step: 4630, train/loss: 0.002899999963119626\n",
      "Step: 4630, train/grad_norm: 0.007523098960518837\n",
      "Step: 4630, train/learning_rate: 1.1480865396151785e-05\n",
      "Step: 4630, train/epoch: 7.703826904296875\n",
      "Step: 4640, train/loss: 0.006099999882280827\n",
      "Step: 4640, train/grad_norm: 0.006036947946995497\n",
      "Step: 4640, train/learning_rate: 1.1397670277801808e-05\n",
      "Step: 4640, train/epoch: 7.720465660095215\n",
      "Step: 4650, train/loss: 0.04960000142455101\n",
      "Step: 4650, train/grad_norm: 8.943552017211914\n",
      "Step: 4650, train/learning_rate: 1.1314476068946533e-05\n",
      "Step: 4650, train/epoch: 7.737104892730713\n",
      "Step: 4660, train/loss: 0.008700000122189522\n",
      "Step: 4660, train/grad_norm: 13.57039737701416\n",
      "Step: 4660, train/learning_rate: 1.1231280950596556e-05\n",
      "Step: 4660, train/epoch: 7.753743648529053\n",
      "Step: 4670, train/loss: 0.00559999980032444\n",
      "Step: 4670, train/grad_norm: 0.1131676733493805\n",
      "Step: 4670, train/learning_rate: 1.1148086741741281e-05\n",
      "Step: 4670, train/epoch: 7.770382881164551\n",
      "Step: 4680, train/loss: 0.00800000037997961\n",
      "Step: 4680, train/grad_norm: 0.009173382073640823\n",
      "Step: 4680, train/learning_rate: 1.1064891623391304e-05\n",
      "Step: 4680, train/epoch: 7.787021636962891\n",
      "Step: 4690, train/loss: 0.0017000000225380063\n",
      "Step: 4690, train/grad_norm: 0.004128666128963232\n",
      "Step: 4690, train/learning_rate: 1.0981697414536029e-05\n",
      "Step: 4690, train/epoch: 7.8036603927612305\n",
      "Step: 4700, train/loss: 0.03720000013709068\n",
      "Step: 4700, train/grad_norm: 18.543659210205078\n",
      "Step: 4700, train/learning_rate: 1.0898502296186052e-05\n",
      "Step: 4700, train/epoch: 7.8202996253967285\n",
      "Step: 4710, train/loss: 0.0003000000142492354\n",
      "Step: 4710, train/grad_norm: 0.053395435214042664\n",
      "Step: 4710, train/learning_rate: 1.0815308087330777e-05\n",
      "Step: 4710, train/epoch: 7.836938381195068\n",
      "Step: 4720, train/loss: 0.039900001138448715\n",
      "Step: 4720, train/grad_norm: 0.042120106518268585\n",
      "Step: 4720, train/learning_rate: 1.07321129689808e-05\n",
      "Step: 4720, train/epoch: 7.853577136993408\n",
      "Step: 4730, train/loss: 0.022199999541044235\n",
      "Step: 4730, train/grad_norm: 0.0016329768113791943\n",
      "Step: 4730, train/learning_rate: 1.0648918760125525e-05\n",
      "Step: 4730, train/epoch: 7.870216369628906\n",
      "Step: 4740, train/loss: 0.014100000262260437\n",
      "Step: 4740, train/grad_norm: 0.003983767703175545\n",
      "Step: 4740, train/learning_rate: 1.0565723641775548e-05\n",
      "Step: 4740, train/epoch: 7.886855125427246\n",
      "Step: 4750, train/loss: 0.05460000038146973\n",
      "Step: 4750, train/grad_norm: 0.12214413285255432\n",
      "Step: 4750, train/learning_rate: 1.0482529432920273e-05\n",
      "Step: 4750, train/epoch: 7.903494358062744\n",
      "Step: 4760, train/loss: 0.0008999999845400453\n",
      "Step: 4760, train/grad_norm: 0.6034405827522278\n",
      "Step: 4760, train/learning_rate: 1.0399334314570297e-05\n",
      "Step: 4760, train/epoch: 7.920133113861084\n",
      "Step: 4770, train/loss: 0.010599999688565731\n",
      "Step: 4770, train/grad_norm: 0.010191811248660088\n",
      "Step: 4770, train/learning_rate: 1.0316140105715021e-05\n",
      "Step: 4770, train/epoch: 7.936771869659424\n",
      "Step: 4780, train/loss: 0.012600000016391277\n",
      "Step: 4780, train/grad_norm: 0.005151811055839062\n",
      "Step: 4780, train/learning_rate: 1.0232944987365045e-05\n",
      "Step: 4780, train/epoch: 7.953411102294922\n",
      "Step: 4790, train/loss: 0.00019999999494757503\n",
      "Step: 4790, train/grad_norm: 0.004864716902375221\n",
      "Step: 4790, train/learning_rate: 1.014975077850977e-05\n",
      "Step: 4790, train/epoch: 7.970049858093262\n",
      "Step: 4800, train/loss: 0.018200000748038292\n",
      "Step: 4800, train/grad_norm: 0.0033757209312170744\n",
      "Step: 4800, train/learning_rate: 1.0066555660159793e-05\n",
      "Step: 4800, train/epoch: 7.986688613891602\n",
      "Step: 4808, eval/loss: 1.2348039150238037\n",
      "Step: 4808, eval/accuracy: 0.8292378187179565\n",
      "Step: 4808, eval/f1: 0.8241517543792725\n",
      "Step: 4808, eval/runtime: 28.85140037536621\n",
      "Step: 4808, eval/samples_per_second: 249.65899658203125\n",
      "Step: 4808, eval/steps_per_second: 4.4710001945495605\n",
      "Step: 4808, train/epoch: 8.0\n",
      "Step: 4810, train/loss: 9.999999747378752e-05\n",
      "Step: 4810, train/grad_norm: 0.7666314244270325\n",
      "Step: 4810, train/learning_rate: 9.983361451304518e-06\n",
      "Step: 4810, train/epoch: 8.003327369689941\n",
      "Step: 4820, train/loss: 0.00430000014603138\n",
      "Step: 4820, train/grad_norm: 0.023722415789961815\n",
      "Step: 4820, train/learning_rate: 9.900166332954541e-06\n",
      "Step: 4820, train/epoch: 8.019967079162598\n",
      "Step: 4830, train/loss: 0.0003000000142492354\n",
      "Step: 4830, train/grad_norm: 0.012214124202728271\n",
      "Step: 4830, train/learning_rate: 9.816972124099266e-06\n",
      "Step: 4830, train/epoch: 8.036605834960938\n",
      "Step: 4840, train/loss: 0.00019999999494757503\n",
      "Step: 4840, train/grad_norm: 0.2513265907764435\n",
      "Step: 4840, train/learning_rate: 9.733777005749289e-06\n",
      "Step: 4840, train/epoch: 8.053244590759277\n",
      "Step: 4850, train/loss: 9.999999747378752e-05\n",
      "Step: 4850, train/grad_norm: 0.029458660632371902\n",
      "Step: 4850, train/learning_rate: 9.650582796894014e-06\n",
      "Step: 4850, train/epoch: 8.069883346557617\n",
      "Step: 4860, train/loss: 0.0005000000237487257\n",
      "Step: 4860, train/grad_norm: 2.3000376224517822\n",
      "Step: 4860, train/learning_rate: 9.567387678544037e-06\n",
      "Step: 4860, train/epoch: 8.086522102355957\n",
      "Step: 4870, train/loss: 0.0010999999940395355\n",
      "Step: 4870, train/grad_norm: 0.5276331305503845\n",
      "Step: 4870, train/learning_rate: 9.48419256019406e-06\n",
      "Step: 4870, train/epoch: 8.103161811828613\n",
      "Step: 4880, train/loss: 0.00279999990016222\n",
      "Step: 4880, train/grad_norm: 0.08473066985607147\n",
      "Step: 4880, train/learning_rate: 9.400998351338785e-06\n",
      "Step: 4880, train/epoch: 8.119800567626953\n",
      "Step: 4890, train/loss: 9.999999747378752e-05\n",
      "Step: 4890, train/grad_norm: 0.03068920411169529\n",
      "Step: 4890, train/learning_rate: 9.317803232988808e-06\n",
      "Step: 4890, train/epoch: 8.136439323425293\n",
      "Step: 4900, train/loss: 0.010700000450015068\n",
      "Step: 4900, train/grad_norm: 41.194847106933594\n",
      "Step: 4900, train/learning_rate: 9.234609024133533e-06\n",
      "Step: 4900, train/epoch: 8.153078079223633\n",
      "Step: 4910, train/loss: 9.999999747378752e-05\n",
      "Step: 4910, train/grad_norm: 0.10251782834529877\n",
      "Step: 4910, train/learning_rate: 9.151413905783556e-06\n",
      "Step: 4910, train/epoch: 8.169716835021973\n",
      "Step: 4920, train/loss: 0.00989999994635582\n",
      "Step: 4920, train/grad_norm: 0.0016976845217868686\n",
      "Step: 4920, train/learning_rate: 9.068219696928281e-06\n",
      "Step: 4920, train/epoch: 8.186356544494629\n",
      "Step: 4930, train/loss: 0.0003000000142492354\n",
      "Step: 4930, train/grad_norm: 0.0010363926412537694\n",
      "Step: 4930, train/learning_rate: 8.985024578578304e-06\n",
      "Step: 4930, train/epoch: 8.202995300292969\n",
      "Step: 4940, train/loss: 9.999999747378752e-05\n",
      "Step: 4940, train/grad_norm: 0.06777878850698471\n",
      "Step: 4940, train/learning_rate: 8.90183036972303e-06\n",
      "Step: 4940, train/epoch: 8.219634056091309\n",
      "Step: 4950, train/loss: 9.999999747378752e-05\n",
      "Step: 4950, train/grad_norm: 0.010048332624137402\n",
      "Step: 4950, train/learning_rate: 8.818635251373053e-06\n",
      "Step: 4950, train/epoch: 8.236272811889648\n",
      "Step: 4960, train/loss: 0.01510000042617321\n",
      "Step: 4960, train/grad_norm: 0.0009986992226913571\n",
      "Step: 4960, train/learning_rate: 8.735441042517778e-06\n",
      "Step: 4960, train/epoch: 8.252911567687988\n",
      "Step: 4970, train/loss: 0.03819999843835831\n",
      "Step: 4970, train/grad_norm: 39.41048049926758\n",
      "Step: 4970, train/learning_rate: 8.6522459241678e-06\n",
      "Step: 4970, train/epoch: 8.269550323486328\n",
      "Step: 4980, train/loss: 0.01600000075995922\n",
      "Step: 4980, train/grad_norm: 24.60858726501465\n",
      "Step: 4980, train/learning_rate: 8.569051715312526e-06\n",
      "Step: 4980, train/epoch: 8.286190032958984\n",
      "Step: 4990, train/loss: 0.02669999934732914\n",
      "Step: 4990, train/grad_norm: 23.370330810546875\n",
      "Step: 4990, train/learning_rate: 8.485856596962549e-06\n",
      "Step: 4990, train/epoch: 8.302828788757324\n",
      "Step: 5000, train/loss: 0.052400000393390656\n",
      "Step: 5000, train/grad_norm: 0.017830781638622284\n",
      "Step: 5000, train/learning_rate: 8.402662388107274e-06\n",
      "Step: 5000, train/epoch: 8.319467544555664\n",
      "Step: 5010, train/loss: 0.00019999999494757503\n",
      "Step: 5010, train/grad_norm: 0.617307722568512\n",
      "Step: 5010, train/learning_rate: 8.319467269757297e-06\n",
      "Step: 5010, train/epoch: 8.336106300354004\n",
      "Step: 5020, train/loss: 0.00019999999494757503\n",
      "Step: 5020, train/grad_norm: 0.0034916673321276903\n",
      "Step: 5020, train/learning_rate: 8.236273060902022e-06\n",
      "Step: 5020, train/epoch: 8.352745056152344\n",
      "Step: 5030, train/loss: 0.00039999998989515007\n",
      "Step: 5030, train/grad_norm: 0.008065717294812202\n",
      "Step: 5030, train/learning_rate: 8.153077942552045e-06\n",
      "Step: 5030, train/epoch: 8.369384765625\n",
      "Step: 5040, train/loss: 0.000699999975040555\n",
      "Step: 5040, train/grad_norm: 0.19997301697731018\n",
      "Step: 5040, train/learning_rate: 8.06988373369677e-06\n",
      "Step: 5040, train/epoch: 8.38602352142334\n",
      "Step: 5050, train/loss: 0.0007999999797903001\n",
      "Step: 5050, train/grad_norm: 0.006396290380507708\n",
      "Step: 5050, train/learning_rate: 7.986688615346793e-06\n",
      "Step: 5050, train/epoch: 8.40266227722168\n",
      "Step: 5060, train/loss: 0.023800000548362732\n",
      "Step: 5060, train/grad_norm: 0.8045639991760254\n",
      "Step: 5060, train/learning_rate: 7.903494406491518e-06\n",
      "Step: 5060, train/epoch: 8.41930103302002\n",
      "Step: 5070, train/loss: 0.00019999999494757503\n",
      "Step: 5070, train/grad_norm: 0.028077615424990654\n",
      "Step: 5070, train/learning_rate: 7.820299288141541e-06\n",
      "Step: 5070, train/epoch: 8.43593978881836\n",
      "Step: 5080, train/loss: 0.006300000008195639\n",
      "Step: 5080, train/grad_norm: 0.02752799354493618\n",
      "Step: 5080, train/learning_rate: 7.737105079286266e-06\n",
      "Step: 5080, train/epoch: 8.452579498291016\n",
      "Step: 5090, train/loss: 0.004800000227987766\n",
      "Step: 5090, train/grad_norm: 0.02187427319586277\n",
      "Step: 5090, train/learning_rate: 7.65390996093629e-06\n",
      "Step: 5090, train/epoch: 8.469218254089355\n",
      "Step: 5100, train/loss: 0.00039999998989515007\n",
      "Step: 5100, train/grad_norm: 0.09056638926267624\n",
      "Step: 5100, train/learning_rate: 7.570715297333663e-06\n",
      "Step: 5100, train/epoch: 8.485857009887695\n",
      "Step: 5110, train/loss: 0.0026000000070780516\n",
      "Step: 5110, train/grad_norm: 0.03562169149518013\n",
      "Step: 5110, train/learning_rate: 7.487520633731037e-06\n",
      "Step: 5110, train/epoch: 8.502495765686035\n",
      "Step: 5120, train/loss: 0.01860000006854534\n",
      "Step: 5120, train/grad_norm: 0.011841276660561562\n",
      "Step: 5120, train/learning_rate: 7.4043259701284114e-06\n",
      "Step: 5120, train/epoch: 8.519134521484375\n",
      "Step: 5130, train/loss: 0.00039999998989515007\n",
      "Step: 5130, train/grad_norm: 2.3545103073120117\n",
      "Step: 5130, train/learning_rate: 7.3211313065257855e-06\n",
      "Step: 5130, train/epoch: 8.535773277282715\n",
      "Step: 5140, train/loss: 0.001500000013038516\n",
      "Step: 5140, train/grad_norm: 0.041702598333358765\n",
      "Step: 5140, train/learning_rate: 7.2379366429231595e-06\n",
      "Step: 5140, train/epoch: 8.552412986755371\n",
      "Step: 5150, train/loss: 0.00139999995008111\n",
      "Step: 5150, train/grad_norm: 0.009719899855554104\n",
      "Step: 5150, train/learning_rate: 7.1547419793205336e-06\n",
      "Step: 5150, train/epoch: 8.569051742553711\n",
      "Step: 5160, train/loss: 0.010700000450015068\n",
      "Step: 5160, train/grad_norm: 1.8450812101364136\n",
      "Step: 5160, train/learning_rate: 7.071547315717908e-06\n",
      "Step: 5160, train/epoch: 8.58569049835205\n",
      "Step: 5170, train/loss: 0.011599999852478504\n",
      "Step: 5170, train/grad_norm: 0.008758406154811382\n",
      "Step: 5170, train/learning_rate: 6.988352652115282e-06\n",
      "Step: 5170, train/epoch: 8.60232925415039\n",
      "Step: 5180, train/loss: 9.999999747378752e-05\n",
      "Step: 5180, train/grad_norm: 0.01902041770517826\n",
      "Step: 5180, train/learning_rate: 6.905157988512656e-06\n",
      "Step: 5180, train/epoch: 8.61896800994873\n",
      "Step: 5190, train/loss: 0.01269999984651804\n",
      "Step: 5190, train/grad_norm: 15.93992805480957\n",
      "Step: 5190, train/learning_rate: 6.82196332491003e-06\n",
      "Step: 5190, train/epoch: 8.635607719421387\n",
      "Step: 5200, train/loss: 9.999999747378752e-05\n",
      "Step: 5200, train/grad_norm: 0.040208447724580765\n",
      "Step: 5200, train/learning_rate: 6.738768661307404e-06\n",
      "Step: 5200, train/epoch: 8.652246475219727\n",
      "Step: 5210, train/loss: 0.0012000000569969416\n",
      "Step: 5210, train/grad_norm: 0.3222430944442749\n",
      "Step: 5210, train/learning_rate: 6.655573997704778e-06\n",
      "Step: 5210, train/epoch: 8.668885231018066\n",
      "Step: 5220, train/loss: 9.999999747378752e-05\n",
      "Step: 5220, train/grad_norm: 0.02102123387157917\n",
      "Step: 5220, train/learning_rate: 6.572379334102152e-06\n",
      "Step: 5220, train/epoch: 8.685523986816406\n",
      "Step: 5230, train/loss: 0.003599999938160181\n",
      "Step: 5230, train/grad_norm: 0.0033665832597762346\n",
      "Step: 5230, train/learning_rate: 6.489184670499526e-06\n",
      "Step: 5230, train/epoch: 8.702162742614746\n",
      "Step: 5240, train/loss: 0.008899999782443047\n",
      "Step: 5240, train/grad_norm: 2.5286030769348145\n",
      "Step: 5240, train/learning_rate: 6.4059900068969e-06\n",
      "Step: 5240, train/epoch: 8.718802452087402\n",
      "Step: 5250, train/loss: 0.010300000198185444\n",
      "Step: 5250, train/grad_norm: 0.009241238236427307\n",
      "Step: 5250, train/learning_rate: 6.322795343294274e-06\n",
      "Step: 5250, train/epoch: 8.735441207885742\n",
      "Step: 5260, train/loss: 0.010200000368058681\n",
      "Step: 5260, train/grad_norm: 0.1517401486635208\n",
      "Step: 5260, train/learning_rate: 6.239600679691648e-06\n",
      "Step: 5260, train/epoch: 8.752079963684082\n",
      "Step: 5270, train/loss: 0.0019000000320374966\n",
      "Step: 5270, train/grad_norm: 0.001256847637705505\n",
      "Step: 5270, train/learning_rate: 6.156406016089022e-06\n",
      "Step: 5270, train/epoch: 8.768718719482422\n",
      "Step: 5280, train/loss: 0.012500000186264515\n",
      "Step: 5280, train/grad_norm: 18.91196632385254\n",
      "Step: 5280, train/learning_rate: 6.073211352486396e-06\n",
      "Step: 5280, train/epoch: 8.785357475280762\n",
      "Step: 5290, train/loss: 0.00279999990016222\n",
      "Step: 5290, train/grad_norm: 0.00689125107601285\n",
      "Step: 5290, train/learning_rate: 5.99001668888377e-06\n",
      "Step: 5290, train/epoch: 8.801996231079102\n",
      "Step: 5300, train/loss: 9.999999747378752e-05\n",
      "Step: 5300, train/grad_norm: 0.0065519860945641994\n",
      "Step: 5300, train/learning_rate: 5.906822025281144e-06\n",
      "Step: 5300, train/epoch: 8.818635940551758\n",
      "Step: 5310, train/loss: 0.0012000000569969416\n",
      "Step: 5310, train/grad_norm: 0.30924028158187866\n",
      "Step: 5310, train/learning_rate: 5.823627361678518e-06\n",
      "Step: 5310, train/epoch: 8.835274696350098\n",
      "Step: 5320, train/loss: 0.0012000000569969416\n",
      "Step: 5320, train/grad_norm: 0.0037480993196368217\n",
      "Step: 5320, train/learning_rate: 5.740432698075892e-06\n",
      "Step: 5320, train/epoch: 8.851913452148438\n",
      "Step: 5330, train/loss: 0.008200000040233135\n",
      "Step: 5330, train/grad_norm: 0.007094337604939938\n",
      "Step: 5330, train/learning_rate: 5.6572380344732665e-06\n",
      "Step: 5330, train/epoch: 8.868552207946777\n",
      "Step: 5340, train/loss: 0.00019999999494757503\n",
      "Step: 5340, train/grad_norm: 0.017980068922042847\n",
      "Step: 5340, train/learning_rate: 5.5740433708706405e-06\n",
      "Step: 5340, train/epoch: 8.885190963745117\n",
      "Step: 5350, train/loss: 0.00019999999494757503\n",
      "Step: 5350, train/grad_norm: 0.00283476198092103\n",
      "Step: 5350, train/learning_rate: 5.4908487072680146e-06\n",
      "Step: 5350, train/epoch: 8.901830673217773\n",
      "Step: 5360, train/loss: 9.999999747378752e-05\n",
      "Step: 5360, train/grad_norm: 0.014209472574293613\n",
      "Step: 5360, train/learning_rate: 5.407654043665389e-06\n",
      "Step: 5360, train/epoch: 8.918469429016113\n",
      "Step: 5370, train/loss: 0.00279999990016222\n",
      "Step: 5370, train/grad_norm: 0.01969996467232704\n",
      "Step: 5370, train/learning_rate: 5.324459380062763e-06\n",
      "Step: 5370, train/epoch: 8.935108184814453\n",
      "Step: 5380, train/loss: 9.999999747378752e-05\n",
      "Step: 5380, train/grad_norm: 0.04772414639592171\n",
      "Step: 5380, train/learning_rate: 5.241264716460137e-06\n",
      "Step: 5380, train/epoch: 8.951746940612793\n",
      "Step: 5390, train/loss: 0.006500000134110451\n",
      "Step: 5390, train/grad_norm: 0.001938745379447937\n",
      "Step: 5390, train/learning_rate: 5.158070052857511e-06\n",
      "Step: 5390, train/epoch: 8.968385696411133\n",
      "Step: 5400, train/loss: 0.006000000052154064\n",
      "Step: 5400, train/grad_norm: 0.007824479602277279\n",
      "Step: 5400, train/learning_rate: 5.074875389254885e-06\n",
      "Step: 5400, train/epoch: 8.985025405883789\n",
      "Step: 5409, eval/loss: 1.2309720516204834\n",
      "Step: 5409, eval/accuracy: 0.8377065062522888\n",
      "Step: 5409, eval/f1: 0.8315966129302979\n",
      "Step: 5409, eval/runtime: 28.948699951171875\n",
      "Step: 5409, eval/samples_per_second: 248.81900024414062\n",
      "Step: 5409, eval/steps_per_second: 4.455999851226807\n",
      "Step: 5409, train/epoch: 9.0\n",
      "Step: 5410, train/loss: 9.999999747378752e-05\n",
      "Step: 5410, train/grad_norm: 0.07887700945138931\n",
      "Step: 5410, train/learning_rate: 4.991680725652259e-06\n",
      "Step: 5410, train/epoch: 9.001664161682129\n",
      "Step: 5420, train/loss: 0.003599999938160181\n",
      "Step: 5420, train/grad_norm: 0.006857562344521284\n",
      "Step: 5420, train/learning_rate: 4.908486062049633e-06\n",
      "Step: 5420, train/epoch: 9.018302917480469\n",
      "Step: 5430, train/loss: 0.0005000000237487257\n",
      "Step: 5430, train/grad_norm: 7.571389675140381\n",
      "Step: 5430, train/learning_rate: 4.825291398447007e-06\n",
      "Step: 5430, train/epoch: 9.034941673278809\n",
      "Step: 5440, train/loss: 0.0\n",
      "Step: 5440, train/grad_norm: 0.0012691993033513427\n",
      "Step: 5440, train/learning_rate: 4.74209628009703e-06\n",
      "Step: 5440, train/epoch: 9.051580429077148\n",
      "Step: 5450, train/loss: 0.00019999999494757503\n",
      "Step: 5450, train/grad_norm: 0.011338585987687111\n",
      "Step: 5450, train/learning_rate: 4.658901616494404e-06\n",
      "Step: 5450, train/epoch: 9.068219184875488\n",
      "Step: 5460, train/loss: 0.0017000000225380063\n",
      "Step: 5460, train/grad_norm: 0.027152331545948982\n",
      "Step: 5460, train/learning_rate: 4.575706952891778e-06\n",
      "Step: 5460, train/epoch: 9.084858894348145\n",
      "Step: 5470, train/loss: 9.999999747378752e-05\n",
      "Step: 5470, train/grad_norm: 0.0015688189305365086\n",
      "Step: 5470, train/learning_rate: 4.492512289289152e-06\n",
      "Step: 5470, train/epoch: 9.101497650146484\n",
      "Step: 5480, train/loss: 0.01269999984651804\n",
      "Step: 5480, train/grad_norm: 20.11486053466797\n",
      "Step: 5480, train/learning_rate: 4.409317625686526e-06\n",
      "Step: 5480, train/epoch: 9.118136405944824\n",
      "Step: 5490, train/loss: 0.012400000356137753\n",
      "Step: 5490, train/grad_norm: 0.0028535404708236456\n",
      "Step: 5490, train/learning_rate: 4.3261229620839e-06\n",
      "Step: 5490, train/epoch: 9.134775161743164\n",
      "Step: 5500, train/loss: 9.999999747378752e-05\n",
      "Step: 5500, train/grad_norm: 0.18671156466007233\n",
      "Step: 5500, train/learning_rate: 4.242928298481274e-06\n",
      "Step: 5500, train/epoch: 9.151413917541504\n",
      "Step: 5510, train/loss: 9.999999747378752e-05\n",
      "Step: 5510, train/grad_norm: 0.04849660024046898\n",
      "Step: 5510, train/learning_rate: 4.1597336348786484e-06\n",
      "Step: 5510, train/epoch: 9.16805362701416\n",
      "Step: 5520, train/loss: 0.007899999618530273\n",
      "Step: 5520, train/grad_norm: 0.018368057906627655\n",
      "Step: 5520, train/learning_rate: 4.0765389712760225e-06\n",
      "Step: 5520, train/epoch: 9.1846923828125\n",
      "Step: 5530, train/loss: 9.999999747378752e-05\n",
      "Step: 5530, train/grad_norm: 0.00171018042601645\n",
      "Step: 5530, train/learning_rate: 3.9933443076733965e-06\n",
      "Step: 5530, train/epoch: 9.20133113861084\n",
      "Step: 5540, train/loss: 0.0008999999845400453\n",
      "Step: 5540, train/grad_norm: 0.009237313643097878\n",
      "Step: 5540, train/learning_rate: 3.910149644070771e-06\n",
      "Step: 5540, train/epoch: 9.21796989440918\n",
      "Step: 5550, train/loss: 9.999999747378752e-05\n",
      "Step: 5550, train/grad_norm: 0.0028690234757959843\n",
      "Step: 5550, train/learning_rate: 3.826954980468145e-06\n",
      "Step: 5550, train/epoch: 9.23460865020752\n",
      "Step: 5560, train/loss: 0.0\n",
      "Step: 5560, train/grad_norm: 0.011292828246951103\n",
      "Step: 5560, train/learning_rate: 3.7437603168655187e-06\n",
      "Step: 5560, train/epoch: 9.251248359680176\n",
      "Step: 5570, train/loss: 0.0012000000569969416\n",
      "Step: 5570, train/grad_norm: 22.78629493713379\n",
      "Step: 5570, train/learning_rate: 3.6605656532628927e-06\n",
      "Step: 5570, train/epoch: 9.267887115478516\n",
      "Step: 5580, train/loss: 0.0\n",
      "Step: 5580, train/grad_norm: 0.0016825993079692125\n",
      "Step: 5580, train/learning_rate: 3.5773709896602668e-06\n",
      "Step: 5580, train/epoch: 9.284525871276855\n",
      "Step: 5590, train/loss: 9.999999747378752e-05\n",
      "Step: 5590, train/grad_norm: 0.0021301552187651396\n",
      "Step: 5590, train/learning_rate: 3.494176326057641e-06\n",
      "Step: 5590, train/epoch: 9.301164627075195\n",
      "Step: 5600, train/loss: 9.999999747378752e-05\n",
      "Step: 5600, train/grad_norm: 0.0030837515369057655\n",
      "Step: 5600, train/learning_rate: 3.410981662455015e-06\n",
      "Step: 5600, train/epoch: 9.317803382873535\n",
      "Step: 5610, train/loss: 0.0\n",
      "Step: 5610, train/grad_norm: 0.0013668129686266184\n",
      "Step: 5610, train/learning_rate: 3.327786998852389e-06\n",
      "Step: 5610, train/epoch: 9.334442138671875\n",
      "Step: 5620, train/loss: 0.002899999963119626\n",
      "Step: 5620, train/grad_norm: 0.0033569466322660446\n",
      "Step: 5620, train/learning_rate: 3.244592335249763e-06\n",
      "Step: 5620, train/epoch: 9.351081848144531\n",
      "Step: 5630, train/loss: 0.006899999920278788\n",
      "Step: 5630, train/grad_norm: 0.0012497592251747847\n",
      "Step: 5630, train/learning_rate: 3.161397671647137e-06\n",
      "Step: 5630, train/epoch: 9.367720603942871\n",
      "Step: 5640, train/loss: 0.0024999999441206455\n",
      "Step: 5640, train/grad_norm: 31.68977928161621\n",
      "Step: 5640, train/learning_rate: 3.078203008044511e-06\n",
      "Step: 5640, train/epoch: 9.384359359741211\n",
      "Step: 5650, train/loss: 0.008799999952316284\n",
      "Step: 5650, train/grad_norm: 0.6410245895385742\n",
      "Step: 5650, train/learning_rate: 2.995008344441885e-06\n",
      "Step: 5650, train/epoch: 9.40099811553955\n",
      "Step: 5660, train/loss: 0.0\n",
      "Step: 5660, train/grad_norm: 0.07817915827035904\n",
      "Step: 5660, train/learning_rate: 2.911813680839259e-06\n",
      "Step: 5660, train/epoch: 9.41763687133789\n",
      "Step: 5670, train/loss: 9.999999747378752e-05\n",
      "Step: 5670, train/grad_norm: 0.0011319156037643552\n",
      "Step: 5670, train/learning_rate: 2.8286190172366332e-06\n",
      "Step: 5670, train/epoch: 9.434276580810547\n",
      "Step: 5680, train/loss: 9.999999747378752e-05\n",
      "Step: 5680, train/grad_norm: 0.44602835178375244\n",
      "Step: 5680, train/learning_rate: 2.7454243536340073e-06\n",
      "Step: 5680, train/epoch: 9.450915336608887\n",
      "Step: 5690, train/loss: 9.999999747378752e-05\n",
      "Step: 5690, train/grad_norm: 0.08410465717315674\n",
      "Step: 5690, train/learning_rate: 2.6622296900313813e-06\n",
      "Step: 5690, train/epoch: 9.467554092407227\n",
      "Step: 5700, train/loss: 9.999999747378752e-05\n",
      "Step: 5700, train/grad_norm: 0.000882161024492234\n",
      "Step: 5700, train/learning_rate: 2.5790350264287554e-06\n",
      "Step: 5700, train/epoch: 9.484192848205566\n",
      "Step: 5710, train/loss: 0.004000000189989805\n",
      "Step: 5710, train/grad_norm: 0.004816730041056871\n",
      "Step: 5710, train/learning_rate: 2.4958403628261294e-06\n",
      "Step: 5710, train/epoch: 9.500831604003906\n",
      "Step: 5720, train/loss: 0.004600000102072954\n",
      "Step: 5720, train/grad_norm: 27.799068450927734\n",
      "Step: 5720, train/learning_rate: 2.4126456992235035e-06\n",
      "Step: 5720, train/epoch: 9.517471313476562\n",
      "Step: 5730, train/loss: 0.00019999999494757503\n",
      "Step: 5730, train/grad_norm: 1.5886996984481812\n",
      "Step: 5730, train/learning_rate: 2.329450808247202e-06\n",
      "Step: 5730, train/epoch: 9.534110069274902\n",
      "Step: 5740, train/loss: 0.0020000000949949026\n",
      "Step: 5740, train/grad_norm: 0.0027122304309159517\n",
      "Step: 5740, train/learning_rate: 2.246256144644576e-06\n",
      "Step: 5740, train/epoch: 9.550748825073242\n",
      "Step: 5750, train/loss: 0.0006000000284984708\n",
      "Step: 5750, train/grad_norm: 0.008674096316099167\n",
      "Step: 5750, train/learning_rate: 2.16306148104195e-06\n",
      "Step: 5750, train/epoch: 9.567387580871582\n",
      "Step: 5760, train/loss: 0.0\n",
      "Step: 5760, train/grad_norm: 0.0009577866876497865\n",
      "Step: 5760, train/learning_rate: 2.0798668174393242e-06\n",
      "Step: 5760, train/epoch: 9.584026336669922\n",
      "Step: 5770, train/loss: 0.00039999998989515007\n",
      "Step: 5770, train/grad_norm: 0.0008097283425740898\n",
      "Step: 5770, train/learning_rate: 1.9966721538366983e-06\n",
      "Step: 5770, train/epoch: 9.600665092468262\n",
      "Step: 5780, train/loss: 0.0\n",
      "Step: 5780, train/grad_norm: 0.023085884749889374\n",
      "Step: 5780, train/learning_rate: 1.9134774902340723e-06\n",
      "Step: 5780, train/epoch: 9.617304801940918\n",
      "Step: 5790, train/loss: 9.999999747378752e-05\n",
      "Step: 5790, train/grad_norm: 0.0008483512792736292\n",
      "Step: 5790, train/learning_rate: 1.8302828266314464e-06\n",
      "Step: 5790, train/epoch: 9.633943557739258\n",
      "Step: 5800, train/loss: 0.0012000000569969416\n",
      "Step: 5800, train/grad_norm: 0.12559109926223755\n",
      "Step: 5800, train/learning_rate: 1.7470881630288204e-06\n",
      "Step: 5800, train/epoch: 9.650582313537598\n",
      "Step: 5810, train/loss: 0.0\n",
      "Step: 5810, train/grad_norm: 0.0011427321005612612\n",
      "Step: 5810, train/learning_rate: 1.6638934994261945e-06\n",
      "Step: 5810, train/epoch: 9.667221069335938\n",
      "Step: 5820, train/loss: 9.999999747378752e-05\n",
      "Step: 5820, train/grad_norm: 0.008365927264094353\n",
      "Step: 5820, train/learning_rate: 1.5806988358235685e-06\n",
      "Step: 5820, train/epoch: 9.683859825134277\n",
      "Step: 5830, train/loss: 0.0\n",
      "Step: 5830, train/grad_norm: 0.0007141615496948361\n",
      "Step: 5830, train/learning_rate: 1.4975041722209426e-06\n",
      "Step: 5830, train/epoch: 9.700499534606934\n",
      "Step: 5840, train/loss: 0.0017000000225380063\n",
      "Step: 5840, train/grad_norm: 0.07495198398828506\n",
      "Step: 5840, train/learning_rate: 1.4143095086183166e-06\n",
      "Step: 5840, train/epoch: 9.717138290405273\n",
      "Step: 5850, train/loss: 0.0\n",
      "Step: 5850, train/grad_norm: 0.02950187213718891\n",
      "Step: 5850, train/learning_rate: 1.3311148450156907e-06\n",
      "Step: 5850, train/epoch: 9.733777046203613\n",
      "Step: 5860, train/loss: 9.999999747378752e-05\n",
      "Step: 5860, train/grad_norm: 0.08292409777641296\n",
      "Step: 5860, train/learning_rate: 1.2479201814130647e-06\n",
      "Step: 5860, train/epoch: 9.750415802001953\n",
      "Step: 5870, train/loss: 0.0038999998942017555\n",
      "Step: 5870, train/grad_norm: 0.0014788688858971\n",
      "Step: 5870, train/learning_rate: 1.164725404123601e-06\n",
      "Step: 5870, train/epoch: 9.767054557800293\n",
      "Step: 5880, train/loss: 9.999999747378752e-05\n",
      "Step: 5880, train/grad_norm: 0.0009702467359602451\n",
      "Step: 5880, train/learning_rate: 1.081530740520975e-06\n",
      "Step: 5880, train/epoch: 9.78369426727295\n",
      "Step: 5890, train/loss: 0.00039999998989515007\n",
      "Step: 5890, train/grad_norm: 0.005022787488996983\n",
      "Step: 5890, train/learning_rate: 9.983360769183491e-07\n",
      "Step: 5890, train/epoch: 9.800333023071289\n",
      "Step: 5900, train/loss: 0.0007999999797903001\n",
      "Step: 5900, train/grad_norm: 0.5078767538070679\n",
      "Step: 5900, train/learning_rate: 9.151414133157232e-07\n",
      "Step: 5900, train/epoch: 9.816971778869629\n",
      "Step: 5910, train/loss: 0.0\n",
      "Step: 5910, train/grad_norm: 0.0008245991193689406\n",
      "Step: 5910, train/learning_rate: 8.319467497130972e-07\n",
      "Step: 5910, train/epoch: 9.833610534667969\n",
      "Step: 5920, train/loss: 0.0003000000142492354\n",
      "Step: 5920, train/grad_norm: 0.009511932730674744\n",
      "Step: 5920, train/learning_rate: 7.487520861104713e-07\n",
      "Step: 5920, train/epoch: 9.850249290466309\n",
      "Step: 5930, train/loss: 9.999999747378752e-05\n",
      "Step: 5930, train/grad_norm: 0.003310758387669921\n",
      "Step: 5930, train/learning_rate: 6.655574225078453e-07\n",
      "Step: 5930, train/epoch: 9.866888046264648\n",
      "Step: 5940, train/loss: 0.0006000000284984708\n",
      "Step: 5940, train/grad_norm: 12.530436515808105\n",
      "Step: 5940, train/learning_rate: 5.823627020618005e-07\n",
      "Step: 5940, train/epoch: 9.883527755737305\n",
      "Step: 5950, train/loss: 9.999999747378752e-05\n",
      "Step: 5950, train/grad_norm: 0.0011168799828737974\n",
      "Step: 5950, train/learning_rate: 4.991680384591746e-07\n",
      "Step: 5950, train/epoch: 9.900166511535645\n",
      "Step: 5960, train/loss: 0.00019999999494757503\n",
      "Step: 5960, train/grad_norm: 0.0026257438585162163\n",
      "Step: 5960, train/learning_rate: 4.159733748565486e-07\n",
      "Step: 5960, train/epoch: 9.916805267333984\n",
      "Step: 5970, train/loss: 0.0\n",
      "Step: 5970, train/grad_norm: 0.0010788902873173356\n",
      "Step: 5970, train/learning_rate: 3.3277871125392267e-07\n",
      "Step: 5970, train/epoch: 9.933444023132324\n",
      "Step: 5980, train/loss: 0.016599999740719795\n",
      "Step: 5980, train/grad_norm: 0.0007386934012174606\n",
      "Step: 5980, train/learning_rate: 2.495840192295873e-07\n",
      "Step: 5980, train/epoch: 9.950082778930664\n",
      "Step: 5990, train/loss: 0.002899999963119626\n",
      "Step: 5990, train/grad_norm: 0.0009490972151979804\n",
      "Step: 5990, train/learning_rate: 1.6638935562696133e-07\n",
      "Step: 5990, train/epoch: 9.96672248840332\n",
      "Step: 6000, train/loss: 0.0\n",
      "Step: 6000, train/grad_norm: 0.0011803392553701997\n",
      "Step: 6000, train/learning_rate: 8.319467781348067e-08\n",
      "Step: 6000, train/epoch: 9.98336124420166\n",
      "Step: 6010, train/loss: 0.006200000178068876\n",
      "Step: 6010, train/grad_norm: 0.0033837538212537766\n",
      "Step: 6010, train/learning_rate: 0.0\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, eval/loss: 1.2737236022949219\n",
      "Step: 6010, eval/accuracy: 0.8403443098068237\n",
      "Step: 6010, eval/f1: 0.8336495757102966\n",
      "Step: 6010, eval/runtime: 28.912200927734375\n",
      "Step: 6010, eval/samples_per_second: 249.13400268554688\n",
      "Step: 6010, eval/steps_per_second: 4.461999893188477\n",
      "Step: 6010, train/epoch: 10.0\n",
      "Step: 6010, train/train_runtime: 3220.8798828125\n",
      "Step: 6010, train/train_samples_per_second: 104.35299682617188\n",
      "Step: 6010, train/train_steps_per_second: 1.8660000562667847\n",
      "Step: 6010, train/total_flos: 8.843425279796838e+16\n",
      "Step: 6010, train/train_loss: 0.12472765147686005\n",
      "Step: 6010, train/epoch: 10.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Construct the logs directory path\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "# Retrieve all event files matching the pattern\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "# Function to print out TensorBoard event logs\n",
    "def print_events_from_file(event_files):\n",
    "    for event_file in event_files:\n",
    "        print(f\"Reading events from file: {event_file}\")\n",
    "        try:\n",
    "            for e in summary_iterator(event_file):\n",
    "                for v in e.summary.value:\n",
    "                    if v.HasField('simple_value'):\n",
    "                        print(f\"Step: {e.step}, {v.tag}: {v.simple_value}\")\n",
    "        except Exception as e:  # Just in case the event file is not readable\n",
    "            print(f\"Failed to read {event_file}: {e}\")\n",
    "\n",
    "print_events_from_file(event_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f8e0ccc-f7c4-4a02-a1e6-5e3012717125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step Train Loss Eval Loss  Accuracy        F1\n",
      "0   601   0.444500  0.520057  0.746356  0.745938\n",
      "1  1202   0.326400  0.422200  0.821186  0.816812\n",
      "2  1803   0.168300  0.487362  0.827294  0.805340\n",
      "3  2404   0.039800  0.725667  0.829793  0.823925\n",
      "4  3005   0.076100  0.742037  0.837707  0.823463\n",
      "5  3606   0.028900  1.216491  0.811467  0.808151\n",
      "6  4207   0.003000  1.271679  0.825073  0.820291\n",
      "7  4808   0.018200  1.234804  0.829238  0.824152\n",
      "8  5409   0.006000  1.230972  0.837707  0.831597\n",
      "9  6010   0.006200  1.273724  0.840344  0.833650\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "\n",
    "logs_directory = os.path.join('./', project_name, 'logs')\n",
    "file_pattern = 'events.out.tfevents.*'\n",
    "\n",
    "event_files = glob.glob(os.path.join(logs_directory, file_pattern))\n",
    "\n",
    "def extract_metrics(event_files):\n",
    "    data = []\n",
    "    last_train_loss = None\n",
    "\n",
    "    for event_file in event_files:\n",
    "        for e in summary_iterator(event_file):\n",
    "            for v in e.summary.value:\n",
    "                if v.HasField('simple_value'):\n",
    "                    step = e.step\n",
    "                    metric_name = v.tag.split('/')[-1]\n",
    "                    metric_value = v.simple_value\n",
    "\n",
    "                    formatted_value = f\"{metric_value:.6f}\"\n",
    "\n",
    "                    if 'train/loss' in v.tag:\n",
    "                        last_train_loss = formatted_value\n",
    "\n",
    "                    if 'eval' in v.tag:\n",
    "                        entry = next((item for item in data if item['Step'] == step), None)\n",
    "                        if not entry:\n",
    "                            entry = {'Step': step, 'Train Loss': last_train_loss, 'Eval Loss': None, 'Accuracy': None, 'F1': None}\n",
    "                            data.append(entry)\n",
    "                        if 'loss' in v.tag:\n",
    "                            entry['Eval Loss'] = formatted_value\n",
    "                        elif 'accuracy' in v.tag:\n",
    "                            entry['Accuracy'] = formatted_value\n",
    "                        elif 'f1' in v.tag:\n",
    "                            entry['F1'] = formatted_value\n",
    "    return data\n",
    "\n",
    "metrics_data = extract_metrics(event_files)\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "df = df.sort_values(by='Step')\n",
    "\n",
    "file_path = \"../images/\"+model_name+\"_Checkpoint_Data.csv\"\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52633cb6-5847-4d51-86c5-c6153e72cc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Checkpoint Step: checkpoint-5409\n",
      "Step              5409\n",
      "Train Loss    0.006000\n",
      "Eval Loss     1.230972\n",
      "Accuracy      0.837707\n",
      "F1            0.831597\n",
      "Rank Sum          11.0\n",
      "Name: 8, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.fillna({\n",
    "    'Eval Loss': float('inf'),\n",
    "    'Accuracy': 0,\n",
    "    'F1': 0\n",
    "}, inplace=True)\n",
    "\n",
    "df['Eval Loss'] = df['Eval Loss'].astype(float)\n",
    "df['Accuracy'] = df['Accuracy'].astype(float)\n",
    "df['F1'] = df['F1'].astype(float)\n",
    "\n",
    "df['Eval Loss Rank'] = df['Eval Loss'].rank(method='min', ascending=True)\n",
    "df['Accuracy Rank'] = df['Accuracy'].rank(method='min', ascending=False)\n",
    "df['F1 Rank'] = df['F1'].rank(method='min', ascending=False)\n",
    "\n",
    "df['Rank Sum'] = df['Eval Loss Rank'] + df['Accuracy Rank'] + df['F1 Rank']\n",
    "\n",
    "best_checkpoint = df.loc[df['Rank Sum'].idxmin()]\n",
    "\n",
    "checkpoint_folder_name = f\"checkpoint-{best_checkpoint['Step']}\"\n",
    "print(f\"Best Checkpoint Step: {checkpoint_folder_name}\")\n",
    "print(best_checkpoint[['Step', 'Train Loss', 'Eval Loss', 'Accuracy', 'F1', 'Rank Sum']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4044b-4ffc-409e-ac1a-8b4c58e72499",
   "metadata": {},
   "source": [
    "### Run TensorBoard\n",
    "tensorboard --logdir=~/kuk/Praxis/praxis-Llama-2-7b-hf-small-finetune/logs --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5a88a-7620-4280-970b-ff171568aebb",
   "metadata": {},
   "source": [
    "### PAUSE SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fc853d-d293-402d-87d0-e27c83915c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My flag to pause the script, set to True to pause\n",
    "pause_script = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53887034-7922-4ed5-bd9a-8c80d6596ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        print(\"Script Paused\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8248c6-2c95-4d82-a7f0-2e0ac37ab320",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pause_script:\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19be6e26-d888-4da0-b3f8-836d68ac2051",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b140136-4f0d-44e4-9955-d95a342f250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import pipeline\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "test_checkpoint_name = checkpoint_folder_name\n",
    "savedmodel = pipeline('text-classification', model=output_dir_path + \"/\" + test_checkpoint_name, device=accelerator.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e0c22fa-0da7-4a30-8d27-128cab0bdfc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 7203\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f2ad84-10b4-4f10-bbea-25571bfcda78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c19c1729ad4923b5f82b39494e25f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "max_tokens = 512\n",
    "truncated_articles = []\n",
    "\n",
    "for article in tqdm(tokenized_test_ds['article']):\n",
    "    encoded_article = tokenizer.encode(\n",
    "        article,\n",
    "        max_length=max_tokens,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "    truncated_article = tokenizer.decode(encoded_article, skip_special_tokens=True)\n",
    "    truncated_articles.append(truncated_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d687d739-aafa-4ae2-b938-e8465856746e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f25771c8f94690a05aab20a69db7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6481e20e88b048979dd1c9ff8b5423b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Labeling articles:   0%|          | 0/7203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create a dataset from the truncated articles\n",
    "data = {\"text\": truncated_articles}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Define a function to make predictions\n",
    "def classify_batch(batch):\n",
    "    return savedmodel(batch[\"text\"])\n",
    "\n",
    "# Process the dataset with batching\n",
    "test_predictions = []\n",
    "for batch in tqdm(dataset.to_dict()[\"text\"], desc=\"Classifying articles\"):\n",
    "    predictions = classify_batch({\"text\": batch})\n",
    "    test_predictions.extend(predictions)\n",
    "\n",
    "test_predictions_labels = []\n",
    "for prediction in tqdm(test_predictions, desc=\"Labeling articles\"):\n",
    "    label = 0 if prediction['label'] == 'LABEL_0' else 1\n",
    "    test_predictions_labels.append(label)\n",
    "\n",
    "print(test_predictions_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26d6eead-d836-4e80-8ff6-b86b8599b245",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "print(true_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10e33442-ac7f-49d3-a6ad-5eb4b892eedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(prediction):\n",
    "  return int(prediction['label'].split('_')[1])\n",
    "\n",
    "processed_predictions = [get_label(prediction) for prediction in test_predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f06a2c-5ded-4da0-8232-145383967c9d",
   "metadata": {},
   "source": [
    "<h1>Accuracy and F1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb4d8350-a8d1-4853-a9f8-74ae9ea36bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8403443009857005\n"
     ]
    }
   ],
   "source": [
    "true_articles = tokenized_test_ds['label']\n",
    "accuracy = accuracy_score(true_articles, test_predictions_labels)\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40780784-e77d-4b26-9dae-572137e591bf",
   "metadata": {},
   "source": [
    "<p>precision (how many of the items identified as positive are actually positive) and the recall (how many of the actual positives were identified correctly)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30465934-1b77-4a2c-8c52-34e239e37abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score: 0.834288180065729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(true_articles, test_predictions_labels, average='macro')\n",
    "print(\"f1_score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcaac14-2b83-4c24-b83a-f6d0e73a2d2a",
   "metadata": {},
   "source": [
    "<h1>Confusion Matrix</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57e64afb-e3ee-4c79-8228-b2d79806229e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(tokenized_test_ds['label'], processed_predictions)\n",
    "\n",
    "labels = ['human', 'machine']\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c87380b0-553d-4642-9697-26c76c98dde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook praxis-bert-base-cased-small-finetune-v7.ipynb to html\n",
      "[NbConvertApp] WARNING | Alternative text is missing on 1 image(s).\n",
      "[NbConvertApp] Writing 574392 bytes to html/praxis-bert-base-cased-small-finetune-v7.html\n"
     ]
    }
   ],
   "source": [
    "file_name = f\"{project_name}.ipynb\"\n",
    "html_file_name = f\"{file_name.replace('.ipynb', '.html')}\"\n",
    "\n",
    "command = f\"jupyter nbconvert '{file_name}' --to html --output-dir './html' --output '{html_file_name}'\"\n",
    "get_ipython().system(command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
